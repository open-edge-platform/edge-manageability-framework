# Design Proposal: Orchestrator Component Status Service

Author(s): Scott Baker

Last updated: 2025-11-18

## Abstract

This proposal describes a service that will publish the list of installed components in an orchestrator
as well as other associated metadata. The purpose is to be able to inspect an orchestrator to see what
components are installed and available, to inform the CLI and GUI.

## Problem Statement

Now that EMF is available in a modular configuration, it is possible to leave certain components or features
disabled. This may include:

- Disabling an entire subsystem, such as application-orchestration.

- Disabling a feature that is part of a subsystem, such as the Device Onboarding feature of EIM.

The CLI and GUI need to be able to intelligently react to these configurations, disabling functionality that is
not relevant to the user and providing appropriate error messages when necessary.

The GUI has the advantage that it *could* be statically configured at installation time to exclude certain
components, but the CLI does not have that capability. The same CLI binary must function with the orchestrator
regardless of how the orchestrator was configured.

Furthermore, the ability to inspect an orchestrator to determine its features programmatically may be useful
to higher level automation (bash scripts, terraform, ansible) to adapt to the orchestrator. For example, if
an orchestrator has app-orch installed, then use app-orch to install an application, otherwise download
a kubeconfig file and use helm to install an application. Similarly, if observability is installed, then
query loki for logs, otherwise retrieve the logs directly.

> **Note:** Features are not the same as endpoints, nor are features the same as components.
  For example, an inventory service (such as the one exposed by EIM) may be common to many
  configurations, but certain fields within objects in the inventory could be nonsensical
  if the appropriate feature does not exist.

## Proposal 1: Implement a microservice that returns a list of installed services

### Component Status Service

The proposal is to implement an endpoint in the orchestrator that returns information about the deployed configuration
of the orchestrator. The endpoint shall be made available at the following URL:

```text
https://api.<hostname>/v1/orchestrator
```

Note that this API is not project scoped. The status of the installation is gobal to an entire orchestrator,
across all organizations and projects.

This service shall be configured using a yaml file.
This file is intended to be simple and straightforward, yet also easily extensible.

The following is a hypothetical example:

```yaml
schema-version: 1.0
orchestrator:
  version: 2026.0
  features:
    application-orchestration:
      installed: false
    cluster-orchestration:
      installed: true
    edge-infrastructure-manager:
      installed: true
      inventory:
        installed: true
      out-of-band-managerment:
        installed: true
      device-onboarding:
        installed: false
    observability:
      installed: true
    multitenancy:
      installed: false
```

> *Note:* This file would be generated by the installer automatically.

<!-- prevent linter error from two consecutive block quotes -->

> *Note:* As an alternative to using a static file, a dynamic approach could be implemented that uses a list
  of services to generate the information from examining the system (for example by inspecting
  kubernetes) rather than pre-specifying the list of installed components.

This example indicates that application-orchestration is not installed, but cluster-orch and EIM are installed.
Furthermore, within the EIM service, the inventory and out-of-band management services are installed, but the
device-onboarding feature is not installed. Observability is installed, but multitenancy is not.

The yaml is intended to be hierarchical in nature. If the caller (i.e. the CLI) wishes to clarify the existence
of the out-of-band management feature, then it would use a simple bottom-up algorithm:

1. Look for the key `orchestrator.edge-infrastructure-manager.out-of-band-management.installed` and return the
   value if found.
2. If that key was not found, look for `orchestrator.edge-infrastructure-manager.installed` and return the
   value if found.
3. If that key was not found, then return `false`.

This `installed` field indicates choices that are made at installation time. It does not indicate the runtime
availability of a feature. i.e. if a component is down, then it is still considered installed as being down
is a temporary condition.

In addition to component installation, the following additional fields are present in the
yaml file:

- `schema-version`. Indicates the version of the schema that is being used.

- `orchestrator.version`. Indicates the version number of the EMF release that is being used.

The format is intended to be extensible. If additional fields were desired, such as component runtime status
`running` / `down`, they could easily be added in a backward compatible fashion.

The component status service shall render this yaml as json. For example,

```json
{
  "schema-version": 1.0,
  "orchestrator": {
    "version": 2026,
    "features": {
      "application-orchestration": {
        "installed": false
      },
      "cluster-orchestration": {
        "installed": true
      },
      "edge-infrastructure-manager": {
        "installed": true,
        "inventory": {
          "installed": true
        },
        "out-of-band-managerment": {
          "installed": true
        },
        "device-onboarding": {
          "installed": false
        }
      },
      "observability": {
        "installed": true
      },
      "multitenancy": {
        "installed": false
      }
    }
  }
}
```

The component status service will be instantiated by a helm chart and installed by argocd as all orchestrator
service are.
The helm chart shall take the yaml file as part of its `values.yaml`.
The helm chart's `values.yaml` shall be populated by the installer, similar to how other components are configured.

If/when we move to support installation mechanisms that do not use argocd, then the component status service, being
a helm chart, can be installed using whatever future mechanism is used.

### CLI Changes

The CLI shall be updated as follows:

- The CLI shall call the component status service to determine available features before executing a command.
  If a feature cannot be performed due to a feature that is not installed, then the CLI shall emit the
  appropriate error.

- The CLI `version` command shall be augmented to display both the CLI's version and the remote orchestrator's
  version, assuming the remote orchestrator endpoint has been configured.
  This is similar to how `kubectl version` reports both client and server version.

- The CLI command `list features` shall return a list of known features and their installation status.

- The CLI help commands such as `list -h` should only return nouns that exist. For example, if the app-orch
  feature is not installed, then `list applications` does not exist, and `applications` should not be in the
  help for `list`.

## Proposal 2: Rely on response codes

In the proposal, each backend service shall be modified to return a `501 not implemented` error if it receives
an API request for a feature that does not exist. If the gateway itself receives a request for an endpoint that
does not exist, then the gateway shall return the `501` error. The CLI in turn shall print an appropriate error
message when a `501` error is encountered.

Contrary to Proposal 1, there shall be no addition to the `version` command, no `list features` command, and no
limitation of `help` commands to the the feature set. Proposal 2 is solely intended to return intelligent error
responses when unavailable features are exercised.

## Proposal 3: Expose the health endpoints externally to the CLI

All well-written Kubernetes pods include `/livez`, `/readyz` and/or `/healthz` endpoints to determine the status
of the pod. This could be used to infer whether or not a feature is installed. If the `/readyz` or `/healthz` does
not exist, then the feature is likely unavailable. If the endpoints do exist, then the caller can further inspect
them to determine health.

These endpoints are not exposed through the API GW at this time and would need to be exposed, so the CLI can check
them.

Furthermore, these endpoints would also need to be authenticated, because the installation status to an unauthorized
user would be a security leak of potential sensitive information -- i.e. an attacker could learn what features are
not available, and tailor an attack to the orchestrator. Given that a proposal is in progress to eliminate the
API GW's AuthZ/AuthN in favor of only having backend service AuthZ/AuthN, this means these endpoints would have to
be authenticated in the backend service. Authenticating these endpoints is a potential complication
as Kubernetes uses these endpoints to determine health of pods.

## Rationale

Both proposals were designed for their simplicity. Proposal 1's service may be implemented as a simple web server in go,
serving up a single static response. The response does not change for the lifetime of an orchestrator,
other than potentially when upgrades occur and add additional features. Likewise, it is extensible. For example, if
it is later determined that dynamic component status would be useful, then the service can be extended to provide
that information.

Proposal 2 is simpler than proposal 1, but with the following limitations:

- Customers using automation/scripts cannot determine whether a feature exists unless they try it and observe a
  failure. They may be able to use a nondestructive probe like `get applications` to facilitate checking.

- The CLI enhancements for listing available features, limiting help to available features, and obtaining the
  orchestrator version are not available.

- The CLI and the GUI will not have the capability to degrade commands that are common across features. For example,
  the ability for the `host` command to omit some information if certain features are disabled, or to fail early
  if the customer sets values that are not available. At this time, these situations might be hypothetical -- i.e.
  we don't know the cases where a shared component (like inventory) needs to be treated differently based on the
  features.

Proposal 3 is also simpler than Proposal 1, but presents security concerns around the exposure of `/health` and
`/status` endpoints that may make it infeasiable. It also may not be possible in all circumstances to distinguish
between a service that is down and a service that is not installed.

## Affected components and Teams

- Proposal 1: Orchestrator Component Status Service (new component)

- Proposal 2: All backend services, with minor changes to API GW

- Proposal 3: API GW

- CLI

## Implementation plan

### Proposal 1

The component status service shall be implemented in golang, published as a docker container
with a helm chart. It shall be added to argocd and deployed as part of the orchestrator.

Once the component status service is available, the CLI shall be modified to query the
component status service.

### Proposal 2

Modify each backend service to return a 501 error as appropriate.

Modify the gateway to return a 501 error as appropriate.

Modify the CLI to issue the appropriate error emssages.

### Proposal 3

Modify the API GW to pass the health and status endpoints.

Modify the CLI to check these endpoints.

## Decision

Pending.

## Open issues (if applicable)

None.
