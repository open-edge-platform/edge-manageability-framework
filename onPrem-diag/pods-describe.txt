Name:             argocd-application-controller-0
Namespace:        argocd
Priority:         0
Service Account:  argocd-application-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:29:32 +0000
Labels:           app.kubernetes.io/component=application-controller
                  app.kubernetes.io/instance=argocd
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=argocd-application-controller
                  app.kubernetes.io/part-of=argocd
                  app.kubernetes.io/version=v3.0.12
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=argocd-application-controller-64c67f856b
                  helm.sh/chart=argo-cd-8.2.7
                  statefulset.kubernetes.io/pod-name=argocd-application-controller-0
Annotations:      checksum/cm: 985c4e0e56b3969c1562564578a145212894b5bf9cd0de3939efd3384ed63a15
                  checksum/cmd-params: c58c44db1ab4c245fe8339f6ed85575b2dc3a3d3bd5ec9faaebffdde577cda10
                  cni.projectcalico.org/containerID: 07f4739f257e8a3005b04916684d1c24a255295e14eeaaa55425128043ca4313
                  cni.projectcalico.org/podIP: 10.42.65.83/32
                  cni.projectcalico.org/podIPs: 10.42.65.83/32
Status:           Running
IP:               10.42.65.83
IPs:
  IP:           10.42.65.83
Controlled By:  StatefulSet/argocd-application-controller
Containers:
  application-controller:
    Container ID:    containerd://4bd194b5f5ee8d7840e902de8e0fe047f5d66bcc03be5f276abed764322416c3
    Image:           quay.io/argoproj/argocd:v3.0.12
    Image ID:        quay.io/argoproj/argocd@sha256:e5b3a85a8b01811601b3dbf81dd768000e5de5672eb448c83c1169a01a71889f
    Port:            8082/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      /usr/local/bin/argocd-application-controller
      --metrics-port=8082
    State:          Running
      Started:      Tue, 03 Feb 2026 17:29:33 +0000
    Ready:          True
    Restart Count:  0
    Readiness:      http-get http://:metrics/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ARGOCD_CONTROLLER_REPLICAS:                                        1
      ARGOCD_APPLICATION_CONTROLLER_NAME:                                argocd-application-controller
      ARGOCD_RECONCILIATION_TIMEOUT:                                     <set to the key 'timeout.reconciliation' of config map 'argocd-cm'>                                          Optional: true
      ARGOCD_HARD_RECONCILIATION_TIMEOUT:                                <set to the key 'timeout.hard.reconciliation' of config map 'argocd-cm'>                                     Optional: true
      ARGOCD_RECONCILIATION_JITTER:                                      <set to the key 'timeout.reconciliation.jitter' of config map 'argocd-cm'>                                   Optional: true
      ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS:                            <set to the key 'controller.repo.error.grace.period.seconds' of config map 'argocd-cmd-params-cm'>           Optional: true
      ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER:                         <set to the key 'repo.server' of config map 'argocd-cmd-params-cm'>                                          Optional: true
      ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS:         <set to the key 'controller.repo.server.timeout.seconds' of config map 'argocd-cmd-params-cm'>               Optional: true
      ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS:                   <set to the key 'controller.status.processors' of config map 'argocd-cmd-params-cm'>                         Optional: true
      ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS:                <set to the key 'controller.operation.processors' of config map 'argocd-cmd-params-cm'>                      Optional: true
      ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT:                           <set to the key 'controller.log.format' of config map 'argocd-cmd-params-cm'>                                Optional: true
      ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL:                            <set to the key 'controller.log.level' of config map 'argocd-cmd-params-cm'>                                 Optional: true
      ARGOCD_LOG_FORMAT_TIMESTAMP:                                       <set to the key 'log.format.timestamp' of config map 'argocd-cmd-params-cm'>                                 Optional: true
      ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION:            <set to the key 'controller.metrics.cache.expiration' of config map 'argocd-cmd-params-cm'>                  Optional: true
      ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS:           <set to the key 'controller.self.heal.timeout.seconds' of config map 'argocd-cmd-params-cm'>                 Optional: true
      ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS:   <set to the key 'controller.self.heal.backoff.timeout.seconds' of config map 'argocd-cmd-params-cm'>         Optional: true
      ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR:            <set to the key 'controller.self.heal.backoff.factor' of config map 'argocd-cmd-params-cm'>                  Optional: true
      ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS:       <set to the key 'controller.self.heal.backoff.cap.seconds' of config map 'argocd-cmd-params-cm'>             Optional: true
      ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS:  <set to the key 'controller.self.heal.backoff.cooldown.seconds' of config map 'argocd-cmd-params-cm'>        Optional: true
      ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT:                        <set to the key 'controller.sync.timeout.seconds' of config map 'argocd-cmd-params-cm'>                      Optional: true
      ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT:               <set to the key 'controller.repo.server.plaintext' of config map 'argocd-cmd-params-cm'>                     Optional: true
      ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS:              <set to the key 'controller.repo.server.strict.tls' of config map 'argocd-cmd-params-cm'>                    Optional: true
      ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH:             <set to the key 'controller.resource.health.persist' of config map 'argocd-cmd-params-cm'>                   Optional: true
      ARGOCD_APP_STATE_CACHE_EXPIRATION:                                 <set to the key 'controller.app.state.cache.expiration' of config map 'argocd-cmd-params-cm'>                Optional: true
      REDIS_SERVER:                                                      <set to the key 'redis.server' of config map 'argocd-cmd-params-cm'>                                         Optional: true
      REDIS_COMPRESSION:                                                 <set to the key 'redis.compression' of config map 'argocd-cmd-params-cm'>                                    Optional: true
      REDISDB:                                                           <set to the key 'redis.db' of config map 'argocd-cmd-params-cm'>                                             Optional: true
      REDIS_USERNAME:                                                    <set to the key 'redis-username' in secret 'argocd-redis'>                                                   Optional: true
      REDIS_PASSWORD:                                                    <set to the key 'auth' in secret 'argocd-redis'>                                                             Optional: false
      REDIS_SENTINEL_USERNAME:                                           <set to the key 'redis-sentinel-username' in secret 'argocd-redis'>                                          Optional: true
      REDIS_SENTINEL_PASSWORD:                                           <set to the key 'redis-sentinel-password' in secret 'argocd-redis'>                                          Optional: true
      ARGOCD_DEFAULT_CACHE_EXPIRATION:                                   <set to the key 'controller.default.cache.expiration' of config map 'argocd-cmd-params-cm'>                  Optional: true
      ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS:                        <set to the key 'otlp.address' of config map 'argocd-cmd-params-cm'>                                         Optional: true
      ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE:                       <set to the key 'otlp.insecure' of config map 'argocd-cmd-params-cm'>                                        Optional: true
      ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS:                        <set to the key 'otlp.headers' of config map 'argocd-cmd-params-cm'>                                         Optional: true
      ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS:                          <set to the key 'otlp.attrs' of config map 'argocd-cmd-params-cm'>                                           Optional: true
      ARGOCD_APPLICATION_NAMESPACES:                                     <set to the key 'application.namespaces' of config map 'argocd-cmd-params-cm'>                               Optional: true
      ARGOCD_CONTROLLER_SHARDING_ALGORITHM:                              <set to the key 'controller.sharding.algorithm' of config map 'argocd-cmd-params-cm'>                        Optional: true
      ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT:           <set to the key 'controller.kubectl.parallelism.limit' of config map 'argocd-cmd-params-cm'>                 Optional: true
      ARGOCD_K8SCLIENT_RETRY_MAX:                                        <set to the key 'controller.k8sclient.retry.max' of config map 'argocd-cmd-params-cm'>                       Optional: true
      ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF:                               <set to the key 'controller.k8sclient.retry.base.backoff' of config map 'argocd-cmd-params-cm'>              Optional: true
      ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF:                    <set to the key 'controller.diff.server.side' of config map 'argocd-cmd-params-cm'>                          Optional: true
      ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT:                               <set to the key 'controller.ignore.normalizer.jq.timeout' of config map 'argocd-cmd-params-cm'>              Optional: true
      ARGOCD_HYDRATOR_ENABLED:                                           <set to the key 'hydrator.enabled' of config map 'argocd-cmd-params-cm'>                                     Optional: true
      ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING:                      <set to the key 'controller.cluster.cache.batch.events.processing' of config map 'argocd-cmd-params-cm'>     Optional: true
      ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL:                   <set to the key 'controller.cluster.cache.events.processing.interval' of config map 'argocd-cmd-params-cm'>  Optional: true
      ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER:                       <set to the key 'commit.server' of config map 'argocd-cmd-params-cm'>                                        Optional: true
      KUBECACHEDIR:                                                      /tmp/kubecache
    Mounts:
      /app/config/controller/tls from argocd-repo-server-tls (rw)
      /home/argocd from argocd-home (rw)
      /home/argocd/params from argocd-cmd-params-cm (rw)
      /tmp from argocd-application-controller-tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ndslh (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  argocd-home:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  argocd-application-controller-tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  argocd-repo-server-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  argocd-repo-server-tls
    Optional:    true
  argocd-cmd-params-cm:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-cmd-params-cm
    Optional:  true
  kube-api-access-ndslh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             argocd-applicationset-controller-5c566f94b4-n5f8n
Namespace:        argocd
Priority:         0
Service Account:  argocd-applicationset-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:29:32 +0000
Labels:           app.kubernetes.io/component=applicationset-controller
                  app.kubernetes.io/instance=argocd
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=argocd-applicationset-controller
                  app.kubernetes.io/part-of=argocd
                  app.kubernetes.io/version=v3.0.12
                  helm.sh/chart=argo-cd-8.2.7
                  pod-template-hash=5c566f94b4
Annotations:      checksum/cmd-params: c58c44db1ab4c245fe8339f6ed85575b2dc3a3d3bd5ec9faaebffdde577cda10
                  cni.projectcalico.org/containerID: 0648f8703985cac2f5498ad928baf98eda53c2ad42d9213f5d817fa08e4de357
                  cni.projectcalico.org/podIP: 10.42.65.82/32
                  cni.projectcalico.org/podIPs: 10.42.65.82/32
Status:           Running
IP:               10.42.65.82
IPs:
  IP:           10.42.65.82
Controlled By:  ReplicaSet/argocd-applicationset-controller-5c566f94b4
Containers:
  applicationset-controller:
    Container ID:    containerd://b3f0f981c8a7285fd6f4a4b270c9dad83d8b48a5f8070ff957094b3852c605c1
    Image:           quay.io/argoproj/argocd:v3.0.12
    Image ID:        quay.io/argoproj/argocd@sha256:e5b3a85a8b01811601b3dbf81dd768000e5de5672eb448c83c1169a01a71889f
    Ports:           8080/TCP, 8081/TCP, 7000/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      /usr/local/bin/argocd-applicationset-controller
      --metrics-addr=:8080
      --probe-addr=:8081
      --webhook-addr=:7000
    State:          Running
      Started:      Tue, 03 Feb 2026 17:29:33 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NAMESPACE:                                                      argocd (v1:metadata.namespace)
      ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS:  <set to the key 'applicationsetcontroller.global.preserved.annotations' of config map 'argocd-cmd-params-cm'>    Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS:       <set to the key 'applicationsetcontroller.global.preserved.labels' of config map 'argocd-cmd-params-cm'>         Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION:        <set to the key 'applicationsetcontroller.enable.leader.election' of config map 'argocd-cmd-params-cm'>          Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER:                   <set to the key 'repo.server' of config map 'argocd-cmd-params-cm'>                                              Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_POLICY:                        <set to the key 'applicationsetcontroller.policy' of config map 'argocd-cmd-params-cm'>                          Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE:        <set to the key 'applicationsetcontroller.enable.policy.override' of config map 'argocd-cmd-params-cm'>          Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG:                         <set to the key 'applicationsetcontroller.debug' of config map 'argocd-cmd-params-cm'>                           Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT:                     <set to the key 'applicationsetcontroller.log.format' of config map 'argocd-cmd-params-cm'>                      Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL:                      <set to the key 'applicationsetcontroller.log.level' of config map 'argocd-cmd-params-cm'>                       Optional: true
      ARGOCD_LOG_FORMAT_TIMESTAMP:                                    <set to the key 'log.format.timestamp' of config map 'argocd-cmd-params-cm'>                                     Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN:                       <set to the key 'applicationsetcontroller.dryrun' of config map 'argocd-cmd-params-cm'>                          Optional: true
      ARGOCD_GIT_MODULES_ENABLED:                                     <set to the key 'applicationsetcontroller.enable.git.submodule' of config map 'argocd-cmd-params-cm'>            Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS:      <set to the key 'applicationsetcontroller.enable.progressive.syncs' of config map 'argocd-cmd-params-cm'>        Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE:          <set to the key 'applicationsetcontroller.enable.tokenref.strict.mode' of config map 'argocd-cmd-params-cm'>     Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING:  <set to the key 'applicationsetcontroller.enable.new.git.file.globbing' of config map 'argocd-cmd-params-cm'>    Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT:         <set to the key 'applicationsetcontroller.repo.server.plaintext' of config map 'argocd-cmd-params-cm'>           Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS:        <set to the key 'applicationsetcontroller.repo.server.strict.tls' of config map 'argocd-cmd-params-cm'>          Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS:   <set to the key 'applicationsetcontroller.repo.server.timeout.seconds' of config map 'argocd-cmd-params-cm'>     Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS:    <set to the key 'applicationsetcontroller.concurrent.reconciliations.max' of config map 'argocd-cmd-params-cm'>  Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES:                    <set to the key 'applicationsetcontroller.namespaces' of config map 'argocd-cmd-params-cm'>                      Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH:              <set to the key 'applicationsetcontroller.scm.root.ca.path' of config map 'argocd-cmd-params-cm'>                Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS:         <set to the key 'applicationsetcontroller.allowed.scm.providers' of config map 'argocd-cmd-params-cm'>           Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS:          <set to the key 'applicationsetcontroller.enable.scm.providers' of config map 'argocd-cmd-params-cm'>            Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT:     <set to the key 'applicationsetcontroller.webhook.parallelism.limit' of config map 'argocd-cmd-params-cm'>       Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER:                 <set to the key 'applicationsetcontroller.requeue.after' of config map 'argocd-cmd-params-cm'>                   Optional: true
    Mounts:
      /app/config/gpg/keys from gpg-keyring (rw)
      /app/config/gpg/source from gpg-keys (rw)
      /app/config/reposerver/tls from argocd-repo-server-tls (rw)
      /app/config/ssh from ssh-known-hosts (rw)
      /app/config/tls from tls-certs (rw)
      /etc/ssl/certs/ca-certificates.crt from tls-from-node (rw)
      /etc/ssl/certs/gitea_cert.crt from gitea-tls (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t8wlj (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tls-from-node:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs/ca-certificates.crt
    HostPathType:  
  gitea-tls:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates/gitea_cert.crt
    HostPathType:  
  ssh-known-hosts:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-ssh-known-hosts-cm
    Optional:  false
  tls-certs:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-tls-certs-cm
    Optional:  false
  gpg-keys:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-gpg-keys-cm
    Optional:  false
  gpg-keyring:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  argocd-repo-server-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  argocd-repo-server-tls
    Optional:    true
  kube-api-access-t8wlj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: hostPath volumes, field error list: [spec.volumes[0].hostPath is forbidden, forbidden values found: /etc/ssl/certs/ca-certificates.crt, spec.volumes[1].hostPath is forbidden, forbidden values found: /usr/local/share/ca-certificates/gitea_cert.crt])(Forbidden reason: restricted volume types, field error list: [spec.volumes[0].hostPath: Forbidden, spec.volumes[1].hostPath: Forbidden])


Name:             argocd-notifications-controller-986c64cf7-xnkzm
Namespace:        argocd
Priority:         0
Service Account:  argocd-notifications-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:29:32 +0000
Labels:           app.kubernetes.io/component=notifications-controller
                  app.kubernetes.io/instance=argocd
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=argocd-notifications-controller
                  app.kubernetes.io/part-of=argocd
                  app.kubernetes.io/version=v3.0.12
                  helm.sh/chart=argo-cd-8.2.7
                  pod-template-hash=986c64cf7
Annotations:      cni.projectcalico.org/containerID: 59ec52349769e167b5240dbd53ad0130e0540400f4118d82b0a1a894e442aac6
                  cni.projectcalico.org/podIP: 10.42.65.78/32
                  cni.projectcalico.org/podIPs: 10.42.65.78/32
Status:           Running
IP:               10.42.65.78
IPs:
  IP:           10.42.65.78
Controlled By:  ReplicaSet/argocd-notifications-controller-986c64cf7
Containers:
  notifications-controller:
    Container ID:    containerd://dd08493216c062d54ec42bea66ebfa3a9fc701fcf800b16f6cd4077598af6fb3
    Image:           quay.io/argoproj/argocd:v3.0.12
    Image ID:        quay.io/argoproj/argocd@sha256:e5b3a85a8b01811601b3dbf81dd768000e5de5672eb448c83c1169a01a71889f
    Port:            9001/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      /usr/local/bin/argocd-notifications
      --metrics-port=9001
      --namespace=argocd
      --argocd-repo-server=argocd-repo-server:8081
      --secret-name=argocd-notifications-secret
    State:          Running
      Started:      Tue, 03 Feb 2026 17:29:33 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL:                          <set to the key 'notificationscontroller.log.level' of config map 'argocd-cmd-params-cm'>              Optional: true
      ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT:                         <set to the key 'notificationscontroller.log.format' of config map 'argocd-cmd-params-cm'>             Optional: true
      ARGOCD_LOG_FORMAT_TIMESTAMP:                                       <set to the key 'log.format.timestamp' of config map 'argocd-cmd-params-cm'>                           Optional: true
      ARGOCD_APPLICATION_NAMESPACES:                                     <set to the key 'application.namespaces' of config map 'argocd-cmd-params-cm'>                         Optional: true
      ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED:  <set to the key 'notificationscontroller.selfservice.enabled' of config map 'argocd-cmd-params-cm'>    Optional: true
      ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT:              <set to the key 'notificationscontroller.repo.server.plaintext' of config map 'argocd-cmd-params-cm'>  Optional: true
    Mounts:
      /app/config/reposerver/tls from argocd-repo-server-tls (rw)
      /app/config/tls from tls-certs (rw)
      /etc/ssl/certs/ca-certificates.crt from tls-from-node (rw)
      /etc/ssl/certs/gitea_cert.crt from gitea-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n9x4w (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tls-from-node:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs/ca-certificates.crt
    HostPathType:  
  gitea-tls:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates/gitea_cert.crt
    HostPathType:  
  tls-certs:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-tls-certs-cm
    Optional:  false
  argocd-repo-server-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  argocd-repo-server-tls
    Optional:    true
  kube-api-access-n9x4w:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: hostPath volumes, field error list: [spec.volumes[0].hostPath is forbidden, forbidden values found: /etc/ssl/certs/ca-certificates.crt, spec.volumes[1].hostPath is forbidden, forbidden values found: /usr/local/share/ca-certificates/gitea_cert.crt])(Forbidden reason: restricted volume types, field error list: [spec.volumes[0].hostPath: Forbidden, spec.volumes[1].hostPath: Forbidden])


Name:             argocd-redis-55b7998d46-kfzfl
Namespace:        argocd
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:29:32 +0000
Labels:           app.kubernetes.io/component=redis
                  app.kubernetes.io/instance=argocd
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=argocd-redis
                  app.kubernetes.io/part-of=argocd
                  app.kubernetes.io/version=v3.0.12
                  helm.sh/chart=argo-cd-8.2.7
                  pod-template-hash=55b7998d46
Annotations:      cni.projectcalico.org/containerID: 5bb283d3a5f35c37dce62c85119e22a674811bfe66eb9f59566d681b1dd45fb9
                  cni.projectcalico.org/podIP: 10.42.65.79/32
                  cni.projectcalico.org/podIPs: 10.42.65.79/32
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.79
IPs:
  IP:           10.42.65.79
Controlled By:  ReplicaSet/argocd-redis-55b7998d46
Containers:
  redis:
    Container ID:  containerd://5b51997906e2406f08a13c3e9bd2d14b0c76b87b220a3034271836065d6119ad
    Image:         ecr-public.aws.com/docker/library/redis:7.2.8-alpine
    Image ID:      ecr-public.aws.com/docker/library/redis@sha256:c88ea2979a49ca497bbf7d39241b237f86c98e58cb2f6b1bc2dd167621f819bb
    Port:          6379/TCP
    Host Port:     0/TCP
    Args:
      --save
      
      --appendonly
      no
      --requirepass $(REDIS_PASSWORD)
    State:          Running
      Started:      Tue, 03 Feb 2026 17:29:35 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      REDIS_PASSWORD:  <set to the key 'auth' in secret 'argocd-redis'>  Optional: false
    Mounts:
      /health from health (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w9pv9 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  health:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-redis-health-configmap
    Optional:  false
  kube-api-access-w9pv9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             argocd-repo-server-7fc579f7f4-nrsqs
Namespace:        argocd
Priority:         0
Service Account:  argocd-repo-server
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:29:32 +0000
Labels:           app.kubernetes.io/component=repo-server
                  app.kubernetes.io/instance=argocd
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=argocd-repo-server
                  app.kubernetes.io/part-of=argocd
                  app.kubernetes.io/version=v3.0.12
                  helm.sh/chart=argo-cd-8.2.7
                  pod-template-hash=7fc579f7f4
Annotations:      checksum/cm: 985c4e0e56b3969c1562564578a145212894b5bf9cd0de3939efd3384ed63a15
                  checksum/cmd-params: c58c44db1ab4c245fe8339f6ed85575b2dc3a3d3bd5ec9faaebffdde577cda10
                  cni.projectcalico.org/containerID: adaafc0de4a5dcfbb31ab75424ae3e2ce027d3d887726b2cd81a438611d7049a
                  cni.projectcalico.org/podIP: 10.42.65.80/32
                  cni.projectcalico.org/podIPs: 10.42.65.80/32
Status:           Running
IP:               10.42.65.80
IPs:
  IP:           10.42.65.80
Controlled By:  ReplicaSet/argocd-repo-server-7fc579f7f4
Init Containers:
  copyutil:
    Container ID:    containerd://e4898874493df655e86b9a4adf43bc7d9f464ab74f6b1b206c93ed0cfa6ab831
    Image:           quay.io/argoproj/argocd:v3.0.12
    Image ID:        quay.io/argoproj/argocd@sha256:e5b3a85a8b01811601b3dbf81dd768000e5de5672eb448c83c1169a01a71889f
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/cp
      -n
      /usr/local/bin/argocd
      /var/run/argocd/argocd-cmp-server
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:29:33 +0000
      Finished:     Tue, 03 Feb 2026 17:29:33 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/argocd from var-files (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ghcn6 (ro)
Containers:
  repo-server:
    Container ID:    containerd://a3d3a38d3b8f12b13484a6090091b1c414b3e201d43cf3c53f5a75be5e03783c
    Image:           quay.io/argoproj/argocd:v3.0.12
    Image ID:        quay.io/argoproj/argocd@sha256:e5b3a85a8b01811601b3dbf81dd768000e5de5672eb448c83c1169a01a71889f
    Ports:           8081/TCP, 8084/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      /usr/local/bin/argocd-repo-server
      --port=8081
      --metrics-port=8084
    State:          Running
      Started:      Tue, 03 Feb 2026 17:29:34 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:metrics/healthz%3Ffull=true delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:metrics/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ARGOCD_REPO_SERVER_NAME:                                      argocd-repo-server
      ARGOCD_RECONCILIATION_TIMEOUT:                                <set to the key 'timeout.reconciliation' of config map 'argocd-cm'>                                          Optional: true
      ARGOCD_REPO_SERVER_LOGFORMAT:                                 <set to the key 'reposerver.log.format' of config map 'argocd-cmd-params-cm'>                                Optional: true
      ARGOCD_REPO_SERVER_LOGLEVEL:                                  <set to the key 'reposerver.log.level' of config map 'argocd-cmd-params-cm'>                                 Optional: true
      ARGOCD_LOG_FORMAT_TIMESTAMP:                                  <set to the key 'log.format.timestamp' of config map 'argocd-cmd-params-cm'>                                 Optional: true
      ARGOCD_REPO_SERVER_PARALLELISM_LIMIT:                         <set to the key 'reposerver.parallelism.limit' of config map 'argocd-cmd-params-cm'>                         Optional: true
      ARGOCD_REPO_SERVER_LISTEN_ADDRESS:                            <set to the key 'reposerver.listen.address' of config map 'argocd-cmd-params-cm'>                            Optional: true
      ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS:                    <set to the key 'reposerver.metrics.listen.address' of config map 'argocd-cmd-params-cm'>                    Optional: true
      ARGOCD_REPO_SERVER_DISABLE_TLS:                               <set to the key 'reposerver.disable.tls' of config map 'argocd-cmd-params-cm'>                               Optional: true
      ARGOCD_TLS_MIN_VERSION:                                       <set to the key 'reposerver.tls.minversion' of config map 'argocd-cmd-params-cm'>                            Optional: true
      ARGOCD_TLS_MAX_VERSION:                                       <set to the key 'reposerver.tls.maxversion' of config map 'argocd-cmd-params-cm'>                            Optional: true
      ARGOCD_TLS_CIPHERS:                                           <set to the key 'reposerver.tls.ciphers' of config map 'argocd-cmd-params-cm'>                               Optional: true
      ARGOCD_REPO_CACHE_EXPIRATION:                                 <set to the key 'reposerver.repo.cache.expiration' of config map 'argocd-cmd-params-cm'>                     Optional: true
      REDIS_SERVER:                                                 <set to the key 'redis.server' of config map 'argocd-cmd-params-cm'>                                         Optional: true
      REDIS_COMPRESSION:                                            <set to the key 'redis.compression' of config map 'argocd-cmd-params-cm'>                                    Optional: true
      REDISDB:                                                      <set to the key 'redis.db' of config map 'argocd-cmd-params-cm'>                                             Optional: true
      REDIS_USERNAME:                                               <set to the key 'redis-username' in secret 'argocd-redis'>                                                   Optional: true
      REDIS_PASSWORD:                                               <set to the key 'auth' in secret 'argocd-redis'>                                                             Optional: false
      REDIS_SENTINEL_USERNAME:                                      <set to the key 'redis-sentinel-username' in secret 'argocd-redis'>                                          Optional: true
      REDIS_SENTINEL_PASSWORD:                                      <set to the key 'redis-sentinel-password' in secret 'argocd-redis'>                                          Optional: true
      ARGOCD_DEFAULT_CACHE_EXPIRATION:                              <set to the key 'reposerver.default.cache.expiration' of config map 'argocd-cmd-params-cm'>                  Optional: true
      ARGOCD_REPO_SERVER_OTLP_ADDRESS:                              <set to the key 'otlp.address' of config map 'argocd-cmd-params-cm'>                                         Optional: true
      ARGOCD_REPO_SERVER_OTLP_INSECURE:                             <set to the key 'otlp.insecure' of config map 'argocd-cmd-params-cm'>                                        Optional: true
      ARGOCD_REPO_SERVER_OTLP_HEADERS:                              <set to the key 'otlp.headers' of config map 'argocd-cmd-params-cm'>                                         Optional: true
      ARGOCD_REPO_SERVER_OTLP_ATTRS:                                <set to the key 'otlp.attrs' of config map 'argocd-cmd-params-cm'>                                           Optional: true
      ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE:     <set to the key 'reposerver.max.combined.directory.manifests.size' of config map 'argocd-cmd-params-cm'>     Optional: true
      ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS:                     <set to the key 'reposerver.plugin.tar.exclusions' of config map 'argocd-cmd-params-cm'>                     Optional: true
      ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS:        <set to the key 'reposerver.plugin.use.manifest.generate.paths' of config map 'argocd-cmd-params-cm'>        Optional: true
      ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS:              <set to the key 'reposerver.allow.oob.symlinks' of config map 'argocd-cmd-params-cm'>                        Optional: true
      ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE:            <set to the key 'reposerver.streamed.manifest.max.tar.size' of config map 'argocd-cmd-params-cm'>            Optional: true
      ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE:      <set to the key 'reposerver.streamed.manifest.max.extracted.size' of config map 'argocd-cmd-params-cm'>      Optional: true
      ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE:          <set to the key 'reposerver.helm.manifest.max.extracted.size' of config map 'argocd-cmd-params-cm'>          Optional: true
      ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE:  <set to the key 'reposerver.disable.helm.manifest.max.extracted.size' of config map 'argocd-cmd-params-cm'>  Optional: true
      ARGOCD_GIT_MODULES_ENABLED:                                   <set to the key 'reposerver.enable.git.submodule' of config map 'argocd-cmd-params-cm'>                      Optional: true
      ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT:                       <set to the key 'reposerver.git.lsremote.parallelism.limit' of config map 'argocd-cmd-params-cm'>            Optional: true
      ARGOCD_GIT_REQUEST_TIMEOUT:                                   <set to the key 'reposerver.git.request.timeout' of config map 'argocd-cmd-params-cm'>                       Optional: true
      ARGOCD_REVISION_CACHE_LOCK_TIMEOUT:                           <set to the key 'reposerver.revision.cache.lock.timeout' of config map 'argocd-cmd-params-cm'>               Optional: true
      ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES:                <set to the key 'reposerver.include.hidden.directories' of config map 'argocd-cmd-params-cm'>                Optional: true
      HELM_CACHE_HOME:                                              /helm-working-dir
      HELM_CONFIG_HOME:                                             /helm-working-dir
      HELM_DATA_HOME:                                               /helm-working-dir
    Mounts:
      /app/config/gpg/keys from gpg-keyring (rw)
      /app/config/gpg/source from gpg-keys (rw)
      /app/config/reposerver/tls from argocd-repo-server-tls (rw)
      /app/config/ssh from ssh-known-hosts (rw)
      /app/config/tls from tls-certs (rw)
      /etc/ssl/certs/ca-certificates.crt from tls-from-node (rw)
      /etc/ssl/certs/gitea_cert.crt from gitea-tls (rw)
      /helm-working-dir from helm-working-dir (rw)
      /home/argocd/cmp-server/plugins from plugins (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ghcn6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tls-from-node:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs/ca-certificates.crt
    HostPathType:  
  gitea-tls:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates/gitea_cert.crt
    HostPathType:  
  helm-working-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  plugins:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  var-files:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  ssh-known-hosts:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-ssh-known-hosts-cm
    Optional:  false
  tls-certs:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-tls-certs-cm
    Optional:  false
  gpg-keys:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-gpg-keys-cm
    Optional:  false
  gpg-keyring:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  argocd-repo-server-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  argocd-repo-server-tls
    Optional:    true
  kube-api-access-ghcn6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: restricted volume types, field error list: [spec.volumes[0].hostPath: Forbidden, spec.volumes[1].hostPath: Forbidden])(Forbidden reason: hostPath volumes, field error list: [spec.volumes[0].hostPath is forbidden, forbidden values found: /etc/ssl/certs/ca-certificates.crt, spec.volumes[1].hostPath is forbidden, forbidden values found: /usr/local/share/ca-certificates/gitea_cert.crt])
  Warning  Unhealthy        53m   kubelet       Liveness probe failed: Get "http://10.42.65.80:8084/healthz?full=true": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             argocd-server-d6cb7d97b-tgs55
Namespace:        argocd
Priority:         0
Service Account:  argocd-server
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:29:32 +0000
Labels:           app.kubernetes.io/component=server
                  app.kubernetes.io/instance=argocd
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=argocd-server
                  app.kubernetes.io/part-of=argocd
                  app.kubernetes.io/version=v3.0.12
                  helm.sh/chart=argo-cd-8.2.7
                  pod-template-hash=d6cb7d97b
Annotations:      checksum/cm: 985c4e0e56b3969c1562564578a145212894b5bf9cd0de3939efd3384ed63a15
                  checksum/cmd-params: c58c44db1ab4c245fe8339f6ed85575b2dc3a3d3bd5ec9faaebffdde577cda10
                  cni.projectcalico.org/containerID: e491dd6a9c8e9563691fc496e856dcc1b5baf9f5a867f75e1fe41af1f214da77
                  cni.projectcalico.org/podIP: 10.42.65.81/32
                  cni.projectcalico.org/podIPs: 10.42.65.81/32
Status:           Running
IP:               10.42.65.81
IPs:
  IP:           10.42.65.81
Controlled By:  ReplicaSet/argocd-server-d6cb7d97b
Containers:
  server:
    Container ID:    containerd://e884023669925e082a6ccb47d75ad916eaa4e8e0568f46c2f07888d60c6441ea
    Image:           quay.io/argoproj/argocd:v3.0.12
    Image ID:        quay.io/argoproj/argocd@sha256:e5b3a85a8b01811601b3dbf81dd768000e5de5672eb448c83c1169a01a71889f
    Ports:           8080/TCP, 8083/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      /usr/local/bin/argocd-server
      --port=8080
      --metrics-port=8083
    State:          Running
      Started:      Tue, 03 Feb 2026 17:29:33 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:server/healthz%3Ffull=true delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:server/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ARGOCD_SERVER_NAME:                                             argocd-server
      ARGOCD_SERVER_INSECURE:                                         <set to the key 'server.insecure' of config map 'argocd-cmd-params-cm'>                                        Optional: true
      ARGOCD_SERVER_BASEHREF:                                         <set to the key 'server.basehref' of config map 'argocd-cmd-params-cm'>                                        Optional: true
      ARGOCD_SERVER_ROOTPATH:                                         <set to the key 'server.rootpath' of config map 'argocd-cmd-params-cm'>                                        Optional: true
      ARGOCD_SERVER_LOGFORMAT:                                        <set to the key 'server.log.format' of config map 'argocd-cmd-params-cm'>                                      Optional: true
      ARGOCD_SERVER_LOG_LEVEL:                                        <set to the key 'server.log.level' of config map 'argocd-cmd-params-cm'>                                       Optional: true
      ARGOCD_SERVER_REPO_SERVER:                                      <set to the key 'repo.server' of config map 'argocd-cmd-params-cm'>                                            Optional: true
      ARGOCD_SERVER_DEX_SERVER:                                       <set to the key 'server.dex.server' of config map 'argocd-cmd-params-cm'>                                      Optional: true
      ARGOCD_SERVER_DISABLE_AUTH:                                     <set to the key 'server.disable.auth' of config map 'argocd-cmd-params-cm'>                                    Optional: true
      ARGOCD_SERVER_ENABLE_GZIP:                                      <set to the key 'server.enable.gzip' of config map 'argocd-cmd-params-cm'>                                     Optional: true
      ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS:                      <set to the key 'server.repo.server.timeout.seconds' of config map 'argocd-cmd-params-cm'>                     Optional: true
      ARGOCD_SERVER_X_FRAME_OPTIONS:                                  <set to the key 'server.x.frame.options' of config map 'argocd-cmd-params-cm'>                                 Optional: true
      ARGOCD_SERVER_CONTENT_SECURITY_POLICY:                          <set to the key 'server.content.security.policy' of config map 'argocd-cmd-params-cm'>                         Optional: true
      ARGOCD_SERVER_REPO_SERVER_PLAINTEXT:                            <set to the key 'server.repo.server.plaintext' of config map 'argocd-cmd-params-cm'>                           Optional: true
      ARGOCD_SERVER_REPO_SERVER_STRICT_TLS:                           <set to the key 'server.repo.server.strict.tls' of config map 'argocd-cmd-params-cm'>                          Optional: true
      ARGOCD_SERVER_DEX_SERVER_PLAINTEXT:                             <set to the key 'server.dex.server.plaintext' of config map 'argocd-cmd-params-cm'>                            Optional: true
      ARGOCD_SERVER_DEX_SERVER_STRICT_TLS:                            <set to the key 'server.dex.server.strict.tls' of config map 'argocd-cmd-params-cm'>                           Optional: true
      ARGOCD_TLS_MIN_VERSION:                                         <set to the key 'server.tls.minversion' of config map 'argocd-cmd-params-cm'>                                  Optional: true
      ARGOCD_TLS_MAX_VERSION:                                         <set to the key 'server.tls.maxversion' of config map 'argocd-cmd-params-cm'>                                  Optional: true
      ARGOCD_TLS_CIPHERS:                                             <set to the key 'server.tls.ciphers' of config map 'argocd-cmd-params-cm'>                                     Optional: true
      ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION:               <set to the key 'server.connection.status.cache.expiration' of config map 'argocd-cmd-params-cm'>              Optional: true
      ARGOCD_SERVER_OIDC_CACHE_EXPIRATION:                            <set to the key 'server.oidc.cache.expiration' of config map 'argocd-cmd-params-cm'>                           Optional: true
      ARGOCD_SERVER_LOGIN_ATTEMPTS_EXPIRATION:                        <set to the key 'server.login.attempts.expiration' of config map 'argocd-cmd-params-cm'>                       Optional: true
      ARGOCD_SERVER_STATIC_ASSETS:                                    <set to the key 'server.staticassets' of config map 'argocd-cmd-params-cm'>                                    Optional: true
      ARGOCD_APP_STATE_CACHE_EXPIRATION:                              <set to the key 'server.app.state.cache.expiration' of config map 'argocd-cmd-params-cm'>                      Optional: true
      REDIS_SERVER:                                                   <set to the key 'redis.server' of config map 'argocd-cmd-params-cm'>                                           Optional: true
      REDIS_COMPRESSION:                                              <set to the key 'redis.compression' of config map 'argocd-cmd-params-cm'>                                      Optional: true
      REDISDB:                                                        <set to the key 'redis.db' of config map 'argocd-cmd-params-cm'>                                               Optional: true
      REDIS_USERNAME:                                                 <set to the key 'redis-username' in secret 'argocd-redis'>                                                     Optional: true
      REDIS_PASSWORD:                                                 <set to the key 'auth' in secret 'argocd-redis'>                                                               Optional: false
      REDIS_SENTINEL_USERNAME:                                        <set to the key 'redis-sentinel-username' in secret 'argocd-redis'>                                            Optional: true
      REDIS_SENTINEL_PASSWORD:                                        <set to the key 'redis-sentinel-password' in secret 'argocd-redis'>                                            Optional: true
      ARGOCD_DEFAULT_CACHE_EXPIRATION:                                <set to the key 'server.default.cache.expiration' of config map 'argocd-cmd-params-cm'>                        Optional: true
      ARGOCD_MAX_COOKIE_NUMBER:                                       <set to the key 'server.http.cookie.maxnumber' of config map 'argocd-cmd-params-cm'>                           Optional: true
      ARGOCD_SERVER_LISTEN_ADDRESS:                                   <set to the key 'server.listen.address' of config map 'argocd-cmd-params-cm'>                                  Optional: true
      ARGOCD_SERVER_METRICS_LISTEN_ADDRESS:                           <set to the key 'server.metrics.listen.address' of config map 'argocd-cmd-params-cm'>                          Optional: true
      ARGOCD_SERVER_OTLP_ADDRESS:                                     <set to the key 'otlp.address' of config map 'argocd-cmd-params-cm'>                                           Optional: true
      ARGOCD_SERVER_OTLP_INSECURE:                                    <set to the key 'otlp.insecure' of config map 'argocd-cmd-params-cm'>                                          Optional: true
      ARGOCD_SERVER_OTLP_HEADERS:                                     <set to the key 'otlp.headers' of config map 'argocd-cmd-params-cm'>                                           Optional: true
      ARGOCD_SERVER_OTLP_ATTRS:                                       <set to the key 'otlp.attrs' of config map 'argocd-cmd-params-cm'>                                             Optional: true
      ARGOCD_APPLICATION_NAMESPACES:                                  <set to the key 'application.namespaces' of config map 'argocd-cmd-params-cm'>                                 Optional: true
      ARGOCD_SERVER_ENABLE_PROXY_EXTENSION:                           <set to the key 'server.enable.proxy.extension' of config map 'argocd-cmd-params-cm'>                          Optional: true
      ARGOCD_K8SCLIENT_RETRY_MAX:                                     <set to the key 'server.k8sclient.retry.max' of config map 'argocd-cmd-params-cm'>                             Optional: true
      ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF:                            <set to the key 'server.k8sclient.retry.base.backoff' of config map 'argocd-cmd-params-cm'>                    Optional: true
      ARGOCD_API_CONTENT_TYPES:                                       <set to the key 'server.api.content.types' of config map 'argocd-cmd-params-cm'>                               Optional: true
      ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT:                        <set to the key 'server.webhook.parallelism.limit' of config map 'argocd-cmd-params-cm'>                       Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING:  <set to the key 'applicationsetcontroller.enable.new.git.file.globbing' of config map 'argocd-cmd-params-cm'>  Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH:              <set to the key 'applicationsetcontroller.scm.root.ca.path' of config map 'argocd-cmd-params-cm'>              Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS:         <set to the key 'applicationsetcontroller.allowed.scm.providers' of config map 'argocd-cmd-params-cm'>         Optional: true
      ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS:          <set to the key 'applicationsetcontroller.enable.scm.providers' of config map 'argocd-cmd-params-cm'>          Optional: true
      ARGOCD_HYDRATOR_ENABLED:                                        <set to the key 'hydrator.enabled' of config map 'argocd-cmd-params-cm'>                                       Optional: true
      ARGOCD_SYNC_WITH_REPLACE_ALLOWED:                               <set to the key 'server.sync.replace.allowed' of config map 'argocd-cmd-params-cm'>                            Optional: true
    Mounts:
      /app/config/dex/tls from argocd-dex-server-tls (rw)
      /app/config/server/tls from argocd-repo-server-tls (rw)
      /app/config/ssh from ssh-known-hosts (rw)
      /app/config/tls from tls-certs (rw)
      /etc/ssl/certs/ca-certificates.crt from tls-from-node (rw)
      /etc/ssl/certs/gitea_cert.crt from gitea-tls (rw)
      /home/argocd from plugins-home (rw)
      /home/argocd/params from argocd-cmd-params-cm (rw)
      /shared/app/custom from styles (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pp6pp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tls-from-node:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs/ca-certificates.crt
    HostPathType:  
  gitea-tls:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates/gitea_cert.crt
    HostPathType:  
  plugins-home:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  ssh-known-hosts:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-ssh-known-hosts-cm
    Optional:  false
  tls-certs:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-tls-certs-cm
    Optional:  false
  styles:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-styles-cm
    Optional:  true
  argocd-repo-server-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  argocd-repo-server-tls
    Optional:    true
  argocd-dex-server-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  argocd-dex-server-tls
    Optional:    true
  argocd-cmd-params-cm:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      argocd-cmd-params-cm
    Optional:  true
  kube-api-access-pp6pp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age                From          Message
  ----     ------           ----               ----          -------
  Warning  PolicyViolation  13m                kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: restricted volume types, field error list: [spec.volumes[0].hostPath: Forbidden, spec.volumes[1].hostPath: Forbidden])(Forbidden reason: hostPath volumes, field error list: [spec.volumes[0].hostPath is forbidden, forbidden values found: /etc/ssl/certs/ca-certificates.crt, spec.volumes[1].hostPath is forbidden, forbidden values found: /usr/local/share/ca-certificates/gitea_cert.crt])
  Warning  Unhealthy        52m                kubelet       Readiness probe failed: Get "https://10.42.65.81:8080/healthz": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy        52m                kubelet       Readiness probe failed: Get "http://10.42.65.81:8080/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy        51m (x2 over 52m)  kubelet       Liveness probe failed: Get "https://10.42.65.81:8080/healthz?full=true": context deadline exceeded


Name:                 calico-kube-controllers-6446ff4c44-2gts8
Namespace:            calico-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      calico-kube-controllers
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:23:42 +0000
Labels:               app.kubernetes.io/name=calico-kube-controllers
                      k8s-app=calico-kube-controllers
                      pod-template-hash=6446ff4c44
Annotations:          cni.projectcalico.org/containerID: d4e827c1be822c2455075e4c5668db2af908de28e4d77c1a53aed31312da6cb3
                      cni.projectcalico.org/podIP: 10.42.65.70/32
                      cni.projectcalico.org/podIPs: 10.42.65.70/32
                      hash.operator.tigera.io/system: afea2595203eb027afcee25a2f11a6666a8de557
                      tigera-operator.hash.operator.tigera.io/tigera-ca-private: b62c731ab99e34a11242c0ba9e8ab85885b27353
Status:               Running
IP:                   10.42.65.70
IPs:
  IP:           10.42.65.70
Controlled By:  ReplicaSet/calico-kube-controllers-6446ff4c44
Containers:
  calico-kube-controllers:
    Container ID:    containerd://17de0dadfdd57d8020a7c3297c857eee461fc9745f9088a2ac9608d4a4179be4
    Image:           docker.io/rancher/mirrored-calico-kube-controllers:v3.30.3
    Image ID:        sha256:df191a54fb79de3c693f8b1b864a1bd3bd14f63b3fff9d5fa4869c471ce3cd37
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:23:58 +0000
    Ready:           True
    Restart Count:   0
    Liveness:        exec [/usr/bin/check-status -l] delay=10s timeout=10s period=60s #success=1 #failure=6
    Readiness:       exec [/usr/bin/check-status -r] delay=0s timeout=10s period=30s #success=1 #failure=3
    Environment:
      KUBE_CONTROLLERS_CONFIG_NAME:         default
      DATASTORE_TYPE:                       kubernetes
      ENABLED_CONTROLLERS:                  node,loadbalancer
      DISABLE_KUBE_CONTROLLERS_CONFIG_API:  false
      KUBERNETES_SERVICE_HOST:              10.43.0.1
      KUBERNETES_SERVICE_PORT:              443
      CA_CRT_PATH:                          /etc/pki/tls/certs/tigera-ca-bundle.crt
    Mounts:
      /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
      /etc/pki/tls/certs from tigera-ca-bundle (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w4x24 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tigera-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tigera-ca-bundle
    Optional:  false
  kube-api-access-w4x24:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy require-ro-rootfs/validate-readOnlyRootFilesystem fail: validation error: Root filesystem must be read-only. rule validate-readOnlyRootFilesystem failed at path /spec/containers/0/securityContext/readOnlyRootFilesystem/


Name:                 calico-node-8r5dx
Namespace:            calico-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      calico-node
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:23:40 +0000
Labels:               app.kubernetes.io/name=calico-node
                      controller-revision-hash=5c4bddf576
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          hash.operator.tigera.io/cni-config: d09f3e23480512609bae00abb3ca3c9b9d408e59
                      hash.operator.tigera.io/system: afea2595203eb027afcee25a2f11a6666a8de557
                      tigera-operator.hash.operator.tigera.io/tigera-ca-private: b62c731ab99e34a11242c0ba9e8ab85885b27353
Status:               Running
IP:                   192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  DaemonSet/calico-node
Init Containers:
  flexvol-driver:
    Container ID:    containerd://61c218863a51502a6bcdae14c23f90b4254eda56b733ea7c68b8e0ab43954c3a
    Image:           docker.io/rancher/mirrored-calico-pod2daemon-flexvol:v3.30.3
    Image ID:        sha256:4f2b088ed6fdfc6a97ac0650a4ba8171107d6656ce265c592e4c8423fd10e5c4
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Terminated
      Reason:        Completed
      Exit Code:     0
      Started:       Tue, 03 Feb 2026 17:23:41 +0000
      Finished:      Tue, 03 Feb 2026 17:23:41 +0000
    Ready:           True
    Restart Count:   0
    Environment:     <none>
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6j5f (ro)
  install-cni:
    Container ID:    containerd://8fa875813e77d1ea404cc6c93a86ee2fae5c2e78faeb0cb1ce966c312ad6edbe
    Image:           docker.io/rancher/mirrored-calico-cni:v3.30.3
    Image ID:        sha256:034822460c2f667e1f4a7679c843cc35ce1bf2c25dec86f04e07fb403df7e458
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /opt/cni/bin/install
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:23:42 +0000
      Finished:     Tue, 03 Feb 2026 17:23:42 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:            10-calico.conflist
      SLEEP:                    false
      CNI_NET_DIR:              /etc/cni/net.d
      CNI_NETWORK_CONFIG:       <set to the key 'config' of config map 'cni-config'>  Optional: false
      KUBERNETES_SERVICE_HOST:  10.43.0.1
      KUBERNETES_SERVICE_PORT:  443
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6j5f (ro)
Containers:
  calico-node:
    Container ID:    containerd://d56e5916cc8e43e1f8334b57753a7d9cb3851d3156f19dab73c15fd927000fba
    Image:           docker.io/rancher/mirrored-calico-node:v3.30.3
    Image ID:        sha256:ce9c4ac0f175f22c56e80844e65379d9ebe1d8a4e2bbb38dc1db0f53a8826f0f
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:23:44 +0000
    Ready:           True
    Restart Count:   0
    Liveness:        http-get http://localhost:9099/liveness delay=0s timeout=10s period=60s #success=1 #failure=3
    Readiness:       exec [/bin/calico-node -felix-ready] delay=0s timeout=10s period=30s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      WAIT_FOR_DATASTORE:                 true
      CLUSTER_TYPE:                       k8s,operator
      CALICO_DISABLE_FILE_LOGGING:        false
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_HEALTHENABLED:                true
      FELIX_HEALTHPORT:                   9099
      NODENAME:                            (v1:spec.nodeName)
      NAMESPACE:                          calico-system (v1:metadata.namespace)
      FELIX_TYPHAK8SNAMESPACE:            calico-system
      FELIX_TYPHAK8SSERVICENAME:          calico-typha
      FELIX_TYPHACAFILE:                  /etc/pki/tls/certs/tigera-ca-bundle.crt
      FELIX_TYPHACERTFILE:                /node-certs/tls.crt
      FELIX_TYPHAKEYFILE:                 /node-certs/tls.key
      NO_DEFAULT_POOLS:                   true
      FELIX_TYPHACN:                      typha-server
      CALICO_MANAGE_CNI:                  true
      CALICO_NETWORKING_BACKEND:          vxlan
      IP:                                 autodetect
      IP_AUTODETECTION_METHOD:            first-found
      IP6:                                none
      FELIX_IPV6SUPPORT:                  false
      KUBERNETES_SERVICE_HOST:            10.43.0.1
      KUBERNETES_SERVICE_PORT:            443
    Mounts:
      /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
      /etc/pki/tls/certs from tigera-ca-bundle (ro)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /lib/modules from lib-modules (ro)
      /node-certs from node-certs (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/log/calico/cni from cni-log-dir (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6j5f (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  DirectoryOrCreate
  cni-log-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/log/calico/cni
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/kubelet/volumeplugins/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  node-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-certs
    Optional:    false
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  tigera-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tigera-ca-bundle
    Optional:  false
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  DirectoryOrCreate
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  DirectoryOrCreate
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  kube-api-access-p6j5f:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 :NoSchedule op=Exists
                             :NoExecute op=Exists
                             CriticalAddonsOnly op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: runAsUser=0, field error list: [spec.initContainers[0].securityContext.runAsUser is forbidden, forbidden values found: 0, spec.initContainers[1].securityContext.runAsUser is forbidden, forbidden values found: 0, spec.containers[0].securityContext.runAsUser is forbidden, forbidden values found: 0])(Forbidden reason: allowPrivilegeEscalation != false, field error list: [spec.initContainers[0].securityContext.allowPrivilegeEscalation is forbidden, forbidden values found: true, spec.initContainers[1].securityContext.allowPrivilegeEscalation is forbidden, forbidden values found: true, spec.containers[0].securityContext.allowPrivilegeEscalation is forbidden, forbidden values found: true])(Forbidden reason: host namespaces, field error list: [spec.hostNetwork is forbidden, forbidden values found: true])(Forbidden reason: hostPath volumes, field...
  Warning  PolicyViolation  13m   kyverno-scan  policy require-ro-rootfs/validate-readOnlyRootFilesystem fail: validation error: Root filesystem must be read-only. rule validate-readOnlyRootFilesystem failed at path /spec/containers/0/securityContext/readOnlyRootFilesystem/


Name:                 calico-typha-5f876897c6-f459x
Namespace:            calico-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      calico-typha
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:23:42 +0000
Labels:               app.kubernetes.io/name=calico-typha
                      k8s-app=calico-typha
                      pod-template-hash=5f876897c6
Annotations:          hash.operator.tigera.io/system: afea2595203eb027afcee25a2f11a6666a8de557
                      tigera-operator.hash.operator.tigera.io/tigera-ca-private: b62c731ab99e34a11242c0ba9e8ab85885b27353
                      tigera-operator.hash.operator.tigera.io/typha-certs: e63ce634ac735a8230ca7630b5c209b2cc59a813
Status:               Running
IP:                   192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  ReplicaSet/calico-typha-5f876897c6
Containers:
  calico-typha:
    Container ID:    containerd://ab5e998374df7b0d25f09e198945bc114a1588cb843f78450ba843de2196237c
    Image:           docker.io/rancher/mirrored-calico-typha:v3.30.3
    Image ID:        sha256:1d7bb7b0cce2924d35c7c26f6b6600409ea7c9535074c3d2e517ffbb3a0e0b36
    Port:            5473/TCP
    Host Port:       5473/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:23:43 +0000
    Ready:           True
    Restart Count:   0
    Liveness:        http-get http://localhost:9098/liveness delay=0s timeout=10s period=60s #success=1 #failure=3
    Readiness:       http-get http://localhost:9098/readiness delay=0s timeout=10s period=30s #success=1 #failure=3
    Environment:
      TYPHA_LOGSEVERITYSCREEN:          info
      TYPHA_LOGFILEPATH:                none
      TYPHA_LOGSEVERITYSYS:             none
      TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
      TYPHA_DATASTORETYPE:              kubernetes
      TYPHA_HEALTHENABLED:              true
      TYPHA_HEALTHPORT:                 9098
      TYPHA_K8SNAMESPACE:               calico-system
      TYPHA_CAFILE:                     /etc/pki/tls/certs/tigera-ca-bundle.crt
      TYPHA_SERVERCERTFILE:             /typha-certs/tls.crt
      TYPHA_SERVERKEYFILE:              /typha-certs/tls.key
      TYPHA_SHUTDOWNTIMEOUTSECS:        300
      TYPHA_CLIENTCN:                   typha-client
      KUBERNETES_SERVICE_HOST:          10.43.0.1
      KUBERNETES_SERVICE_PORT:          443
    Mounts:
      /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
      /etc/pki/tls/certs from tigera-ca-bundle (ro)
      /typha-certs from typha-certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sk4ds (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tigera-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tigera-ca-bundle
    Optional:  false
  typha-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  typha-certs
    Optional:    false
  kube-api-access-sk4ds:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy require-ro-rootfs/validate-readOnlyRootFilesystem fail: validation error: Root filesystem must be read-only. rule validate-readOnlyRootFilesystem failed at path /spec/containers/0/securityContext/readOnlyRootFilesystem/
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: host namespaces, field error list: [spec.hostNetwork is forbidden, forbidden values found: true])(Forbidden reason: hostPort, field error list: [spec.containers[0].ports[0].hostPort is forbidden, forbidden values found: 5473])(Forbidden reason: probe or lifecycle host, field error list: [])


Name:             capi-operator-cluster-api-operator-6cd7579764-msrk6
Namespace:        capi-operator-system
Priority:         0
Service Account:  capi-operator-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app=cluster-api-operator
                  app.kubernetes.io/component=controller
                  app.kubernetes.io/instance=capi-operator
                  app.kubernetes.io/name=cluster-api-operator
                  clusterctl.cluster.x-k8s.io/core=capi-operator
                  control-plane=controller-manager
                  pod-template-hash=6cd7579764
Annotations:      cni.projectcalico.org/containerID: e8110aef32402a1380ef99df2d2c3d05f5ff2f3b8b8c304b819e6dbdf2083ebc
                  cni.projectcalico.org/podIP: 10.42.65.133/32
                  cni.projectcalico.org/podIPs: 10.42.65.133/32
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.133
IPs:
  IP:           10.42.65.133
Controlled By:  ReplicaSet/capi-operator-cluster-api-operator-6cd7579764
Containers:
  manager:
    Container ID:  containerd://124c3ae020732faa7a53faf8f5710d1b9c959cf8bb4b3e4237b32867baf70246
    Image:         registry.k8s.io/capi-operator/cluster-api-operator:v0.23.0
    Image ID:      registry.k8s.io/capi-operator/cluster-api-operator@sha256:987f1179626ba802a558204d4a3a374a8a357ae747d171d4a3d052834774c114
    Ports:         6060/TCP, 9443/TCP, 8080/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Command:
      /manager
    Args:
      --v=2
      --health-addr=:9440
      --diagnostics-address=:8080
      --insecure-diagnostics=true
      --leader-elect=true
      --profiler-address=localhost:6060
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:9440/healthz delay=15s timeout=1s period=20s #success=1 #failure=3
    Readiness:    http-get http://:9440/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp/k8s-webhook-server/serving-certs from cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89b9z (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  capi-operator-webhook-service-cert
    Optional:    false
  kube-api-access-89b9z:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             capi-controller-manager-6f9cb69f7d-kj89x
Namespace:        capi-system
Priority:         0
Service Account:  capi-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:38 +0000
Labels:           cluster.x-k8s.io/provider=cluster-api
                  control-plane=controller-manager
                  pod-template-hash=6f9cb69f7d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=capi-controller-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: b4ebadbc384faca390a26682faa659fcf20bef12d47d1ec63d9f993ef67b3f8e
                  cni.projectcalico.org/podIP: 10.42.65.179/32
                  cni.projectcalico.org/podIPs: 10.42.65.179/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: manager
                  kubectl.kubernetes.io/default-logs-container: manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.179
IPs:
  IP:           10.42.65.179
Controlled By:  ReplicaSet/capi-controller-manager-6f9cb69f7d
Init Containers:
  istio-init:
    Container ID:  containerd://e944d39c24c02ce3639b13c4872081155c6c1c8ef00867464e49f2214fbb8f92
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:39:40 +0000
      Finished:     Tue, 03 Feb 2026 17:39:40 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2msdg (ro)
  istio-proxy:
    Container ID:  containerd://06363af99d28e46718ebae8b8feb7f26f49ccba51e50bf8416c451891dd2fd5a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:41 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      capi-controller-manager-6f9cb69f7d-kj89x (v1:metadata.name)
      POD_NAMESPACE:                 capi-system (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"webhook-server","containerPort":9443,"protocol":"TCP"}
                                         ,{"name":"healthz","containerPort":9440,"protocol":"TCP"}
                                         ,{"name":"metrics","containerPort":8443,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      capi-controller-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/capi-system/deployments/capi-controller-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/manager/livez":{"httpGet":{"path":"/healthz","port":9440,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/manager/readyz":{"httpGet":{"path":"/readyz","port":9440,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2msdg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  manager:
    Container ID:  containerd://a2e956ea52bd3d8688421d1b29bb947728715c418c4f3d0ec70f60c56630d1f2
    Image:         registry.k8s.io/cluster-api/cluster-api-controller:v1.10.7
    Image ID:      registry.k8s.io/cluster-api/cluster-api-controller@sha256:3cda2bb5f363f60eb5a6b6d84d047c2afd1cce547e6b4fea445d590fee39779d
    Ports:         9443/TCP, 9440/TCP, 8443/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Command:
      /manager
    Args:
      --leader-elect
      --diagnostics-address=:8443
      --insecure-diagnostics=true
      --feature-gates=ClusterResourceSet=true,ClusterTopology=true,MachinePool=true,MachineSetPreflightChecks=true,MachineWaitForVolumeDetachConsiderVolumeAttachments=true,RuntimeSDK=false
      --kube-api-burst=150
      --kube-api-qps=100
      --machine-concurrency=10
      --additional-sync-machine-labels=.*
      --cluster-concurrency=10
      --clustercache-client-burst=150
      --clustercache-client-qps=100
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:49 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:15020/app-health/manager/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:15020/app-health/manager/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAMESPACE:  capi-system (v1:metadata.namespace)
      POD_NAME:       capi-controller-manager-6f9cb69f7d-kj89x (v1:metadata.name)
      POD_UID:         (v1:metadata.uid)
    Mounts:
      /tmp/k8s-webhook-server/serving-certs from cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2msdg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  capi-webhook-service-cert
    Optional:    false
  kube-api-access-2msdg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             capi-k3s-bootstrap-controller-manager-58cc88cb6-9xzz2
Namespace:        capk-system
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:51 +0000
Labels:           cluster.x-k8s.io/provider=bootstrap-k3s
                  control-plane=controller-manager
                  pod-template-hash=58cc88cb6
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=capi-k3s-bootstrap-controller-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: eac98e97ea1af5522b60cc0b79445c2aca0e739d2d2225027415acaecef10396
                  cni.projectcalico.org/podIP: 10.42.65.184/32
                  cni.projectcalico.org/podIPs: 10.42.65.184/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: manager
                  kubectl.kubernetes.io/default-logs-container: manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.184
IPs:
  IP:           10.42.65.184
Controlled By:  ReplicaSet/capi-k3s-bootstrap-controller-manager-58cc88cb6
Init Containers:
  istio-init:
    Container ID:  containerd://8b9df06b6a2a25c3054e875e26ccd1b6a6058567bf09c61d3a6416bbc2865fd0
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:39:53 +0000
      Finished:     Tue, 03 Feb 2026 17:39:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhslx (ro)
  istio-proxy:
    Container ID:  containerd://c86c8da79f14ee28606b764c5df69bd674dddbf2f46c257d4afa03e25ac185b8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      capi-k3s-bootstrap-controller-manager-58cc88cb6-9xzz2 (v1:metadata.name)
      POD_NAMESPACE:                 capk-system (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"webhook-server","containerPort":9443,"protocol":"TCP"}
                                         ,{"name":"https","containerPort":8443,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     manager,kube-rbac-proxy
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      capi-k3s-bootstrap-controller-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/capk-system/deployments/capi-k3s-bootstrap-controller-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhslx (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  manager:
    Container ID:    containerd://e2a31999d73c75ee91e16f6f630008e928fcca6ea55897aae336cab5dcd8943f
    Image:           ghcr.io/k3s-io/cluster-api-k3s/bootstrap-controller:v0.3.0
    Image ID:        ghcr.io/k3s-io/cluster-api-k3s/bootstrap-controller@sha256:5e382ab351f2c583ca00cdb18d307bc8e1e4f2cc8c457ff6a99ed524d4c7a95e
    Port:            9443/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /manager
    Args:
      --metrics-addr=127.0.0.1:8080
      --enable-leader-election
    State:          Running
      Started:      Tue, 03 Feb 2026 17:40:03 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /tmp/k8s-webhook-server/serving-certs from cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhslx (ro)
  kube-rbac-proxy:
    Container ID:    containerd://bb52814386abf4531a3fe70fd305e32130f42f034930a3b45987a5d1a78777c0
    Image:           gcr.io/kubebuilder/kube-rbac-proxy:v0.16.0
    Image ID:        gcr.io/kubebuilder/kube-rbac-proxy@sha256:771a9a173e033a3ad8b46f5c00a7036eaa88c8d8d1fbd89217325168998113ea
    Port:            8443/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --secure-listen-address=0.0.0.0:8443
      --upstream=http://127.0.0.1:8080/
      --v=10
    State:          Running
      Started:      Tue, 03 Feb 2026 17:40:06 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhslx (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  capi-k3s-bootstrap-webhook-service-cert
    Optional:    false
  kube-api-access-jhslx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             capi-k3s-control-plane-controller-manager-64465bf4fc-lkm25
Namespace:        capk-system
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:51 +0000
Labels:           cluster.x-k8s.io/provider=control-plane-k3s
                  control-plane=controller-manager
                  pod-template-hash=64465bf4fc
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=capi-k3s-control-plane-controller-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: fa566f56fc19b3f1307715c20f7bbd21e761079fe4aa5178fa6eda6e0363f76b
                  cni.projectcalico.org/podIP: 10.42.65.146/32
                  cni.projectcalico.org/podIPs: 10.42.65.146/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: manager
                  kubectl.kubernetes.io/default-logs-container: manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.146
IPs:
  IP:           10.42.65.146
Controlled By:  ReplicaSet/capi-k3s-control-plane-controller-manager-64465bf4fc
Init Containers:
  istio-init:
    Container ID:  containerd://11ab9098dc82b2e52d420bd264bef72a8e2e917c4592b44259acb536e9655774
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:39:54 +0000
      Finished:     Tue, 03 Feb 2026 17:39:54 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zbhz5 (ro)
  istio-proxy:
    Container ID:  containerd://8a1f99b1167ad68f694506e8ad6f18d55c86f45bd73d2a8699cbc3c332718e95
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      capi-k3s-control-plane-controller-manager-64465bf4fc-lkm25 (v1:metadata.name)
      POD_NAMESPACE:                 capk-system (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"webhook-server","containerPort":9443,"protocol":"TCP"}
                                         ,{"name":"https","containerPort":8443,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     manager,kube-rbac-proxy
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      capi-k3s-control-plane-controller-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/capk-system/deployments/capi-k3s-control-plane-controller-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zbhz5 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  manager:
    Container ID:    containerd://39ad2fc01a12cb52a662f8e88ade5f0c4b29294989f1cbc9bf658e54246e87b7
    Image:           ghcr.io/k3s-io/cluster-api-k3s/controlplane-controller:v0.3.0
    Image ID:        ghcr.io/k3s-io/cluster-api-k3s/controlplane-controller@sha256:e2424f9707583aaf5f761d5302b9e807c8d3ee318fe9d8e6b49e6c8beea5b492
    Port:            9443/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /manager
    Args:
      --metrics-addr=127.0.0.1:8080
      --enable-leader-election
    State:          Running
      Started:      Tue, 03 Feb 2026 17:40:03 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /tmp/k8s-webhook-server/serving-certs from cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zbhz5 (ro)
  kube-rbac-proxy:
    Container ID:    containerd://8f9baf0ac64f567aa50e8f04d2f598e3eb995854cc6c81ee457dfc0085e68018
    Image:           gcr.io/kubebuilder/kube-rbac-proxy:v0.16.0
    Image ID:        gcr.io/kubebuilder/kube-rbac-proxy@sha256:771a9a173e033a3ad8b46f5c00a7036eaa88c8d8d1fbd89217325168998113ea
    Port:            8443/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --secure-listen-address=0.0.0.0:8443
      --upstream=http://127.0.0.1:8080/
      --v=10
    State:          Running
      Started:      Tue, 03 Feb 2026 17:40:06 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zbhz5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  capi-k3s-control-plane-webhook-service-cert
    Optional:    false
  kube-api-access-zbhz5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             fleet-agent-6d676744c-lwjwh
Namespace:        cattle-fleet-system
Priority:         0
Service Account:  fleet-agent
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:25 +0000
Labels:           app=fleet-agent
                  pod-template-hash=6d676744c
Annotations:      cni.projectcalico.org/containerID: a48067c0d35aa5526e2cd12fc52ead8dfd9ef9ec2566179d0c8475e167816040
                  cni.projectcalico.org/podIP: 10.42.65.174/32
                  cni.projectcalico.org/podIPs: 10.42.65.174/32
Status:           Running
IP:               10.42.65.174
IPs:
  IP:           10.42.65.174
Controlled By:  ReplicaSet/fleet-agent-6d676744c
Containers:
  fleet-agent:
    Container ID:  containerd://e6d530e34ef087d03b13c7cb157412c8e2fedb281caef39b51d1690883027b55
    Image:         rancher/fleet-agent:v0.12.8
    Image ID:      docker.io/rancher/fleet-agent@sha256:211a9d97566f2d1e21be4abfdb334e791b8ee3b60f410aa1832af51b9abdac24
    Port:          <none>
    Host Port:     <none>
    Command:
      fleetagent
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:29 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      BUNDLEDEPLOYMENT_RECONCILER_WORKERS:  50
      DRIFT_RECONCILER_WORKERS:             50
      NAMESPACE:                            cattle-fleet-system (v1:metadata.namespace)
      AGENT_SCOPE:                          
      CHECKIN_INTERVAL:                     15m0s
      CATTLE_ELECTION_LEASE_DURATION:       30s
      CATTLE_ELECTION_RETRY_PERIOD:         10s
      CATTLE_ELECTION_RENEW_DEADLINE:       25s
    Mounts:
      /.kube from kube (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bb75s (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-bb75s:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 cattle.io/os=linux:NoSchedule
                             cattle.io/os=linux:NoSchedule
                             node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             fleet-controller-7fffb9b79c-fmbzc
Namespace:        cattle-fleet-system
Priority:         0
Service Account:  fleet-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:08 +0000
Labels:           app=fleet-controller
                  fleet.cattle.io/shard-default=true
                  fleet.cattle.io/shard-id=
                  pod-template-hash=7fffb9b79c
Annotations:      cni.projectcalico.org/containerID: e2686a6889b4ccff3ce576a44092cfff3f9b6ca82d93a401808d1219c28ae055
                  cni.projectcalico.org/podIP: 10.42.65.167/32
                  cni.projectcalico.org/podIPs: 10.42.65.167/32
Status:           Running
IP:               10.42.65.167
IPs:
  IP:           10.42.65.167
Controlled By:  ReplicaSet/fleet-controller-7fffb9b79c
Containers:
  fleet-controller:
    Container ID:  containerd://1a7ba2aacf458c29365751c6ee05334d1fc376f4a38f4f46e78700426d38d9a5
    Image:         rancher/fleet:v0.12.8
    Image ID:      docker.io/rancher/fleet@sha256:513fa8fd7f08df5892e228bd214c821dc6df29855e31ff2a1b142e4d21b9452c
    Port:          8080/TCP
    Host Port:     0/TCP
    Command:
      fleetcontroller
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:18 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NAMESPACE:                            cattle-fleet-system (v1:metadata.namespace)
      CATTLE_ELECTION_LEASE_DURATION:       30s
      CATTLE_ELECTION_RETRY_PERIOD:         10s
      CATTLE_ELECTION_RENEW_DEADLINE:       25s
      BUNDLE_RECONCILER_WORKERS:            50
      BUNDLEDEPLOYMENT_RECONCILER_WORKERS:  50
      CLUSTER_RECONCILER_WORKERS:           50
      CLUSTERGROUP_RECONCILER_WORKERS:      50
      IMAGESCAN_RECONCILER_WORKERS:         50
    Mounts:
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v2r5w (ro)
  fleet-cleanup:
    Container ID:  containerd://8d1629bbd807a162809cde2c7deeb12432e942f9650bec18d8ccef06068fd2a3
    Image:         rancher/fleet:v0.12.8
    Image ID:      docker.io/rancher/fleet@sha256:513fa8fd7f08df5892e228bd214c821dc6df29855e31ff2a1b142e4d21b9452c
    Port:          <none>
    Host Port:     <none>
    Command:
      fleetcontroller
      cleanup
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:18 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NAMESPACE:                       cattle-fleet-system (v1:metadata.namespace)
      CATTLE_ELECTION_LEASE_DURATION:  30s
      CATTLE_ELECTION_RETRY_PERIOD:    10s
      CATTLE_ELECTION_RENEW_DEADLINE:  25s
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v2r5w (ro)
  fleet-agentmanagement:
    Container ID:  containerd://e06b727ac1d12e33c61c3f551d10a6652925be26b7cb8579ae6d62f797893e38
    Image:         rancher/fleet:v0.12.8
    Image ID:      docker.io/rancher/fleet@sha256:513fa8fd7f08df5892e228bd214c821dc6df29855e31ff2a1b142e4d21b9452c
    Port:          <none>
    Host Port:     <none>
    Command:
      fleetcontroller
      agentmanagement
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:18 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NAMESPACE:                                 cattle-fleet-system (v1:metadata.namespace)
      FLEET_PROPAGATE_DEBUG_SETTINGS_TO_AGENTS:  true
      FLEET_DEBUG_DISABLE_SECURITY_CONTEXT:      false
      CATTLE_ELECTION_LEASE_DURATION:            30s
      CATTLE_ELECTION_RETRY_PERIOD:              10s
      CATTLE_ELECTION_RENEW_DEADLINE:            25s
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v2r5w (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-v2r5w:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 cattle.io/os=linux:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             gitjob-8bf95c49-xjkws
Namespace:        cattle-fleet-system
Priority:         0
Service Account:  gitjob
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:08 +0000
Labels:           app=gitjob
                  fleet.cattle.io/shard-default=true
                  fleet.cattle.io/shard-id=
                  pod-template-hash=8bf95c49
Annotations:      cni.projectcalico.org/containerID: c8934c12e640ffa898c8516bf9fe02c97858241926703d0f21cf1146b70df073
                  cni.projectcalico.org/podIP: 10.42.65.168/32
                  cni.projectcalico.org/podIPs: 10.42.65.168/32
Status:           Running
IP:               10.42.65.168
IPs:
  IP:           10.42.65.168
Controlled By:  ReplicaSet/gitjob-8bf95c49
Containers:
  gitjob:
    Container ID:  containerd://29512b92ba95439b6ce7aa9c33a086f019eb5620b248a8f7cdcce4039943e032
    Image:         rancher/fleet:v0.12.8
    Image ID:      docker.io/rancher/fleet@sha256:513fa8fd7f08df5892e228bd214c821dc6df29855e31ff2a1b142e4d21b9452c
    Port:          8081/TCP
    Host Port:     0/TCP
    Args:
      fleetcontroller
      gitjob
      --gitjob-image
      rancher/fleet:v0.12.8
      --insecure-skip-host-key-checks
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:18 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NAMESPACE:                       cattle-fleet-system (v1:metadata.namespace)
      CATTLE_ELECTION_LEASE_DURATION:  30s
      CATTLE_ELECTION_RETRY_PERIOD:    10s
      CATTLE_ELECTION_RENEW_DEADLINE:  25s
      GITREPO_SYNC_PERIOD:             2h
      GITREPO_RECONCILER_WORKERS:      50
    Mounts:
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-npvvn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-npvvn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 cattle.io/os=linux:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             cert-manager-57bd5758d6-mrqvx
Namespace:        cert-manager
Priority:         0
Service Account:  cert-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:07 +0000
Labels:           app=cert-manager
                  app.kubernetes.io/component=controller
                  app.kubernetes.io/instance=cert-manager
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=cert-manager
                  app.kubernetes.io/version=v1.17.4
                  helm.sh/chart=cert-manager-v1.17.4
                  pod-template-hash=57bd5758d6
Annotations:      cni.projectcalico.org/containerID: 212ca3bc1fa5c84fd4499a5fb1e4254dfb5d96bbfdbed5c4d5ddb03787b9717c
                  cni.projectcalico.org/podIP: 10.42.65.91/32
                  cni.projectcalico.org/podIPs: 10.42.65.91/32
                  prometheus.io/path: /metrics
                  prometheus.io/port: 9402
                  prometheus.io/scrape: true
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.91
IPs:
  IP:           10.42.65.91
Controlled By:  ReplicaSet/cert-manager-57bd5758d6
Containers:
  cert-manager-controller:
    Container ID:  containerd://f3e36fdf4e99c904184f7d93a07cf94368ae53696f89c8b4a5f700fef101ccd1
    Image:         quay.io/jetstack/cert-manager-controller:v1.17.4
    Image ID:      quay.io/jetstack/cert-manager-controller@sha256:189920e5752ec7d56a6329347969d847bfd867c07119873c849f300795a0be7b
    Ports:         9402/TCP, 9403/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      --v=2
      --cluster-resource-namespace=$(POD_NAMESPACE)
      --leader-election-namespace=kube-system
      --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.17.4
      --max-concurrent-challenges=60
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Liveness:  http-get http://:http-healthz/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Environment:
      POD_NAMESPACE:  cert-manager (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cg957 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-cg957:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             cert-manager-cainjector-6b76b4c87f-lpd7g
Namespace:        cert-manager
Priority:         0
Service Account:  cert-manager-cainjector
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:07 +0000
Labels:           app=cainjector
                  app.kubernetes.io/component=cainjector
                  app.kubernetes.io/instance=cert-manager
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=cainjector
                  app.kubernetes.io/version=v1.17.4
                  helm.sh/chart=cert-manager-v1.17.4
                  pod-template-hash=6b76b4c87f
Annotations:      cni.projectcalico.org/containerID: f24d714771d08d5a94734b6180c160dbabc6d576c80e6c111cf4da6de18a20d7
                  cni.projectcalico.org/podIP: 10.42.65.92/32
                  cni.projectcalico.org/podIPs: 10.42.65.92/32
                  prometheus.io/path: /metrics
                  prometheus.io/port: 9402
                  prometheus.io/scrape: true
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.92
IPs:
  IP:           10.42.65.92
Controlled By:  ReplicaSet/cert-manager-cainjector-6b76b4c87f
Containers:
  cert-manager-cainjector:
    Container ID:  containerd://04ed559f7ccffec8e6edf1a87c5c13688a2e6f79a691babe6822b209b4a4b236
    Image:         quay.io/jetstack/cert-manager-cainjector:v1.17.4
    Image ID:      quay.io/jetstack/cert-manager-cainjector@sha256:0f7dcc7ca23dc95c32323ddd7b638c3fa318662985378360a1aeb9c72473e147
    Port:          9402/TCP
    Host Port:     0/TCP
    Args:
      --v=2
      --leader-election-namespace=kube-system
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      POD_NAMESPACE:  cert-manager (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qplct (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-qplct:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             cert-manager-webhook-696fcd97b-5jkvh
Namespace:        cert-manager
Priority:         0
Service Account:  cert-manager-webhook
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:07 +0000
Labels:           app=webhook
                  app.kubernetes.io/component=webhook
                  app.kubernetes.io/instance=cert-manager
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=webhook
                  app.kubernetes.io/version=v1.17.4
                  helm.sh/chart=cert-manager-v1.17.4
                  pod-template-hash=696fcd97b
Annotations:      cni.projectcalico.org/containerID: a397771ef8348a572b8befefcfec874f0623b3097b4400300159ceb0d7368810
                  cni.projectcalico.org/podIP: 10.42.65.93/32
                  cni.projectcalico.org/podIPs: 10.42.65.93/32
                  prometheus.io/path: /metrics
                  prometheus.io/port: 9402
                  prometheus.io/scrape: true
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.93
IPs:
  IP:           10.42.65.93
Controlled By:  ReplicaSet/cert-manager-webhook-696fcd97b
Containers:
  cert-manager-webhook:
    Container ID:  containerd://f9a80eb6bb06c752cdb77b8db1e2e1d6368c1913b46dd30c63b8c9c64f51bc6c
    Image:         quay.io/jetstack/cert-manager-webhook:v1.17.4
    Image ID:      quay.io/jetstack/cert-manager-webhook@sha256:1e4711c91ea4c45baab763bec3875528847f8221a577aa81b325cac49d68cd09
    Ports:         10250/TCP, 6080/TCP, 9402/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      --v=2
      --secure-port=10250
      --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      --dynamic-serving-dns-names=cert-manager-webhook
      --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:6080/livez delay=60s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:6080/healthz delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      POD_NAMESPACE:  cert-manager (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z9z24 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-z9z24:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             gitea-78d6db5997-jddnv
Namespace:        gitea
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:25:20 +0000
Labels:           app=gitea
                  app.kubernetes.io/instance=gitea
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=gitea
                  app.kubernetes.io/version=1.25.1
                  helm.sh/chart=gitea-10.4.0
                  pod-template-hash=78d6db5997
                  version=1.25.1
Annotations:      checksum/config: 4e4db1fd93d781a8466850533f67efe022db2c49dfb45a01f203928e04c7a27c
                  cni.projectcalico.org/containerID: dedb7edcb95a77dfec1a9e56c3748d3d589947c2d9e347ba15872fcda38e6fa9
                  cni.projectcalico.org/podIP: 10.42.65.75/32
                  cni.projectcalico.org/podIPs: 10.42.65.75/32
Status:           Running
IP:               10.42.65.75
IPs:
  IP:           10.42.65.75
Controlled By:  ReplicaSet/gitea-78d6db5997
Init Containers:
  init-directories:
    Container ID:    containerd://883d02c49d08f96a1a3cd53fa37a7b8b7a5b5fe334ec0689f60b145728f631a7
    Image:           docker.io/gitea/gitea:1.25.1-rootless
    Image ID:        docker.io/gitea/gitea@sha256:4b50ceba06096cda9a7611c79449af8c20c168d0554e8545200599434864d257
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/sbin/init_directory_structure.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:25:24 +0000
      Finished:     Tue, 03 Feb 2026 17:25:24 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      GITEA_APP_INI:   /data/gitea/conf/app.ini
      GITEA_CUSTOM:    /data/gitea
      GITEA_WORK_DIR:  /data
      GITEA_TEMP:      /tmp/gitea
    Mounts:
      /data from data (rw)
      /tmp from temp (rw)
      /usr/sbin from init (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-68ks6 (ro)
  init-app-ini:
    Container ID:    containerd://6c3df3a8347e4a5a86a2e629aefd956273e92cb8e15a04c8158d16575553a2d9
    Image:           docker.io/gitea/gitea:1.25.1-rootless
    Image ID:        docker.io/gitea/gitea@sha256:4b50ceba06096cda9a7611c79449af8c20c168d0554e8545200599434864d257
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/sbin/config_environment.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:25:25 +0000
      Finished:     Tue, 03 Feb 2026 17:25:25 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      GITEA_APP_INI:   /data/gitea/conf/app.ini
      GITEA_CUSTOM:    /data/gitea
      GITEA_WORK_DIR:  /data
      GITEA_TEMP:      /tmp/gitea
    Mounts:
      /data from data (rw)
      /env-to-ini-mounts/inlines/ from inline-config-sources (rw)
      /tmp from temp (rw)
      /usr/sbin from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-68ks6 (ro)
  configure-gitea:
    Container ID:    containerd://7172716f06fcf13ec1f84d94072669b31418fae007b52795910dcfb4318e53ed
    Image:           docker.io/gitea/gitea:1.25.1-rootless
    Image ID:        docker.io/gitea/gitea@sha256:4b50ceba06096cda9a7611c79449af8c20c168d0554e8545200599434864d257
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/sbin/configure_gitea.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:27:36 +0000
      Finished:     Tue, 03 Feb 2026 17:27:39 +0000
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      GITEA_APP_INI:              /data/gitea/conf/app.ini
      GITEA_CUSTOM:               /data/gitea
      GITEA_WORK_DIR:             /data
      GITEA_TEMP:                 /tmp/gitea
      HOME:                       /data/gitea/git
      GITEA_ADMIN_USERNAME:       <set to the key 'username' in secret 'gitea-cred'>  Optional: false
      GITEA_ADMIN_PASSWORD:       <set to the key 'password' in secret 'gitea-cred'>  Optional: false
      GITEA_ADMIN_PASSWORD_MODE:  keepUpdated
    Mounts:
      /data from data (rw)
      /tmp from temp (rw)
      /usr/sbin from init (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-68ks6 (ro)
Containers:
  gitea:
    Container ID:    containerd://541877c2f86d914e3eef7ad1495d98189f23130a3861ac0b216d9fedf81fe683
    Image:           docker.io/gitea/gitea:1.25.1-rootless
    Image ID:        docker.io/gitea/gitea@sha256:4b50ceba06096cda9a7611c79449af8c20c168d0554e8545200599434864d257
    Ports:           2222/TCP, 443/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:27:40 +0000
    Ready:           True
    Restart Count:   0
    Liveness:        tcp-socket :http delay=200s timeout=1s period=10s #success=1 #failure=10
    Readiness:       tcp-socket :http delay=5s timeout=1s period=10s #success=1 #failure=3
    Startup:         tcp-socket :http delay=60s timeout=1s period=10s #success=1 #failure=10
    Environment:
      SSH_LISTEN_PORT:  2222
      SSH_PORT:         22
      GITEA_APP_INI:    /data/gitea/conf/app.ini
      GITEA_CUSTOM:     /data/gitea
      GITEA_WORK_DIR:   /data
      GITEA_TEMP:       /tmp/gitea
      TMPDIR:           /tmp/gitea
      HOME:             /data/gitea/git
    Mounts:
      /data from data (rw)
      /tmp from temp (rw)
      /tmp/secret-volume from secret-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-68ks6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  init:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gitea-init
    Optional:    false
  config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gitea
    Optional:    false
  secret-volume:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gitea-tls-certs
    Optional:    false
  inline-config-sources:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gitea-inline-config
    Optional:    false
  temp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  gitea-shared-storage
    ReadOnly:   false
  kube-api-access-68ks6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             gitea-init-edge-manageability-framework-ztnkn
Namespace:        gitea
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:30:10 +0000
Labels:           batch.kubernetes.io/controller-uid=85c0756a-ce31-4c57-8bb0-12274d3c1de4
                  batch.kubernetes.io/job-name=gitea-init-edge-manageability-framework
                  controller-uid=85c0756a-ce31-4c57-8bb0-12274d3c1de4
                  job-name=gitea-init-edge-manageability-framework
Annotations:      cni.projectcalico.org/containerID: 980874e4b997785f93d777e2f053fae16f1bbb9a7113dde837791529c76651e7
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
IP:               10.42.65.84
IPs:
  IP:           10.42.65.84
Controlled By:  Job/gitea-init-edge-manageability-framework
Containers:
  alpine:
    Container ID:  containerd://e70a0d73200406c6d1608af8206f8ccdd62350615e313c87298daf2e6b66adc3
    Image:         alpine/git:2.49.1
    Image ID:      docker.io/alpine/git@sha256:c0280cf9572316299b08544065d3bf35db65043d5e3963982ec50647d2746e26
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
    Args:
      git config --global credential.helper store; git config --global user.email $GITEA_USERNAME@orch-installer.com; git config --global user.name $GITEA_USERNAME; git config --global http.sslCAInfo /usr/local/share/ca-certificates/tls.crt; git config --global --add safe.directory /repo; echo "https://$GITEA_USERNAME:$GITEA_PASSWORD@gitea-http.gitea.svc.cluster.local" > /root/.git-credentials; chdir /repo; git init; git remote add gitea https://gitea-http.gitea.svc.cluster.local/$GITEA_USERNAME/edge-manageability-framework.git; git checkout -B main; git add .; git commit --allow-empty -m 'Recreate repo from artifact'; git push --force gitea main;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:30:13 +0000
      Finished:     Tue, 03 Feb 2026 17:30:14 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      GITEA_USERNAME:  <set to the key 'username' in secret 'argocd-gitea-credential'>  Optional: false
      GITEA_PASSWORD:  <set to the key 'password' in secret 'argocd-gitea-credential'>  Optional: false
    Mounts:
      /repo from repo (rw)
      /usr/bin/tea from tea (rw)
      /usr/local/share/ca-certificates/ from tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gcss9 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  tea:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/bin/tea
    HostPathType:  
  repo:
    Type:          HostPath (bare host directory volume)
    Path:          /root/edge-manageability-framework3726909341/edge-manageability-framework
    HostPathType:  
  tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gitea-tls-certs
    Optional:    false
  kube-api-access-gcss9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: restricted volume types, field error list: [spec.volumes[0].hostPath: Forbidden, spec.volumes[1].hostPath: Forbidden])(Forbidden reason: runAsNonRoot != true, field error list: [spec.containers[0].securityContext.runAsNonRoot: Required value])(Forbidden reason: seccompProfile, field error list: [spec.containers[0].securityContext.seccompProfile.type: Required value])(Forbidden reason: allowPrivilegeEscalation != false, field error list: [spec.containers[0].securityContext.allowPrivilegeEscalation: Required value])(Forbidden reason: unrestricted capabilities, field error list: [spec.containers[0].securityContext.capabilities.drop: Required value])(Forbidden reason: hostPath volumes, field error list: [spec.volumes[0].hostPath is forbidden, forbidden values found: /usr/bin/tea, spec.volumes[1].hostPath is forbidden, forbidden values found:...


Name:             gitea-postgresql-0
Namespace:        gitea
Priority:         0
Service Account:  gitea-postgresql
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:25:20 +0000
Labels:           app.kubernetes.io/component=primary
                  app.kubernetes.io/instance=gitea
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=postgresql
                  app.kubernetes.io/version=16.3.0
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=gitea-postgresql-869f54d445
                  helm.sh/chart=postgresql-15.5.17
                  statefulset.kubernetes.io/pod-name=gitea-postgresql-0
Annotations:      cni.projectcalico.org/containerID: f8fb59830b33836c69654a961c261e70546e92e7562185c0ce58f48b1d0a4c16
                  cni.projectcalico.org/podIP: 10.42.65.76/32
                  cni.projectcalico.org/podIPs: 10.42.65.76/32
Status:           Running
IP:               10.42.65.76
IPs:
  IP:           10.42.65.76
Controlled By:  StatefulSet/gitea-postgresql
Init Containers:
  init-config-check:
    Container ID:    containerd://49af07afef2eb533a9f39d40ee1d3f251485dba630fd731cf9a7a247a22b7ce5
    Image:           busybox:1.36
    Image ID:        docker.io/library/busybox@sha256:b9598f8c98e24d0ad42c1742c32516772c3aa2151011ebaf639089bd18c605b8
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      sh
      -c
      if [ -s "/var/postgres/data/PG_VERSION" ]; then
        echo "Previous database detected. Installing postgresql.conf and pg_hba.conf."
        cp /var/postgres/postgresql.conf /var/postgres/data/postgresql.conf
        cp /var/postgres/pg_hba.conf /var/postgres/data/pg_hba.conf
      else
        echo "Fresh install. The official entrypoint will generate the default configuration."
      fi
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:25:23 +0000
      Finished:     Tue, 03 Feb 2026 17:25:23 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/postgres from data (rw)
      /var/postgres/pg_hba.conf from postgres-hba (rw,path="pg_hba.conf")
      /var/postgres/postgresql.conf from postgres-config (rw,path="postgresql.conf")
Containers:
  postgresql:
    Container ID:    containerd://23db7570190c211a98e1b3595f859bfee7d6c3f174e5c303c8fd168509fab6fe
    Image:           docker.io/library/postgres:16.10-bookworm
    Image ID:        docker.io/library/postgres@sha256:38471f330eb885e04de130b768d6db4e10469e2311879c7e5c699f6d2d8a1c74
    Port:            5432/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:25:31 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:                150m
      ephemeral-storage:  2Gi
      memory:             192Mi
    Requests:
      cpu:                100m
      ephemeral-storage:  50Mi
      memory:             128Mi
    Liveness:             exec [/bin/sh -c exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432] delay=30s timeout=5s period=10s #success=1 #failure=6
    Readiness:            exec [/bin/sh -c -e exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
] delay=5s timeout=5s period=10s #success=1 #failure=6
    Environment:
      BITNAMI_DEBUG:                        false
      POSTGRESQL_PORT_NUMBER:               5432
      POSTGRESQL_VOLUME_DIR:                /bitnami/postgresql
      PGDATA:                               /var/postgres/data
      POSTGRES_USER:                        gitea
      POSTGRES_PASSWORD:                    <set to the key 'password' in secret 'gitea-postgresql'>           Optional: false
      POSTGRES_POSTGRES_PASSWORD:           <set to the key 'postgres-password' in secret 'gitea-postgresql'>  Optional: false
      POSTGRES_DATABASE:                    gitea
      POSTGRESQL_ENABLE_LDAP:               no
      POSTGRESQL_ENABLE_TLS:                no
      POSTGRESQL_LOG_HOSTNAME:              false
      POSTGRESQL_LOG_CONNECTIONS:           false
      POSTGRESQL_LOG_DISCONNECTIONS:        false
      POSTGRESQL_PGAUDIT_LOG_CATALOG:       off
      POSTGRESQL_CLIENT_MIN_MESSAGES:       error
      POSTGRESQL_SHARED_PRELOAD_LIBRARIES:  pgaudit
      HOME:                                 /var/postgres
    Mounts:
      /bitnami/postgresql from data (rw)
      /dev/shm from dshm (rw)
      /opt/bitnami/postgresql/conf from empty-dir (rw,path="app-conf-dir")
      /opt/bitnami/postgresql/tmp from empty-dir (rw,path="app-tmp-dir")
      /tmp from empty-dir (rw,path="tmp-dir")
      /var/postgres from data (rw)
      /var/run from postgres-run (rw)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-gitea-postgresql-0
    ReadOnly:   false
  empty-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  postgres-run:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  postgres-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      postgres-config
    Optional:  false
  postgres-hba:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      postgres-hba
    Optional:  false
  dshm:
    Type:        EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:      Memory
    SizeLimit:   <unset>
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             istiod-795675cdbf-6hfrq
Namespace:        istio-system
Priority:         0
Service Account:  istiod
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:33:29 +0000
Labels:           app=istiod
                  app.kubernetes.io/instance=istiod
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=istiod
                  app.kubernetes.io/part-of=istio
                  app.kubernetes.io/version=1.28.0
                  helm.sh/chart=istiod-1.28.0
                  install.operator.istio.io/owning-resource=unknown
                  istio=pilot
                  istio.io/dataplane-mode=none
                  istio.io/rev=default
                  operator.istio.io/component=Pilot
                  pod-template-hash=795675cdbf
                  sidecar.istio.io/inject=false
Annotations:      cni.projectcalico.org/containerID: 1f7c87f5678b0357ec6b241b41c1cdbf98f3c8d338dd4646468187657c42bd8f
                  cni.projectcalico.org/podIP: 10.42.65.118/32
                  cni.projectcalico.org/podIPs: 10.42.65.118/32
                  prometheus.io/port: 15014
                  prometheus.io/scrape: true
                  sidecar.istio.io/inject: false
Status:           Running
IP:               10.42.65.118
IPs:
  IP:           10.42.65.118
Controlled By:  ReplicaSet/istiod-795675cdbf
Containers:
  discovery:
    Container ID:  containerd://bfc0a74716ff860b25787856d51ab57f9aa3dece2d9e25bb67ee3b9ccf779e62
    Image:         docker.io/istio/pilot:1.28.0
    Image ID:      docker.io/istio/pilot@sha256:dcc1431b635a2151f891ba17e8911d56f1aa1e4acaff0c543d0036b849cb0369
    Ports:         8080/TCP, 15010/TCP, 15012/TCP, 15017/TCP, 15014/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Args:
      discovery
      --monitoringAddr=:15014
      --log_output_level=default:info
      --domain
      cluster.local
      --keepaliveMaxServerConnectionAge
      30m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:30 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:8080/ready delay=1s timeout=5s period=3s #success=1 #failure=3
    Environment:
      REVISION:                  default
      PILOT_CERT_PROVIDER:       istiod
      POD_NAME:                  istiod-795675cdbf-6hfrq (v1:metadata.name)
      POD_NAMESPACE:             istio-system (v1:metadata.namespace)
      SERVICE_ACCOUNT:            (v1:spec.serviceAccountName)
      KUBECONFIG:                /var/run/secrets/remote/config
      CA_TRUSTED_NODE_ACCOUNTS:  istio-system/ztunnel
      PILOT_TRACE_SAMPLING:      1
      PILOT_ENABLE_ANALYSIS:     false
      CLUSTER_ID:                Kubernetes
      GOMEMLIMIT:                68719476736 (limits.memory)
      GOMAXPROCS:                64 (limits.cpu)
      PLATFORM:                  
    Mounts:
      /etc/cacerts from cacerts (ro)
      /var/run/secrets/istio-dns from local-certs (rw)
      /var/run/secrets/istiod/ca from istio-csr-ca-configmap (ro)
      /var/run/secrets/istiod/tls from istio-csr-dns-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4t9bf (ro)
      /var/run/secrets/remote from istio-kubeconfig (ro)
      /var/run/secrets/tokens from istio-token (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  local-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  cacerts:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cacerts
    Optional:    true
  istio-kubeconfig:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio-kubeconfig
    Optional:    true
  istio-csr-dns-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istiod-tls
    Optional:    true
  istio-csr-ca-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  true
  kube-api-access-4t9bf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 cni.istio.io/not-ready op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             istiod-795675cdbf-ffgrg
Namespace:        istio-system
Priority:         0
Service Account:  istiod
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:33:14 +0000
Labels:           app=istiod
                  app.kubernetes.io/instance=istiod
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=istiod
                  app.kubernetes.io/part-of=istio
                  app.kubernetes.io/version=1.28.0
                  helm.sh/chart=istiod-1.28.0
                  install.operator.istio.io/owning-resource=unknown
                  istio=pilot
                  istio.io/dataplane-mode=none
                  istio.io/rev=default
                  operator.istio.io/component=Pilot
                  pod-template-hash=795675cdbf
                  sidecar.istio.io/inject=false
Annotations:      cni.projectcalico.org/containerID: 92c6bbf728c79ab4dc706853be00219864c97ab21bc1cac00a7486cc24bd587b
                  cni.projectcalico.org/podIP: 10.42.65.116/32
                  cni.projectcalico.org/podIPs: 10.42.65.116/32
                  prometheus.io/port: 15014
                  prometheus.io/scrape: true
                  sidecar.istio.io/inject: false
Status:           Running
IP:               10.42.65.116
IPs:
  IP:           10.42.65.116
Controlled By:  ReplicaSet/istiod-795675cdbf
Containers:
  discovery:
    Container ID:  containerd://5f5aa9648f234fc03574bf57bfcf9f158ed73650e90f43e10149315a1cd1c0f6
    Image:         docker.io/istio/pilot:1.28.0
    Image ID:      docker.io/istio/pilot@sha256:dcc1431b635a2151f891ba17e8911d56f1aa1e4acaff0c543d0036b849cb0369
    Ports:         8080/TCP, 15010/TCP, 15012/TCP, 15017/TCP, 15014/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Args:
      discovery
      --monitoringAddr=:15014
      --log_output_level=default:info
      --domain
      cluster.local
      --keepaliveMaxServerConnectionAge
      30m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:8080/ready delay=1s timeout=5s period=3s #success=1 #failure=3
    Environment:
      REVISION:                  default
      PILOT_CERT_PROVIDER:       istiod
      POD_NAME:                  istiod-795675cdbf-ffgrg (v1:metadata.name)
      POD_NAMESPACE:             istio-system (v1:metadata.namespace)
      SERVICE_ACCOUNT:            (v1:spec.serviceAccountName)
      KUBECONFIG:                /var/run/secrets/remote/config
      CA_TRUSTED_NODE_ACCOUNTS:  istio-system/ztunnel
      PILOT_TRACE_SAMPLING:      1
      PILOT_ENABLE_ANALYSIS:     false
      CLUSTER_ID:                Kubernetes
      GOMEMLIMIT:                68719476736 (limits.memory)
      GOMAXPROCS:                64 (limits.cpu)
      PLATFORM:                  
    Mounts:
      /etc/cacerts from cacerts (ro)
      /var/run/secrets/istio-dns from local-certs (rw)
      /var/run/secrets/istiod/ca from istio-csr-ca-configmap (ro)
      /var/run/secrets/istiod/tls from istio-csr-dns-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zh868 (ro)
      /var/run/secrets/remote from istio-kubeconfig (ro)
      /var/run/secrets/tokens from istio-token (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  local-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  cacerts:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cacerts
    Optional:    true
  istio-kubeconfig:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio-kubeconfig
    Optional:    true
  istio-csr-dns-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istiod-tls
    Optional:    true
  istio-csr-ca-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  true
  kube-api-access-zh868:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 cni.istio.io/not-ready op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             istiod-795675cdbf-njn78
Namespace:        istio-system
Priority:         0
Service Account:  istiod
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:33:14 +0000
Labels:           app=istiod
                  app.kubernetes.io/instance=istiod
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=istiod
                  app.kubernetes.io/part-of=istio
                  app.kubernetes.io/version=1.28.0
                  helm.sh/chart=istiod-1.28.0
                  install.operator.istio.io/owning-resource=unknown
                  istio=pilot
                  istio.io/dataplane-mode=none
                  istio.io/rev=default
                  operator.istio.io/component=Pilot
                  pod-template-hash=795675cdbf
                  sidecar.istio.io/inject=false
Annotations:      cni.projectcalico.org/containerID: d11466d5ae3f8e566ac3c23e5b002306f85353304950441fc8294c0e625a711d
                  cni.projectcalico.org/podIP: 10.42.65.115/32
                  cni.projectcalico.org/podIPs: 10.42.65.115/32
                  prometheus.io/port: 15014
                  prometheus.io/scrape: true
                  sidecar.istio.io/inject: false
Status:           Running
IP:               10.42.65.115
IPs:
  IP:           10.42.65.115
Controlled By:  ReplicaSet/istiod-795675cdbf
Containers:
  discovery:
    Container ID:  containerd://60d40cd6d26d02fdb0dcaff496c3b09e1d2234605a67b0cc4ea6e1017184fc10
    Image:         docker.io/istio/pilot:1.28.0
    Image ID:      docker.io/istio/pilot@sha256:dcc1431b635a2151f891ba17e8911d56f1aa1e4acaff0c543d0036b849cb0369
    Ports:         8080/TCP, 15010/TCP, 15012/TCP, 15017/TCP, 15014/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Args:
      discovery
      --monitoringAddr=:15014
      --log_output_level=default:info
      --domain
      cluster.local
      --keepaliveMaxServerConnectionAge
      30m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:8080/ready delay=1s timeout=5s period=3s #success=1 #failure=3
    Environment:
      REVISION:                  default
      PILOT_CERT_PROVIDER:       istiod
      POD_NAME:                  istiod-795675cdbf-njn78 (v1:metadata.name)
      POD_NAMESPACE:             istio-system (v1:metadata.namespace)
      SERVICE_ACCOUNT:            (v1:spec.serviceAccountName)
      KUBECONFIG:                /var/run/secrets/remote/config
      CA_TRUSTED_NODE_ACCOUNTS:  istio-system/ztunnel
      PILOT_TRACE_SAMPLING:      1
      PILOT_ENABLE_ANALYSIS:     false
      CLUSTER_ID:                Kubernetes
      GOMEMLIMIT:                68719476736 (limits.memory)
      GOMAXPROCS:                64 (limits.cpu)
      PLATFORM:                  
    Mounts:
      /etc/cacerts from cacerts (ro)
      /var/run/secrets/istio-dns from local-certs (rw)
      /var/run/secrets/istiod/ca from istio-csr-ca-configmap (ro)
      /var/run/secrets/istiod/tls from istio-csr-dns-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-26wtd (ro)
      /var/run/secrets/remote from istio-kubeconfig (ro)
      /var/run/secrets/tokens from istio-token (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  local-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  cacerts:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cacerts
    Optional:    true
  istio-kubeconfig:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio-kubeconfig
    Optional:    true
  istio-csr-dns-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istiod-tls
    Optional:    true
  istio-csr-ca-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  true
  kube-api-access-26wtd:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 cni.istio.io/not-ready op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             istiod-795675cdbf-rhq97
Namespace:        istio-system
Priority:         0
Service Account:  istiod
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:33:14 +0000
Labels:           app=istiod
                  app.kubernetes.io/instance=istiod
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=istiod
                  app.kubernetes.io/part-of=istio
                  app.kubernetes.io/version=1.28.0
                  helm.sh/chart=istiod-1.28.0
                  install.operator.istio.io/owning-resource=unknown
                  istio=pilot
                  istio.io/dataplane-mode=none
                  istio.io/rev=default
                  operator.istio.io/component=Pilot
                  pod-template-hash=795675cdbf
                  sidecar.istio.io/inject=false
Annotations:      cni.projectcalico.org/containerID: d50fd104911b68491c9796f24224d87f6bde20932d2c5e89e0fc5565fd791a0e
                  cni.projectcalico.org/podIP: 10.42.65.114/32
                  cni.projectcalico.org/podIPs: 10.42.65.114/32
                  prometheus.io/port: 15014
                  prometheus.io/scrape: true
                  sidecar.istio.io/inject: false
Status:           Running
IP:               10.42.65.114
IPs:
  IP:           10.42.65.114
Controlled By:  ReplicaSet/istiod-795675cdbf
Containers:
  discovery:
    Container ID:  containerd://d173dbc0d24f7a9f37a58484bd61d061233f09b6a63700b1cd418685a56d2401
    Image:         docker.io/istio/pilot:1.28.0
    Image ID:      docker.io/istio/pilot@sha256:dcc1431b635a2151f891ba17e8911d56f1aa1e4acaff0c543d0036b849cb0369
    Ports:         8080/TCP, 15010/TCP, 15012/TCP, 15017/TCP, 15014/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Args:
      discovery
      --monitoringAddr=:15014
      --log_output_level=default:info
      --domain
      cluster.local
      --keepaliveMaxServerConnectionAge
      30m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:8080/ready delay=1s timeout=5s period=3s #success=1 #failure=3
    Environment:
      REVISION:                  default
      PILOT_CERT_PROVIDER:       istiod
      POD_NAME:                  istiod-795675cdbf-rhq97 (v1:metadata.name)
      POD_NAMESPACE:             istio-system (v1:metadata.namespace)
      SERVICE_ACCOUNT:            (v1:spec.serviceAccountName)
      KUBECONFIG:                /var/run/secrets/remote/config
      CA_TRUSTED_NODE_ACCOUNTS:  istio-system/ztunnel
      PILOT_TRACE_SAMPLING:      1
      PILOT_ENABLE_ANALYSIS:     false
      CLUSTER_ID:                Kubernetes
      GOMEMLIMIT:                68719476736 (limits.memory)
      GOMAXPROCS:                64 (limits.cpu)
      PLATFORM:                  
    Mounts:
      /etc/cacerts from cacerts (ro)
      /var/run/secrets/istio-dns from local-certs (rw)
      /var/run/secrets/istiod/ca from istio-csr-ca-configmap (ro)
      /var/run/secrets/istiod/tls from istio-csr-dns-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wtb (ro)
      /var/run/secrets/remote from istio-kubeconfig (ro)
      /var/run/secrets/tokens from istio-token (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  local-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  cacerts:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cacerts
    Optional:    true
  istio-kubeconfig:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio-kubeconfig
    Optional:    true
  istio-csr-dns-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istiod-tls
    Optional:    true
  istio-csr-ca-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  true
  kube-api-access-f5wtb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 cni.istio.io/not-ready op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             istiod-795675cdbf-s4944
Namespace:        istio-system
Priority:         0
Service Account:  istiod
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:32:29 +0000
Labels:           app=istiod
                  app.kubernetes.io/instance=istiod
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=istiod
                  app.kubernetes.io/part-of=istio
                  app.kubernetes.io/version=1.28.0
                  helm.sh/chart=istiod-1.28.0
                  install.operator.istio.io/owning-resource=unknown
                  istio=pilot
                  istio.io/dataplane-mode=none
                  istio.io/rev=default
                  operator.istio.io/component=Pilot
                  pod-template-hash=795675cdbf
                  sidecar.istio.io/inject=false
Annotations:      cni.projectcalico.org/containerID: 3902e27bf0e1c42cdc98bb1136215296de2e838366ecc64940c23ce42f219991
                  cni.projectcalico.org/podIP: 10.42.65.106/32
                  cni.projectcalico.org/podIPs: 10.42.65.106/32
                  prometheus.io/port: 15014
                  prometheus.io/scrape: true
                  sidecar.istio.io/inject: false
Status:           Running
IP:               10.42.65.106
IPs:
  IP:           10.42.65.106
Controlled By:  ReplicaSet/istiod-795675cdbf
Containers:
  discovery:
    Container ID:  containerd://2e49fe2e0792830e45b0488536b6fc1c1a2da7e87bc792e76e25a94ea2535538
    Image:         docker.io/istio/pilot:1.28.0
    Image ID:      docker.io/istio/pilot@sha256:dcc1431b635a2151f891ba17e8911d56f1aa1e4acaff0c543d0036b849cb0369
    Ports:         8080/TCP, 15010/TCP, 15012/TCP, 15017/TCP, 15014/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Args:
      discovery
      --monitoringAddr=:15014
      --log_output_level=default:info
      --domain
      cluster.local
      --keepaliveMaxServerConnectionAge
      30m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:32:35 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:8080/ready delay=1s timeout=5s period=3s #success=1 #failure=3
    Environment:
      REVISION:                  default
      PILOT_CERT_PROVIDER:       istiod
      POD_NAME:                  istiod-795675cdbf-s4944 (v1:metadata.name)
      POD_NAMESPACE:             istio-system (v1:metadata.namespace)
      SERVICE_ACCOUNT:            (v1:spec.serviceAccountName)
      KUBECONFIG:                /var/run/secrets/remote/config
      CA_TRUSTED_NODE_ACCOUNTS:  istio-system/ztunnel
      PILOT_TRACE_SAMPLING:      1
      PILOT_ENABLE_ANALYSIS:     false
      CLUSTER_ID:                Kubernetes
      GOMEMLIMIT:                68719476736 (limits.memory)
      GOMAXPROCS:                64 (limits.cpu)
      PLATFORM:                  
    Mounts:
      /etc/cacerts from cacerts (ro)
      /var/run/secrets/istio-dns from local-certs (rw)
      /var/run/secrets/istiod/ca from istio-csr-ca-configmap (ro)
      /var/run/secrets/istiod/tls from istio-csr-dns-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vw9nz (ro)
      /var/run/secrets/remote from istio-kubeconfig (ro)
      /var/run/secrets/tokens from istio-token (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  local-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  cacerts:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cacerts
    Optional:    true
  istio-kubeconfig:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio-kubeconfig
    Optional:    true
  istio-csr-dns-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istiod-tls
    Optional:    true
  istio-csr-ca-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  true
  kube-api-access-vw9nz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 cni.istio.io/not-ready op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kiali-d77d9c8fb-ztk2t
Namespace:        istio-system
Priority:         0
Service Account:  kiali
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:32:59 +0000
Labels:           app=kiali
                  app.kubernetes.io/instance=kiali
                  app.kubernetes.io/name=kiali
                  app.kubernetes.io/part-of=kiali
                  app.kubernetes.io/version=v2.18.0
                  helm.sh/chart=kiali-server-2.18.0
                  pod-template-hash=d77d9c8fb
                  version=v2.18.0
Annotations:      checksum/config: 1728055891d73dd6611ed4caeef3cccfe656e5923cf7348642a1c931d8d25c24
                  cni.projectcalico.org/containerID: f7be7aa71e67bc8b907bfd03c70c6f6c9e100f0a0f6b4495fd312c186e55170e
                  cni.projectcalico.org/podIP: 10.42.65.111/32
                  cni.projectcalico.org/podIPs: 10.42.65.111/32
                  kiali.io/dashboards: go,kiali
                  prometheus.io/port: 9090
                  prometheus.io/scrape: true
                  proxy.istio.io/config: { "holdApplicationUntilProxyStarts": true }
Status:           Running
IP:               10.42.65.111
IPs:
  IP:           10.42.65.111
Controlled By:  ReplicaSet/kiali-d77d9c8fb
Containers:
  kiali:
    Container ID:  containerd://5df815fcd6ea42393492560e2cc417143ab8d0c93c026c8026369daf4aa0a7b6
    Image:         quay.io/kiali/kiali:v2.18.0
    Image ID:      quay.io/kiali/kiali@sha256:1dee0c7f6cd905002eec136f01fd62c0933a7d1f6c59596bf61a0501fc6c3ec5
    Ports:         20001/TCP, 9090/TCP
    Host Ports:    0/TCP, 0/TCP
    Command:
      /opt/kiali/kiali
      -config
      /kiali-configuration/config.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:api-port/kiali/healthz delay=5s timeout=1s period=30s #success=1 #failure=3
    Readiness:  http-get http://:api-port/kiali/healthz delay=5s timeout=1s period=30s #success=1 #failure=3
    Startup:    http-get http://:api-port/kiali/healthz delay=30s timeout=1s period=10s #success=1 #failure=6
    Environment:
      ACTIVE_NAMESPACE:       istio-system (v1:metadata.namespace)
      LOG_LEVEL:              info
      LOG_FORMAT:             text
      LOG_TIME_FIELD_FORMAT:  2006-01-02T15:04:05Z07:00
      LOG_SAMPLER_RATE:       1
    Mounts:
      /kiali-cabundle from kiali-cabundle (rw)
      /kiali-cert from kiali-cert (rw)
      /kiali-configuration from kiali-configuration (rw)
      /kiali-remote-cluster-secrets/kiali-multi-cluster-secret from kiali-multi-cluster-secret (ro)
      /kiali-secret from kiali-secret (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-br5m7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kiali-configuration:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kiali
    Optional:  false
  kiali-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio.kiali-service-account
    Optional:    true
  kiali-secret:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kiali
    Optional:    true
  kiali-cabundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kiali-cabundle
    Optional:  true
  kiali-multi-cluster-secret:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kiali-multi-cluster-secret
    Optional:    true
  kube-api-access-br5m7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:                 etcd-orch-tf
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:24:56 +0000
Labels:               component=etcd
                      tier=control-plane
Annotations:          etcd.k3s.io/initial:
                        {"initial-advertise-peer-urls":"https://192.168.99.10:2380","initial-cluster":"orch-tf-6eb43385=https://192.168.99.10:2380","initial-clust...
                      kubernetes.io/config.hash: e6c21b3b061e9ef47c35e5efebae351b
                      kubernetes.io/config.mirror: e6c21b3b061e9ef47c35e5efebae351b
                      kubernetes.io/config.seen: 2026-02-03T17:22:33.347323553Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Node/orch-tf
Containers:
  etcd:
    Container ID:  containerd://3e5f6613e5226f5df706168a77c4633fdb2f28b4b768b964d5159b140f16aab7
    Image:         index.docker.io/rancher/hardened-etcd:v3.6.4-k3s3-build20250908
    Image ID:      docker.io/rancher/hardened-etcd@sha256:3d69c14d716250a66e5cc750f6cd7eec10d3a7aeb8c35cc1b21971ac84dfde38
    Ports:         2379/TCP, 2380/TCP, 2381/TCP
    Host Ports:    2379/TCP, 2380/TCP, 2381/TCP
    Command:
      etcd
    Args:
      --config-file=/var/lib/rancher/rke2/server/db/etcd/config
    State:          Running
      Started:      Tue, 03 Feb 2026 17:22:37 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     200m
      memory:  512Mi
    Liveness:  http-get http://localhost:2381/health%3Fserializable=true delay=10s timeout=15s period=10s #success=1 #failure=8
    Environment:
      FILE_HASH:  6570558b27239169f654264d4d531419b73451d1a1369dc2ea7f42d634353c37
      NO_PROXY:   .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
    Mounts:
      /var/lib/rancher/rke2/server/db/etcd from dir0 (rw)
      /var/lib/rancher/rke2/server/db/etcd/config from file6 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/peer-ca.crt from file5 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.crt from file3 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.key from file4 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt from file2 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/server-client.crt from file0 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/server-client.key from file1 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  dir0:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/db/etcd
    HostPathType:  DirectoryOrCreate
  file0:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/server-client.crt
    HostPathType:  File
  file1:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/server-client.key
    HostPathType:  File
  file2:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
    HostPathType:  File
  file3:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.crt
    HostPathType:  File
  file4:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.key
    HostPathType:  File
  file5:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/peer-ca.crt
    HostPathType:  File
  file6:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/db/etcd/config
    HostPathType:  File
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:             helm-install-rke2-calico-crd-tlzds
Namespace:        kube-system
Priority:         0
Service Account:  helm-rke2-calico-crd
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:22:53 +0000
Labels:           batch.kubernetes.io/controller-uid=62b5d1d7-e9a1-4616-9470-7f94d25e2799
                  batch.kubernetes.io/job-name=helm-install-rke2-calico-crd
                  controller-uid=62b5d1d7-e9a1-4616-9470-7f94d25e2799
                  helmcharts.helm.cattle.io/chart=rke2-calico-crd
                  job-name=helm-install-rke2-calico-crd
Annotations:      helmcharts.helm.cattle.io/configHash: SHA256=600E97F003FEC7F083FC8AF84B11C11825282570F3CC14A65A44BB1A4FB9E83C
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Job/helm-install-rke2-calico-crd
Containers:
  helm:
    Container ID:  containerd://f0c41d2e4c63a6d937b474203e0784deda6adc61110b0f7b22ca0a9d8236e270
    Image:         rancher/klipper-helm:v0.9.8-build20250709
    Image ID:      docker.io/rancher/klipper-helm@sha256:5e2e5ab00d5f2652bfea2bc5554d07b57a015fe574434b275a2cb11c2d135c04
    Port:          <none>
    Host Port:     <none>
    Args:
      install
      --set-string
      global.clusterCIDR=10.42.0.0/16
      --set-string
      global.clusterCIDRv4=10.42.0.0/16
      --set-string
      global.clusterDNS=10.43.0.10
      --set-string
      global.clusterDomain=cluster.local
      --set-string
      global.rke2DataDir=/var/lib/rancher/rke2
      --set-string
      global.serviceCIDR=10.43.0.0/16
      --set-string
      global.systemDefaultIngressClass=ingress-nginx
    State:      Terminated
      Reason:   Completed
      Message:  Installing helm chart

      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:23:19 +0000
      Finished:     Tue, 03 Feb 2026 17:23:22 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      NAME:                      rke2-calico-crd
      VERSION:                   
      REPO:                      
      HELM_DRIVER:               secret
      CHART_NAMESPACE:           kube-system
      CHART:                     
      HELM_VERSION:              
      TARGET_NAMESPACE:          kube-system
      AUTH_PASS_CREDENTIALS:     false
      INSECURE_SKIP_TLS_VERIFY:  false
      PLAIN_HTTP:                false
      KUBERNETES_SERVICE_HOST:   127.0.0.1
      KUBERNETES_SERVICE_PORT:   6443
      BOOTSTRAP:                 true
      NO_PROXY:                  .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      FAILURE_POLICY:            reinstall
    Mounts:
      /chart from content (rw)
      /config from values (rw)
      /home/klipper-helm/.cache from klipper-cache (rw)
      /home/klipper-helm/.config from klipper-config (rw)
      /home/klipper-helm/.helm from klipper-helm (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rbhq4 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  klipper-helm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  values:
    Type:                Projected (a volume that contains injected data from multiple sources)
    SecretName:          chart-values-rke2-calico-crd
    SecretOptionalName:  <nil>
  content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      chart-content-rke2-calico-crd
    Optional:  false
  kube-api-access-rbhq4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
                             node-role.kubernetes.io/control-plane=true
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                             node.kubernetes.io/network-unavailable:NoSchedule
                             node.kubernetes.io/not-ready:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             helm-install-rke2-calico-n59gx
Namespace:        kube-system
Priority:         0
Service Account:  helm-rke2-calico
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:22:53 +0000
Labels:           batch.kubernetes.io/controller-uid=366a6a6a-d74e-4e4f-9ca0-ebd2d01e4127
                  batch.kubernetes.io/job-name=helm-install-rke2-calico
                  controller-uid=366a6a6a-d74e-4e4f-9ca0-ebd2d01e4127
                  helmcharts.helm.cattle.io/chart=rke2-calico
                  job-name=helm-install-rke2-calico
Annotations:      helmcharts.helm.cattle.io/configHash: SHA256=91228F374F2617056E1AE22DB327FFCE83FF2407B5301470736772964A2728FB
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Job/helm-install-rke2-calico
Containers:
  helm:
    Container ID:  containerd://85d727d0dd06f0401bb2354ae758c498e8a504daec28c2f86f18215b821589c2
    Image:         rancher/klipper-helm:v0.9.8-build20250709
    Image ID:      docker.io/rancher/klipper-helm@sha256:5e2e5ab00d5f2652bfea2bc5554d07b57a015fe574434b275a2cb11c2d135c04
    Port:          <none>
    Host Port:     <none>
    Args:
      install
      --set-string
      global.clusterCIDR=10.42.0.0/16
      --set-string
      global.clusterCIDRv4=10.42.0.0/16
      --set-string
      global.clusterDNS=10.43.0.10
      --set-string
      global.clusterDomain=cluster.local
      --set-string
      global.rke2DataDir=/var/lib/rancher/rke2
      --set-string
      global.serviceCIDR=10.43.0.0/16
      --set-string
      global.systemDefaultIngressClass=ingress-nginx
    State:      Terminated
      Reason:   Completed
      Message:  Installing helm chart

      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:23:35 +0000
      Finished:     Tue, 03 Feb 2026 17:23:36 +0000
    Ready:          False
    Restart Count:  2
    Environment:
      NAME:                      rke2-calico
      VERSION:                   
      REPO:                      
      HELM_DRIVER:               secret
      CHART_NAMESPACE:           kube-system
      CHART:                     
      HELM_VERSION:              
      TARGET_NAMESPACE:          kube-system
      AUTH_PASS_CREDENTIALS:     false
      INSECURE_SKIP_TLS_VERIFY:  false
      PLAIN_HTTP:                false
      KUBERNETES_SERVICE_HOST:   127.0.0.1
      KUBERNETES_SERVICE_PORT:   6443
      BOOTSTRAP:                 true
      NO_PROXY:                  .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      FAILURE_POLICY:            reinstall
    Mounts:
      /chart from content (rw)
      /config from values (rw)
      /home/klipper-helm/.cache from klipper-cache (rw)
      /home/klipper-helm/.config from klipper-config (rw)
      /home/klipper-helm/.helm from klipper-helm (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gdhhv (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  klipper-helm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  values:
    Type:                Projected (a volume that contains injected data from multiple sources)
    SecretName:          chart-values-rke2-calico
    SecretOptionalName:  <nil>
  content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      chart-content-rke2-calico
    Optional:  false
  kube-api-access-gdhhv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
                             node-role.kubernetes.io/control-plane=true
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                             node.kubernetes.io/network-unavailable:NoSchedule
                             node.kubernetes.io/not-ready:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             helm-install-rke2-coredns-5svxr
Namespace:        kube-system
Priority:         0
Service Account:  helm-rke2-coredns
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:22:53 +0000
Labels:           batch.kubernetes.io/controller-uid=49b1b0fc-5247-43a9-9fbf-e55cf4197863
                  batch.kubernetes.io/job-name=helm-install-rke2-coredns
                  controller-uid=49b1b0fc-5247-43a9-9fbf-e55cf4197863
                  helmcharts.helm.cattle.io/chart=rke2-coredns
                  job-name=helm-install-rke2-coredns
Annotations:      helmcharts.helm.cattle.io/configHash: SHA256=8D50E0FD22C54A0E322E9492B22E64C9AE887C4D7FCA90526D97087FB6F914EF
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Job/helm-install-rke2-coredns
Containers:
  helm:
    Container ID:  containerd://134d1ed9db7eef7563fc5f26f1b4ef67a2dbcc31457fa6b411542493062bfc2d
    Image:         rancher/klipper-helm:v0.9.8-build20250709
    Image ID:      docker.io/rancher/klipper-helm@sha256:5e2e5ab00d5f2652bfea2bc5554d07b57a015fe574434b275a2cb11c2d135c04
    Port:          <none>
    Host Port:     <none>
    Args:
      install
      --set-string
      global.clusterCIDR=10.42.0.0/16
      --set-string
      global.clusterCIDRv4=10.42.0.0/16
      --set-string
      global.clusterDNS=10.43.0.10
      --set-string
      global.clusterDomain=cluster.local
      --set-string
      global.rke2DataDir=/var/lib/rancher/rke2
      --set-string
      global.serviceCIDR=10.43.0.0/16
      --set-string
      global.systemDefaultIngressClass=ingress-nginx
    State:      Terminated
      Reason:   Completed
      Message:  Installing helm chart

      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:23:19 +0000
      Finished:     Tue, 03 Feb 2026 17:23:20 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      NAME:                      rke2-coredns
      VERSION:                   
      REPO:                      
      HELM_DRIVER:               secret
      CHART_NAMESPACE:           kube-system
      CHART:                     
      HELM_VERSION:              
      TARGET_NAMESPACE:          kube-system
      AUTH_PASS_CREDENTIALS:     false
      INSECURE_SKIP_TLS_VERIFY:  false
      PLAIN_HTTP:                false
      KUBERNETES_SERVICE_HOST:   127.0.0.1
      KUBERNETES_SERVICE_PORT:   6443
      BOOTSTRAP:                 true
      NO_PROXY:                  .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      FAILURE_POLICY:            reinstall
    Mounts:
      /chart from content (rw)
      /config from values (rw)
      /home/klipper-helm/.cache from klipper-cache (rw)
      /home/klipper-helm/.config from klipper-config (rw)
      /home/klipper-helm/.helm from klipper-helm (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z4dm6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  klipper-helm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  values:
    Type:                Projected (a volume that contains injected data from multiple sources)
    SecretName:          chart-values-rke2-coredns
    SecretOptionalName:  <nil>
  content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      chart-content-rke2-coredns
    Optional:  false
  kube-api-access-z4dm6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
                             node-role.kubernetes.io/control-plane=true
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                             node.kubernetes.io/network-unavailable:NoSchedule
                             node.kubernetes.io/not-ready:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             helm-install-rke2-metrics-server-xkxvl
Namespace:        kube-system
Priority:         0
Service Account:  helm-rke2-metrics-server
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:23:42 +0000
Labels:           batch.kubernetes.io/controller-uid=965410e8-9f66-4762-a72c-258146979a3a
                  batch.kubernetes.io/job-name=helm-install-rke2-metrics-server
                  controller-uid=965410e8-9f66-4762-a72c-258146979a3a
                  helmcharts.helm.cattle.io/chart=rke2-metrics-server
                  job-name=helm-install-rke2-metrics-server
Annotations:      cni.projectcalico.org/containerID: 0988d74462cdabfacf7a38129e7b789a2512b4472445a98482b8eb185b81dcd6
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  helmcharts.helm.cattle.io/configHash: SHA256=1AF2872AE6AA632F45FC64892D6B44DEF93BA0AE4B903158D72432E5C29B936E
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               
IPs:              <none>
Controlled By:    Job/helm-install-rke2-metrics-server
Containers:
  helm:
    Container ID:  containerd://91f6f3c795479103abdbcb1ff9274e682bfe7ff4b033df336d2fa9f2d1837d2b
    Image:         rancher/klipper-helm:v0.9.8-build20250709
    Image ID:      docker.io/rancher/klipper-helm@sha256:5e2e5ab00d5f2652bfea2bc5554d07b57a015fe574434b275a2cb11c2d135c04
    Port:          <none>
    Host Port:     <none>
    Args:
      install
      --set-string
      global.clusterCIDR=10.42.0.0/16
      --set-string
      global.clusterCIDRv4=10.42.0.0/16
      --set-string
      global.clusterDNS=10.43.0.10
      --set-string
      global.clusterDomain=cluster.local
      --set-string
      global.rke2DataDir=/var/lib/rancher/rke2
      --set-string
      global.serviceCIDR=10.43.0.0/16
      --set-string
      global.systemDefaultIngressClass=ingress-nginx
    State:      Terminated
      Reason:   Completed
      Message:  Installing helm chart

      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:23:58 +0000
      Finished:     Tue, 03 Feb 2026 17:23:58 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      NAME:                      rke2-metrics-server
      VERSION:                   
      REPO:                      
      HELM_DRIVER:               secret
      CHART_NAMESPACE:           kube-system
      CHART:                     
      HELM_VERSION:              
      TARGET_NAMESPACE:          kube-system
      AUTH_PASS_CREDENTIALS:     false
      INSECURE_SKIP_TLS_VERIFY:  false
      PLAIN_HTTP:                false
      NO_PROXY:                  .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      FAILURE_POLICY:            reinstall
    Mounts:
      /chart from content (rw)
      /config from values (rw)
      /home/klipper-helm/.cache from klipper-cache (rw)
      /home/klipper-helm/.config from klipper-config (rw)
      /home/klipper-helm/.helm from klipper-helm (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dmrr9 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  klipper-helm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  values:
    Type:                Projected (a volume that contains injected data from multiple sources)
    SecretName:          chart-values-rke2-metrics-server
    SecretOptionalName:  <nil>
  content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      chart-content-rke2-metrics-server
    Optional:  false
  kube-api-access-dmrr9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             helm-install-rke2-runtimeclasses-qt77l
Namespace:        kube-system
Priority:         0
Service Account:  helm-rke2-runtimeclasses
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:23:42 +0000
Labels:           batch.kubernetes.io/controller-uid=0ab8da17-c7ee-40c6-ac10-25b6b44324c9
                  batch.kubernetes.io/job-name=helm-install-rke2-runtimeclasses
                  controller-uid=0ab8da17-c7ee-40c6-ac10-25b6b44324c9
                  helmcharts.helm.cattle.io/chart=rke2-runtimeclasses
                  job-name=helm-install-rke2-runtimeclasses
Annotations:      cni.projectcalico.org/containerID: 08bd9cd409e4e5f0643097168896c5369c53dc048f41ed5bf5904cc0d498f18d
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  helmcharts.helm.cattle.io/configHash: SHA256=C60DFF350704864CA82FFA1FD4029A21D44F4B3AB5E02CAA31C6B44BF82A6C53
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               
IPs:              <none>
Controlled By:    Job/helm-install-rke2-runtimeclasses
Containers:
  helm:
    Container ID:  containerd://3c576ee870c6d99b22e77ee7c408a6e9c4e92d8872351bbd0dfb5cfd816d5cbd
    Image:         rancher/klipper-helm:v0.9.8-build20250709
    Image ID:      docker.io/rancher/klipper-helm@sha256:5e2e5ab00d5f2652bfea2bc5554d07b57a015fe574434b275a2cb11c2d135c04
    Port:          <none>
    Host Port:     <none>
    Args:
      install
      --set-string
      global.clusterCIDR=10.42.0.0/16
      --set-string
      global.clusterCIDRv4=10.42.0.0/16
      --set-string
      global.clusterDNS=10.43.0.10
      --set-string
      global.clusterDomain=cluster.local
      --set-string
      global.rke2DataDir=/var/lib/rancher/rke2
      --set-string
      global.serviceCIDR=10.43.0.0/16
      --set-string
      global.systemDefaultIngressClass=ingress-nginx
    State:      Terminated
      Reason:   Completed
      Message:  Installing helm chart

      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:23:54 +0000
      Finished:     Tue, 03 Feb 2026 17:23:54 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      NAME:                      rke2-runtimeclasses
      VERSION:                   
      REPO:                      
      HELM_DRIVER:               secret
      CHART_NAMESPACE:           kube-system
      CHART:                     
      HELM_VERSION:              
      TARGET_NAMESPACE:          kube-system
      AUTH_PASS_CREDENTIALS:     false
      INSECURE_SKIP_TLS_VERIFY:  false
      PLAIN_HTTP:                false
      NO_PROXY:                  .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      FAILURE_POLICY:            reinstall
    Mounts:
      /chart from content (rw)
      /config from values (rw)
      /home/klipper-helm/.cache from klipper-cache (rw)
      /home/klipper-helm/.config from klipper-config (rw)
      /home/klipper-helm/.helm from klipper-helm (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vr5dt (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  klipper-helm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  values:
    Type:                Projected (a volume that contains injected data from multiple sources)
    SecretName:          chart-values-rke2-runtimeclasses
    SecretOptionalName:  <nil>
  content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      chart-content-rke2-runtimeclasses
    Optional:  false
  kube-api-access-vr5dt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             helm-install-rke2-snapshot-controller-crd-8jcdl
Namespace:        kube-system
Priority:         0
Service Account:  helm-rke2-snapshot-controller-crd
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:23:42 +0000
Labels:           batch.kubernetes.io/controller-uid=fdccf10d-8df8-4ef1-975e-681c7927792c
                  batch.kubernetes.io/job-name=helm-install-rke2-snapshot-controller-crd
                  controller-uid=fdccf10d-8df8-4ef1-975e-681c7927792c
                  helmcharts.helm.cattle.io/chart=rke2-snapshot-controller-crd
                  job-name=helm-install-rke2-snapshot-controller-crd
Annotations:      cni.projectcalico.org/containerID: 0a0370149a99390945690dd20a1d625c8a6aa338fc5993be3876c8a122b46801
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  helmcharts.helm.cattle.io/configHash: SHA256=BBA1F24B6D98383A90386F8C5FA6CE6293DEF54D74A4EA6256A6DCBA25585956
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               
IPs:              <none>
Controlled By:    Job/helm-install-rke2-snapshot-controller-crd
Containers:
  helm:
    Container ID:  containerd://fe5a699f56cc6337dfe46637b32906e621dfcdcce1e43d4a6eba3f194c6778c4
    Image:         rancher/klipper-helm:v0.9.8-build20250709
    Image ID:      docker.io/rancher/klipper-helm@sha256:5e2e5ab00d5f2652bfea2bc5554d07b57a015fe574434b275a2cb11c2d135c04
    Port:          <none>
    Host Port:     <none>
    Args:
      install
      --set-string
      global.clusterCIDR=10.42.0.0/16
      --set-string
      global.clusterCIDRv4=10.42.0.0/16
      --set-string
      global.clusterDNS=10.43.0.10
      --set-string
      global.clusterDomain=cluster.local
      --set-string
      global.rke2DataDir=/var/lib/rancher/rke2
      --set-string
      global.serviceCIDR=10.43.0.0/16
      --set-string
      global.systemDefaultIngressClass=ingress-nginx
    State:      Terminated
      Reason:   Completed
      Message:  Installing helm chart

      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:23:55 +0000
      Finished:     Tue, 03 Feb 2026 17:23:55 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      NAME:                      rke2-snapshot-controller-crd
      VERSION:                   
      REPO:                      
      HELM_DRIVER:               secret
      CHART_NAMESPACE:           kube-system
      CHART:                     
      HELM_VERSION:              
      TARGET_NAMESPACE:          kube-system
      AUTH_PASS_CREDENTIALS:     false
      INSECURE_SKIP_TLS_VERIFY:  false
      PLAIN_HTTP:                false
      NO_PROXY:                  .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      FAILURE_POLICY:            reinstall
    Mounts:
      /chart from content (rw)
      /config from values (rw)
      /home/klipper-helm/.cache from klipper-cache (rw)
      /home/klipper-helm/.config from klipper-config (rw)
      /home/klipper-helm/.helm from klipper-helm (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7c2l4 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  klipper-helm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  values:
    Type:                Projected (a volume that contains injected data from multiple sources)
    SecretName:          chart-values-rke2-snapshot-controller-crd
    SecretOptionalName:  <nil>
  content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      chart-content-rke2-snapshot-controller-crd
    Optional:  false
  kube-api-access-7c2l4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:                 kube-apiserver-orch-tf
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:24:56 +0000
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4d2a9fb26945502dd23b1b4974ed27c8
                      kubernetes.io/config.mirror: 4d2a9fb26945502dd23b1b4974ed27c8
                      kubernetes.io/config.seen: 2026-02-03T17:22:43.269110716Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Node/orch-tf
Containers:
  kube-apiserver:
    Container ID:  containerd://362cbcb54cd374da83a7ae252ab422ded861598c1fb1dba1569c8e5845575244
    Image:         index.docker.io/rancher/hardened-kubernetes:v1.34.1-rke2r1-build20250910
    Image ID:      docker.io/rancher/hardened-kubernetes@sha256:c2cb1e6c0e2ea6fb41c3e57caa444bb7b5197286d2b71f7043483902e56c7868
    Port:          6443/TCP
    Host Port:     6443/TCP
    Command:
      kube-apiserver
    Args:
      --admission-control-config-file=/etc/rancher/rke2/rke2-pss.yaml
      --audit-policy-file=/etc/rancher/rke2/audit-policy.yaml
      --audit-log-maxage=30
      --audit-log-maxbackup=10
      --audit-log-maxsize=100
      --audit-log-path=/var/lib/rancher/rke2/server/logs/audit.log
      --allow-privileged=true
      --anonymous-auth=false
      --api-audiences=https://kubernetes.default.svc.cluster.local,rke2
      --authorization-mode=Node,RBAC
      --bind-address=0.0.0.0
      --cert-dir=/var/lib/rancher/rke2/server/tls/temporary-certs
      --client-ca-file=/var/lib/rancher/rke2/server/tls/client-ca.crt
      --egress-selector-config-file=/var/lib/rancher/rke2/server/etc/egress-selector-config.yaml
      --enable-admission-plugins=NodeRestriction
      --enable-aggregator-routing=true
      --enable-bootstrap-token-auth=true
      --encryption-provider-config=/var/lib/rancher/rke2/server/cred/encryption-config.json
      --encryption-provider-config-automatic-reload=true
      --etcd-cafile=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
      --etcd-certfile=/var/lib/rancher/rke2/server/tls/etcd/client.crt
      --etcd-keyfile=/var/lib/rancher/rke2/server/tls/etcd/client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt
      --kubelet-client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
      --kubelet-client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --profiling=false
      --proxy-client-cert-file=/var/lib/rancher/rke2/server/tls/client-auth-proxy.crt
      --proxy-client-key-file=/var/lib/rancher/rke2/server/tls/client-auth-proxy.key
      --requestheader-allowed-names=system:auth-proxy
      --requestheader-client-ca-file=/var/lib/rancher/rke2/server/tls/request-header-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/var/lib/rancher/rke2/server/tls/service.key
      --service-account-signing-key-file=/var/lib/rancher/rke2/server/tls/service.current.key
      --service-cluster-ip-range=10.43.0.0/16
      --service-node-port-range=30000-32767
      --storage-backend=etcd3
      --tls-cert-file=/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt
      --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
      --tls-private-key-file=/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key
    State:          Running
      Started:      Tue, 03 Feb 2026 17:22:43 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      250m
      memory:   1Gi
    Liveness:   exec [kubectl get --server=https://localhost:6443/ --client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt --client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key --certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt --raw=/livez] delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:  exec [kubectl get --server=https://localhost:6443/ --client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt --client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key --certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt --raw=/readyz] delay=0s timeout=15s period=5s #success=1 #failure=3
    Startup:    exec [kubectl get --server=https://localhost:6443/ --client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt --client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key --certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt --raw=/livez] delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:
      FILE_HASH:  c6b6dd174312fc361fedd6a86ccb32dc391db255cca1997bde9694bc0fc2d5b1
      NO_PROXY:   .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
    Mounts:
      /etc/ca-certificates from dir1 (rw)
      /etc/rancher/rke2/audit-policy.yaml from file1 (ro)
      /etc/rancher/rke2/rke2-pss.yaml from file2 (ro)
      /etc/ssl/certs from dir0 (rw)
      /usr/local/share/ca-certificates from dir2 (rw)
      /usr/share/ca-certificates from dir3 (rw)
      /var/lib/rancher/rke2/server from dir5 (rw)
      /var/lib/rancher/rke2/server/db/etcd/name from file0 (ro)
      /var/lib/rancher/rke2/server/etc/egress-selector-config.yaml from file3 (ro)
      /var/lib/rancher/rke2/server/logs from dir4 (rw)
      /var/lib/rancher/rke2/server/tls/client-auth-proxy.crt from file4 (ro)
      /var/lib/rancher/rke2/server/tls/client-auth-proxy.key from file5 (ro)
      /var/lib/rancher/rke2/server/tls/client-ca.crt from file6 (ro)
      /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt from file7 (ro)
      /var/lib/rancher/rke2/server/tls/client-kube-apiserver.key from file8 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/client.crt from file9 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/client.key from file10 (ro)
      /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt from file11 (ro)
      /var/lib/rancher/rke2/server/tls/request-header-ca.crt from file12 (ro)
      /var/lib/rancher/rke2/server/tls/server-ca.crt from file13 (ro)
      /var/lib/rancher/rke2/server/tls/service.current.key from file14 (ro)
      /var/lib/rancher/rke2/server/tls/service.key from file15 (ro)
      /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt from file16 (ro)
      /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key from file17 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  dir0:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  dir1:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  dir2:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  dir3:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  dir4:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/logs
    HostPathType:  DirectoryOrCreate
  dir5:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server
    HostPathType:  DirectoryOrCreate
  file0:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/db/etcd/name
    HostPathType:  File
  file1:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/rancher/rke2/audit-policy.yaml
    HostPathType:  File
  file2:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/rancher/rke2/rke2-pss.yaml
    HostPathType:  File
  file3:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/etc/egress-selector-config.yaml
    HostPathType:  File
  file4:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-auth-proxy.crt
    HostPathType:  File
  file5:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-auth-proxy.key
    HostPathType:  File
  file6:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-ca.crt
    HostPathType:  File
  file7:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
    HostPathType:  File
  file8:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
    HostPathType:  File
  file9:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/client.crt
    HostPathType:  File
  file10:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/client.key
    HostPathType:  File
  file11:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
    HostPathType:  File
  file12:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/request-header-ca.crt
    HostPathType:  File
  file13:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/server-ca.crt
    HostPathType:  File
  file14:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/service.current.key
    HostPathType:  File
  file15:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/service.key
    HostPathType:  File
  file16:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt
    HostPathType:  File
  file17:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key
    HostPathType:  File
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-controller-manager-orch-tf
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:24:56 +0000
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4c28d36834563903aa387e9a1c86d0c7
                      kubernetes.io/config.mirror: 4c28d36834563903aa387e9a1c86d0c7
                      kubernetes.io/config.seen: 2026-02-03T17:22:47.594578555Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Node/orch-tf
Containers:
  kube-controller-manager:
    Container ID:  containerd://ed2ffc607b97c1663abe16b83799db7bc35b8f151c2aeb137974c24dbc7885ec
    Image:         index.docker.io/rancher/hardened-kubernetes:v1.34.1-rke2r1-build20250910
    Image ID:      docker.io/rancher/hardened-kubernetes@sha256:c2cb1e6c0e2ea6fb41c3e57caa444bb7b5197286d2b71f7043483902e56c7868
    Port:          10257/TCP
    Host Port:     10257/TCP
    Command:
      kube-controller-manager
    Args:
      --permit-port-sharing=true
      --flex-volume-plugin-dir=/var/lib/kubelet/volumeplugins
      --terminated-pod-gc-threshold=1000
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig
      --authorization-kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig
      --bind-address=127.0.0.1
      --cluster-cidr=10.42.0.0/16
      --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
      --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/rke2/server/tls/client-ca.key
      --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
      --cluster-signing-kubelet-client-key-file=/var/lib/rancher/rke2/server/tls/client-ca.key
      --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
      --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/rke2/server/tls/server-ca.key
      --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
      --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/rke2/server/tls/server-ca.key
      --controllers=*,tokencleaner
      --kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig
      --profiling=false
      --root-ca-file=/var/lib/rancher/rke2/server/tls/server-ca.crt
      --secure-port=10257
      --service-account-private-key-file=/var/lib/rancher/rke2/server/tls/service.current.key
      --service-cluster-ip-range=10.43.0.0/16
      --tls-cert-file=/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt
      --tls-private-key-file=/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 03 Feb 2026 17:22:48 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     200m
      memory:  256Mi
    Liveness:  http-get https://localhost:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:   http-get https://localhost:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:
      FILE_HASH:  47cd3d5ad3ecbf631d5207af8214e281df77c2471386967d642b8b70507092d7
      NO_PROXY:   .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
    Mounts:
      /etc/ca-certificates from dir1 (rw)
      /etc/ssl/certs from dir0 (rw)
      /usr/local/share/ca-certificates from dir2 (rw)
      /usr/share/ca-certificates from dir3 (rw)
      /var/lib/rancher/rke2/server/cred/controller.kubeconfig from file1 (ro)
      /var/lib/rancher/rke2/server/db/etcd/name from file0 (ro)
      /var/lib/rancher/rke2/server/tls/client-ca.key from file2 (ro)
      /var/lib/rancher/rke2/server/tls/client-ca.nochain.crt from file3 (ro)
      /var/lib/rancher/rke2/server/tls/client-controller.crt from file4 (ro)
      /var/lib/rancher/rke2/server/tls/client-controller.key from file5 (ro)
      /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt from file6 (ro)
      /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key from file7 (ro)
      /var/lib/rancher/rke2/server/tls/server-ca.crt from file8 (ro)
      /var/lib/rancher/rke2/server/tls/server-ca.key from file9 (ro)
      /var/lib/rancher/rke2/server/tls/server-ca.nochain.crt from file10 (ro)
      /var/lib/rancher/rke2/server/tls/service.current.key from file11 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  dir0:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  dir1:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  dir2:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  dir3:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  file0:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/db/etcd/name
    HostPathType:  File
  file1:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/cred/controller.kubeconfig
    HostPathType:  File
  file2:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-ca.key
    HostPathType:  File
  file3:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
    HostPathType:  File
  file4:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-controller.crt
    HostPathType:  File
  file5:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-controller.key
    HostPathType:  File
  file6:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt
    HostPathType:  File
  file7:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key
    HostPathType:  File
  file8:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/server-ca.crt
    HostPathType:  File
  file9:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/server-ca.key
    HostPathType:  File
  file10:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
    HostPathType:  File
  file11:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/service.current.key
    HostPathType:  File
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-orch-tf
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:22:33 +0000
Labels:               component=kube-proxy
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 1fdedd8b8d9efaa4bcbd9b442615e02f
                      kubernetes.io/config.mirror: 1fdedd8b8d9efaa4bcbd9b442615e02f
                      kubernetes.io/config.seen: 2026-02-03T17:24:56.014984479Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Node/orch-tf
Containers:
  kube-proxy:
    Container ID:  containerd://8a2734d2cdd4049016ee5c7914ef422cd54e734adaf580c6e1c5ad99100642ac
    Image:         index.docker.io/rancher/hardened-kubernetes:v1.34.1-rke2r1-build20250910
    Image ID:      docker.io/rancher/hardened-kubernetes@sha256:c2cb1e6c0e2ea6fb41c3e57caa444bb7b5197286d2b71f7043483902e56c7868
    Port:          10256/TCP
    Host Port:     10256/TCP
    Command:
      kube-proxy
    Args:
      --cluster-cidr=10.42.0.0/16
      --conntrack-max-per-core=0
      --conntrack-tcp-timeout-close-wait=0s
      --conntrack-tcp-timeout-established=0s
      --healthz-bind-address=127.0.0.1
      --hostname-override=orch-tf
      --kubeconfig=/var/lib/rancher/rke2/agent/kubeproxy.kubeconfig
      --proxy-mode=iptables
    State:          Running
      Started:      Tue, 03 Feb 2026 17:27:01 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     250m
      memory:  128Mi
    Liveness:  http-get http://localhost:10256/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Environment:
      FILE_HASH:  6d6be12f1ef5c7695464740a0bc0efa37d719cf4a3a753d83e57b344c7ac2c2a
      NO_PROXY:   .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
    Mounts:
      /var/lib/rancher/rke2/agent/client-kube-proxy.crt from file0 (ro)
      /var/lib/rancher/rke2/agent/client-kube-proxy.key from file1 (ro)
      /var/lib/rancher/rke2/agent/kubeproxy.kubeconfig from file2 (ro)
      /var/lib/rancher/rke2/agent/server-ca.crt from file3 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  file0:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/agent/client-kube-proxy.crt
    HostPathType:  File
  file1:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/agent/client-kube-proxy.key
    HostPathType:  File
  file2:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/agent/kubeproxy.kubeconfig
    HostPathType:  File
  file3:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/agent/server-ca.crt
    HostPathType:  File
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-scheduler-orch-tf
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:24:56 +0000
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: c63b93613dce26679d5c92c6dd34ba83
                      kubernetes.io/config.mirror: c63b93613dce26679d5c92c6dd34ba83
                      kubernetes.io/config.seen: 2026-02-03T17:22:47.593007567Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  Node/orch-tf
Containers:
  kube-scheduler:
    Container ID:  containerd://c21f6a4ffccf7b6a194f720bc88225d02e450746693c92753ad60f192acf10a9
    Image:         index.docker.io/rancher/hardened-kubernetes:v1.34.1-rke2r1-build20250910
    Image ID:      docker.io/rancher/hardened-kubernetes@sha256:c2cb1e6c0e2ea6fb41c3e57caa444bb7b5197286d2b71f7043483902e56c7868
    Port:          10259/TCP
    Host Port:     10259/TCP
    Command:
      kube-scheduler
    Args:
      --permit-port-sharing=true
      --authentication-kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
      --authorization-kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
      --bind-address=127.0.0.1
      --kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
      --profiling=false
      --secure-port=10259
      --tls-cert-file=/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt
      --tls-private-key-file=/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key
    State:          Running
      Started:      Tue, 03 Feb 2026 17:22:48 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   128Mi
    Liveness:   http-get https://localhost:10259/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:  http-get https://localhost:10259/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:    http-get https://localhost:10259/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:
      FILE_HASH:  91dff16e6d6783b055a897e3d01ef76920dc19d58b1f1c7cbc375b3e8a863917
      NO_PROXY:   .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
    Mounts:
      /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig from file1 (ro)
      /var/lib/rancher/rke2/server/db/etcd/name from file0 (ro)
      /var/lib/rancher/rke2/server/tls/client-scheduler.crt from file2 (ro)
      /var/lib/rancher/rke2/server/tls/client-scheduler.key from file3 (ro)
      /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt from file4 (ro)
      /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key from file5 (ro)
      /var/lib/rancher/rke2/server/tls/server-ca.crt from file6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  file0:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/db/etcd/name
    HostPathType:  File
  file1:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
    HostPathType:  File
  file2:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-scheduler.crt
    HostPathType:  File
  file3:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/client-scheduler.key
    HostPathType:  File
  file4:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt
    HostPathType:  File
  file5:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key
    HostPathType:  File
  file6:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/rancher/rke2/server/tls/server-ca.crt
    HostPathType:  File
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 rke2-coredns-rke2-coredns-autoscaler-5fcf54974d-9tv22
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      rke2-coredns-rke2-coredns-autoscaler
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:23:42 +0000
Labels:               app.kubernetes.io/instance=rke2-coredns
                      app.kubernetes.io/name=rke2-coredns-autoscaler
                      k8s-app=kube-dns-autoscaler
                      pod-template-hash=5fcf54974d
Annotations:          checksum/configmap: f46516f128487ef3881cb3f4d5abb9eed85c090de135b2aa2ed42c4a9ea68750
                      cni.projectcalico.org/containerID: e380d4a7409595c51e7fb20e76837be73354071d01b154a5018f9a99b4327366
                      cni.projectcalico.org/podIP: 10.42.65.68/32
                      cni.projectcalico.org/podIPs: 10.42.65.68/32
                      scheduler.alpha.kubernetes.io/tolerations: [{"key":"CriticalAddonsOnly", "operator":"Exists"}]
Status:               Running
IP:                   10.42.65.68
IPs:
  IP:           10.42.65.68
Controlled By:  ReplicaSet/rke2-coredns-rke2-coredns-autoscaler-5fcf54974d
Containers:
  autoscaler:
    Container ID:  containerd://7f3ce3bdd86d6dbd119aa98c9cc8b0e821a2baf1f9ebb559b5574fe3d4db02a1
    Image:         rancher/hardened-cluster-autoscaler:v1.10.2-build20250909
    Image ID:      sha256:8fe3e1adeeee231c218893b251eaf366209bf0c3006e41b9789c1bc1558a2025
    Port:          <none>
    Host Port:     <none>
    Command:
      /cluster-proportional-autoscaler
      --namespace=kube-system
      --configmap=rke2-coredns-rke2-coredns-autoscaler
      --target=Deployment/rke2-coredns-rke2-coredns
      --logtostderr=true
      --v=2
    State:          Running
      Started:      Tue, 03 Feb 2026 17:23:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  64Mi
    Requests:
      cpu:        25m
      memory:     16Mi
    Liveness:     http-get http://:8080/healthz delay=10s timeout=10s period=30s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rx4zs (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-rx4zs:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:                 rke2-coredns-rke2-coredns-bd94c5787-79wgz
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      coredns
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:23:42 +0000
Labels:               app.kubernetes.io/instance=rke2-coredns
                      app.kubernetes.io/name=rke2-coredns
                      k8s-app=kube-dns
                      pod-template-hash=bd94c5787
Annotations:          checksum/config: 9071256d311fdc40a4b187bfdd3582d471ead8afe256ec02ae3027ed5acac308
                      cni.projectcalico.org/containerID: 158bff7074480d76cf2ac3be60aca55b28b7de3c0dcfff72f320973cd336d00d
                      cni.projectcalico.org/podIP: 10.42.65.66/32
                      cni.projectcalico.org/podIPs: 10.42.65.66/32
Status:               Running
IP:                   10.42.65.66
IPs:
  IP:           10.42.65.66
Controlled By:  ReplicaSet/rke2-coredns-rke2-coredns-bd94c5787
Containers:
  coredns:
    Container ID:  containerd://3be6be7939462884aa72847c0a01103b50f45d628f6c0067e290f41fc5ea67a5
    Image:         rancher/hardened-coredns:v1.12.3-build20250909
    Image ID:      sha256:2406bbbbe7ba57443346cfe86081f29a9fae6dc4f476a81d386d206a263c8e8e
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 03 Feb 2026 17:23:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=30s timeout=5s period=5s #success=1 #failure=1
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z5krf (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rke2-coredns-rke2-coredns
    Optional:  false
  kube-api-access-z5krf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:                 rke2-metrics-server-67865bc5f9-l45fl
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      rke2-metrics-server
Node:                 orch-tf/192.168.99.10
Start Time:           Tue, 03 Feb 2026 17:23:58 +0000
Labels:               app=rke2-metrics-server
                      app.kubernetes.io/instance=rke2-metrics-server
                      app.kubernetes.io/name=rke2-metrics-server
                      pod-template-hash=67865bc5f9
Annotations:          cni.projectcalico.org/containerID: 8980d1e897723be42d55522a1bb718202aaa9e208d46e64f3d7982a6887b029f
                      cni.projectcalico.org/podIP: 10.42.65.71/32
                      cni.projectcalico.org/podIPs: 10.42.65.71/32
Status:               Running
IP:                   10.42.65.71
IPs:
  IP:           10.42.65.71
Controlled By:  ReplicaSet/rke2-metrics-server-67865bc5f9
Containers:
  metrics-server:
    Container ID:    containerd://a05b5fe8597941bb66283e46008441d841ecbac2c3b1a26b0701a56b7f5b1d50
    Image:           rancher/hardened-k8s-metrics-server:v0.8.0-build20250909
    Image ID:        sha256:b901f3eee16ce005a069dd0b9ddba7b59333632a0f0d3a03a72a00afd8cf92e9
    Port:            10250/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --secure-port=10250
      --cert-dir=/tmp
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    State:          Running
      Started:      Tue, 03 Feb 2026 17:23:59 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w78q5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-w78q5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  48m (x8 over 81m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:             kyverno-admission-controller-79d9d8c5bb-4jxhn
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-admission-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=admission-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=79d9d8c5bb
Annotations:      cni.projectcalico.org/containerID: 6b8eb4cba1ff547c4bd870260549d310e4d0aea2b18876d4efea5828c3e52c49
                  cni.projectcalico.org/podIP: 10.42.65.96/32
                  cni.projectcalico.org/podIPs: 10.42.65.96/32
Status:           Running
IP:               10.42.65.96
IPs:
  IP:           10.42.65.96
Controlled By:  ReplicaSet/kyverno-admission-controller-79d9d8c5bb
Init Containers:
  kyverno-pre:
    Container ID:    containerd://0e868fc8f904a39b736fcaa7af064252f8b199af2f71aeb7a7db2e40e37350d7
    Image:           reg.kyverno.io/kyverno/kyvernopre:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/kyvernopre@sha256:e9b3a40f8da712abeb1d3896dac7af6d81a96284036d73a214e15d2c5e0f0e09
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Args:
      --loggingFormat=text
      --v=2
      --openreportsEnabled=false
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:31:20 +0000
      Finished:     Tue, 03 Feb 2026 17:31:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-admission-controller
      KYVERNO_ROLE_NAME:            kyverno:admission-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_POD_NAME:             kyverno-admission-controller-79d9d8c5bb-4jxhn (v1:metadata.name)
      KYVERNO_DEPLOYMENT:           kyverno-admission-controller
      KYVERNO_SVC:                  kyverno-svc
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ghkpn (ro)
Containers:
  kyverno:
    Container ID:    containerd://f1f39aca9b0f349a7d2db887a3a2f427dd73ad0b730a8d17f16c6670bf4f8ebe
    Image:           reg.kyverno.io/kyverno/kyverno:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/kyverno@sha256:8d78ccacaf2f961e6be1fceec658f34b7f3410ac24822f0d12812bdb022b355f
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --caSecretName=kyverno-svc.kyverno.svc.kyverno-tls-ca
      --tlsSecretName=kyverno-svc.kyverno.svc.kyverno-tls-pair
      --backgroundServiceAccountName=system:serviceaccount:kyverno:kyverno-background-controller
      --reportsServiceAccountName=system:serviceaccount:kyverno:kyverno-reports-controller
      --servicePort=443
      --webhookServerPort=9443
      --resyncPeriod=15m
      --crdWatcher=false
      --disableMetrics=false
      --otelConfig=prometheus
      --metricsPort=8000
      --admissionReports=true
      --maxAdmissionReports=1000
      --autoUpdateWebhooks=true
      --enableConfigMapCaching=true
      --controllerRuntimeMetricsAddress=:8080
      --enableDeferredLoading=true
      --dumpPayload=false
      --forceFailurePolicyIgnore=false
      --generateValidatingAdmissionPolicy=true
      --generateMutatingAdmissionPolicy=false
      --dumpPatches=false
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --omitEvents=PolicyApplied,PolicySkipped
      --enablePolicyException=true
      --exceptionNamespace=kyverno
      --protectManagedResources=false
      --allowInsecureRegistry=false
      --registryCredentialHelpers=default,google,amazon,azure,github
      --enableReporting=validate,mutate,mutateExisting,imageVerify,generate
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get https://:9443/health/liveness delay=15s timeout=5s period=30s #success=1 #failure=2
    Readiness:  http-get https://:9443/health/readiness delay=5s timeout=5s period=10s #success=1 #failure=6
    Startup:    http-get https://:9443/health/liveness delay=2s timeout=1s period=6s #success=1 #failure=20
    Environment:
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_POD_NAME:             kyverno-admission-controller-79d9d8c5bb-4jxhn (v1:metadata.name)
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-admission-controller
      KYVERNO_ROLE_NAME:            kyverno:admission-controller
      KYVERNO_SVC:                  kyverno-svc
      TUF_ROOT:                     /.sigstore
      KYVERNO_DEPLOYMENT:           kyverno-admission-controller
    Mounts:
      /.sigstore from sigstore (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ghkpn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  sigstore:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-ghkpn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-admission-controller-79d9d8c5bb-d46vw
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-admission-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=admission-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=79d9d8c5bb
Annotations:      cni.projectcalico.org/containerID: c4fc7872be999105d22cab4f12cac6ae5176eb5828a4377c481042c4ad66622c
                  cni.projectcalico.org/podIP: 10.42.65.101/32
                  cni.projectcalico.org/podIPs: 10.42.65.101/32
Status:           Running
IP:               10.42.65.101
IPs:
  IP:           10.42.65.101
Controlled By:  ReplicaSet/kyverno-admission-controller-79d9d8c5bb
Init Containers:
  kyverno-pre:
    Container ID:    containerd://68de59f110c44ac3239b63e9f6a32263e1baa38cca7fa12373598427ac305949
    Image:           reg.kyverno.io/kyverno/kyvernopre:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/kyvernopre@sha256:e9b3a40f8da712abeb1d3896dac7af6d81a96284036d73a214e15d2c5e0f0e09
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Args:
      --loggingFormat=text
      --v=2
      --openreportsEnabled=false
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:31:20 +0000
      Finished:     Tue, 03 Feb 2026 17:31:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-admission-controller
      KYVERNO_ROLE_NAME:            kyverno:admission-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_POD_NAME:             kyverno-admission-controller-79d9d8c5bb-d46vw (v1:metadata.name)
      KYVERNO_DEPLOYMENT:           kyverno-admission-controller
      KYVERNO_SVC:                  kyverno-svc
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lkzf6 (ro)
Containers:
  kyverno:
    Container ID:    containerd://a3c23331a7172265d8e535278754a43879741c2f09fc7d4855dc82dcb689fd20
    Image:           reg.kyverno.io/kyverno/kyverno:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/kyverno@sha256:8d78ccacaf2f961e6be1fceec658f34b7f3410ac24822f0d12812bdb022b355f
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --caSecretName=kyverno-svc.kyverno.svc.kyverno-tls-ca
      --tlsSecretName=kyverno-svc.kyverno.svc.kyverno-tls-pair
      --backgroundServiceAccountName=system:serviceaccount:kyverno:kyverno-background-controller
      --reportsServiceAccountName=system:serviceaccount:kyverno:kyverno-reports-controller
      --servicePort=443
      --webhookServerPort=9443
      --resyncPeriod=15m
      --crdWatcher=false
      --disableMetrics=false
      --otelConfig=prometheus
      --metricsPort=8000
      --admissionReports=true
      --maxAdmissionReports=1000
      --autoUpdateWebhooks=true
      --enableConfigMapCaching=true
      --controllerRuntimeMetricsAddress=:8080
      --enableDeferredLoading=true
      --dumpPayload=false
      --forceFailurePolicyIgnore=false
      --generateValidatingAdmissionPolicy=true
      --generateMutatingAdmissionPolicy=false
      --dumpPatches=false
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --omitEvents=PolicyApplied,PolicySkipped
      --enablePolicyException=true
      --exceptionNamespace=kyverno
      --protectManagedResources=false
      --allowInsecureRegistry=false
      --registryCredentialHelpers=default,google,amazon,azure,github
      --enableReporting=validate,mutate,mutateExisting,imageVerify,generate
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:35 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get https://:9443/health/liveness delay=15s timeout=5s period=30s #success=1 #failure=2
    Readiness:  http-get https://:9443/health/readiness delay=5s timeout=5s period=10s #success=1 #failure=6
    Startup:    http-get https://:9443/health/liveness delay=2s timeout=1s period=6s #success=1 #failure=20
    Environment:
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_POD_NAME:             kyverno-admission-controller-79d9d8c5bb-d46vw (v1:metadata.name)
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-admission-controller
      KYVERNO_ROLE_NAME:            kyverno:admission-controller
      KYVERNO_SVC:                  kyverno-svc
      TUF_ROOT:                     /.sigstore
      KYVERNO_DEPLOYMENT:           kyverno-admission-controller
    Mounts:
      /.sigstore from sigstore (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lkzf6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  sigstore:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-lkzf6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-admission-controller-79d9d8c5bb-spbv5
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-admission-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=admission-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=79d9d8c5bb
Annotations:      cni.projectcalico.org/containerID: 353533f485d0cb376f3711681bac4949e1661dd9449ee98169688d0d8ae4ef36
                  cni.projectcalico.org/podIP: 10.42.65.100/32
                  cni.projectcalico.org/podIPs: 10.42.65.100/32
Status:           Running
IP:               10.42.65.100
IPs:
  IP:           10.42.65.100
Controlled By:  ReplicaSet/kyverno-admission-controller-79d9d8c5bb
Init Containers:
  kyverno-pre:
    Container ID:    containerd://8e25fdf60d5879bcaf9667636fbb8d5a4418e0f01432b54d90e6d2ae4da4f689
    Image:           reg.kyverno.io/kyverno/kyvernopre:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/kyvernopre@sha256:e9b3a40f8da712abeb1d3896dac7af6d81a96284036d73a214e15d2c5e0f0e09
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Args:
      --loggingFormat=text
      --v=2
      --openreportsEnabled=false
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:31:20 +0000
      Finished:     Tue, 03 Feb 2026 17:31:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-admission-controller
      KYVERNO_ROLE_NAME:            kyverno:admission-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_POD_NAME:             kyverno-admission-controller-79d9d8c5bb-spbv5 (v1:metadata.name)
      KYVERNO_DEPLOYMENT:           kyverno-admission-controller
      KYVERNO_SVC:                  kyverno-svc
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2rtfg (ro)
Containers:
  kyverno:
    Container ID:    containerd://b55072eea0023fa995a397bb7e01b2fede79806ca6bffdae77a74108458ee4d2
    Image:           reg.kyverno.io/kyverno/kyverno:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/kyverno@sha256:8d78ccacaf2f961e6be1fceec658f34b7f3410ac24822f0d12812bdb022b355f
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --caSecretName=kyverno-svc.kyverno.svc.kyverno-tls-ca
      --tlsSecretName=kyverno-svc.kyverno.svc.kyverno-tls-pair
      --backgroundServiceAccountName=system:serviceaccount:kyverno:kyverno-background-controller
      --reportsServiceAccountName=system:serviceaccount:kyverno:kyverno-reports-controller
      --servicePort=443
      --webhookServerPort=9443
      --resyncPeriod=15m
      --crdWatcher=false
      --disableMetrics=false
      --otelConfig=prometheus
      --metricsPort=8000
      --admissionReports=true
      --maxAdmissionReports=1000
      --autoUpdateWebhooks=true
      --enableConfigMapCaching=true
      --controllerRuntimeMetricsAddress=:8080
      --enableDeferredLoading=true
      --dumpPayload=false
      --forceFailurePolicyIgnore=false
      --generateValidatingAdmissionPolicy=true
      --generateMutatingAdmissionPolicy=false
      --dumpPatches=false
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --omitEvents=PolicyApplied,PolicySkipped
      --enablePolicyException=true
      --exceptionNamespace=kyverno
      --protectManagedResources=false
      --allowInsecureRegistry=false
      --registryCredentialHelpers=default,google,amazon,azure,github
      --enableReporting=validate,mutate,mutateExisting,imageVerify,generate
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get https://:9443/health/liveness delay=15s timeout=5s period=30s #success=1 #failure=2
    Readiness:  http-get https://:9443/health/readiness delay=5s timeout=5s period=10s #success=1 #failure=6
    Startup:    http-get https://:9443/health/liveness delay=2s timeout=1s period=6s #success=1 #failure=20
    Environment:
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_POD_NAME:             kyverno-admission-controller-79d9d8c5bb-spbv5 (v1:metadata.name)
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-admission-controller
      KYVERNO_ROLE_NAME:            kyverno:admission-controller
      KYVERNO_SVC:                  kyverno-svc
      TUF_ROOT:                     /.sigstore
      KYVERNO_DEPLOYMENT:           kyverno-admission-controller
    Mounts:
      /.sigstore from sigstore (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2rtfg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  sigstore:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-2rtfg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-background-controller-65bbf6547c-b84cj
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-background-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=background-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=65bbf6547c
Annotations:      cni.projectcalico.org/containerID: ce9bb25f8caee8b839287b93821b7c1658efc8f8b07384990466017e73e06414
                  cni.projectcalico.org/podIP: 10.42.65.98/32
                  cni.projectcalico.org/podIPs: 10.42.65.98/32
Status:           Running
IP:               10.42.65.98
IPs:
  IP:           10.42.65.98
Controlled By:  ReplicaSet/kyverno-background-controller-65bbf6547c
Containers:
  controller:
    Container ID:    containerd://87060c01100a6ec737017fa31a8d08e11020c4e248717dc53e679d3a2a958674
    Image:           reg.kyverno.io/kyverno/background-controller:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/background-controller@sha256:1243c89f8d0e1ac3194c380e31571236af49aec531251bc724f1e5c51de57feb
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --disableMetrics=false
      --otelConfig=prometheus
      --metricsPort=8000
      --resyncPeriod=15m
      --enableConfigMapCaching=true
      --enableDeferredLoading=true
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --omitEvents=PolicyApplied,PolicySkipped
      --enablePolicyException=true
      --exceptionNamespace=kyverno
      --enableReporting=validate,mutate,mutateExisting,imageVerify,generate
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-background-controller
      KYVERNO_DEPLOYMENT:           kyverno-background-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_POD_NAME:             kyverno-background-controller-65bbf6547c-b84cj (v1:metadata.name)
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqm5p (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-nqm5p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-background-controller-65bbf6547c-ldkbq
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-background-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=background-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=65bbf6547c
Annotations:      cni.projectcalico.org/containerID: b9352f620c4108e2acc8edf81ad43cbbe06105e6e38ae549d70900ffc66f9205
                  cni.projectcalico.org/podIP: 10.42.65.103/32
                  cni.projectcalico.org/podIPs: 10.42.65.103/32
Status:           Running
IP:               10.42.65.103
IPs:
  IP:           10.42.65.103
Controlled By:  ReplicaSet/kyverno-background-controller-65bbf6547c
Containers:
  controller:
    Container ID:    containerd://388b3c566abcc4457e044973ddffbd187cdf4b2cc7fd2ac3149837f558a1b58b
    Image:           reg.kyverno.io/kyverno/background-controller:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/background-controller@sha256:1243c89f8d0e1ac3194c380e31571236af49aec531251bc724f1e5c51de57feb
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --disableMetrics=false
      --otelConfig=prometheus
      --metricsPort=8000
      --resyncPeriod=15m
      --enableConfigMapCaching=true
      --enableDeferredLoading=true
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --omitEvents=PolicyApplied,PolicySkipped
      --enablePolicyException=true
      --exceptionNamespace=kyverno
      --enableReporting=validate,mutate,mutateExisting,imageVerify,generate
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:24 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-background-controller
      KYVERNO_DEPLOYMENT:           kyverno-background-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_POD_NAME:             kyverno-background-controller-65bbf6547c-ldkbq (v1:metadata.name)
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ql2xw (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-ql2xw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-cleanup-controller-665494d4b-g87lf
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-cleanup-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=cleanup-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=665494d4b
Annotations:      cni.projectcalico.org/containerID: 99698c94e8929cbc102f8c734946129f363235cc169db799cb8a971942def7c5
                  cni.projectcalico.org/podIP: 10.42.65.102/32
                  cni.projectcalico.org/podIPs: 10.42.65.102/32
Status:           Running
IP:               10.42.65.102
IPs:
  IP:           10.42.65.102
Controlled By:  ReplicaSet/kyverno-cleanup-controller-665494d4b
Containers:
  controller:
    Container ID:    containerd://6a58afcf42be5741225f540fbcfb50e4cf37f3f3964e6ee934df4e47222f5e34
    Image:           reg.kyverno.io/kyverno/cleanup-controller:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/cleanup-controller@sha256:69e1e6abf5f8ab1f015a1f185b1016b189c7a1713791dd662ad2c7e610b6f359
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --caSecretName=kyverno-cleanup-controller.kyverno.svc.kyverno-tls-ca
      --tlsSecretName=kyverno-cleanup-controller.kyverno.svc.kyverno-tls-pair
      --servicePort=443
      --resyncPeriod=15m
      --cleanupServerPort=9443
      --disableMetrics=false
      --otelConfig=prometheus
      --metricsPort=8000
      --enableDeferredLoading=true
      --dumpPayload=false
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --protectManagedResources=false
      --ttlReconciliationInterval=1m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get https://:9443/health/liveness delay=15s timeout=5s period=30s #success=1 #failure=2
    Readiness:  http-get https://:9443/health/readiness delay=5s timeout=5s period=10s #success=1 #failure=6
    Startup:    http-get https://:9443/health/liveness delay=2s timeout=1s period=6s #success=1 #failure=20
    Environment:
      KYVERNO_DEPLOYMENT:           kyverno-cleanup-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_POD_NAME:             kyverno-cleanup-controller-665494d4b-g87lf (v1:metadata.name)
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-cleanup-controller
      KYVERNO_ROLE_NAME:            kyverno:cleanup-controller
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_SVC:                  kyverno-cleanup-controller
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89jx9 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-89jx9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-cleanup-controller-665494d4b-q6brw
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-cleanup-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=cleanup-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=665494d4b
Annotations:      cni.projectcalico.org/containerID: 4cee5750d82a8f4f553456edd537262bb118fc1523ecad0951cf688672af86d6
                  cni.projectcalico.org/podIP: 10.42.65.97/32
                  cni.projectcalico.org/podIPs: 10.42.65.97/32
Status:           Running
IP:               10.42.65.97
IPs:
  IP:           10.42.65.97
Controlled By:  ReplicaSet/kyverno-cleanup-controller-665494d4b
Containers:
  controller:
    Container ID:    containerd://caf926224b2bd3f6a5d6794926ea788ad9b69a0b14b19756d2e5186a27eb77d7
    Image:           reg.kyverno.io/kyverno/cleanup-controller:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/cleanup-controller@sha256:69e1e6abf5f8ab1f015a1f185b1016b189c7a1713791dd662ad2c7e610b6f359
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --caSecretName=kyverno-cleanup-controller.kyverno.svc.kyverno-tls-ca
      --tlsSecretName=kyverno-cleanup-controller.kyverno.svc.kyverno-tls-pair
      --servicePort=443
      --resyncPeriod=15m
      --cleanupServerPort=9443
      --disableMetrics=false
      --otelConfig=prometheus
      --metricsPort=8000
      --enableDeferredLoading=true
      --dumpPayload=false
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --protectManagedResources=false
      --ttlReconciliationInterval=1m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get https://:9443/health/liveness delay=15s timeout=5s period=30s #success=1 #failure=2
    Readiness:  http-get https://:9443/health/readiness delay=5s timeout=5s period=10s #success=1 #failure=6
    Startup:    http-get https://:9443/health/liveness delay=2s timeout=1s period=6s #success=1 #failure=20
    Environment:
      KYVERNO_DEPLOYMENT:           kyverno-cleanup-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_POD_NAME:             kyverno-cleanup-controller-665494d4b-q6brw (v1:metadata.name)
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-cleanup-controller
      KYVERNO_ROLE_NAME:            kyverno:cleanup-controller
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      KYVERNO_SVC:                  kyverno-cleanup-controller
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x9jwk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-x9jwk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-reports-controller-77c6546f48-q2xdc
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-reports-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=reports-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=77c6546f48
Annotations:      cni.projectcalico.org/containerID: f118bfdd28a8e5540b0abb0ed7aba864171a1abb0f43d4cc1108d1caf0e026c1
                  cni.projectcalico.org/podIP: 10.42.65.99/32
                  cni.projectcalico.org/podIPs: 10.42.65.99/32
Status:           Running
IP:               10.42.65.99
IPs:
  IP:           10.42.65.99
Controlled By:  ReplicaSet/kyverno-reports-controller-77c6546f48
Containers:
  controller:
    Container ID:    containerd://ee4d375121f4a9edeadce3138fc6c7d30484c0ca6edc4d384e9c941867c35c82
    Image:           reg.kyverno.io/kyverno/reports-controller:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/reports-controller@sha256:84291952cc15b3968c29a8a91370d6e8a4b9749136427c1f1be6a03f42703c6a
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --disableMetrics=false
      --openreportsEnabled=false
      --otelConfig=prometheus
      --metricsPort=8000
      --resyncPeriod=15m
      --admissionReports=true
      --aggregateReports=true
      --policyReports=true
      --validatingAdmissionPolicyReports=true
      --mutatingAdmissionPolicyReports=false
      --backgroundScan=true
      --backgroundScanWorkers=2
      --backgroundScanInterval=1h
      --skipResourceFilters=true
      --enableConfigMapCaching=true
      --enableDeferredLoading=true
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --omitEvents=PolicyApplied,PolicySkipped
      --enablePolicyException=true
      --exceptionNamespace=kyverno
      --allowInsecureRegistry=false
      --registryCredentialHelpers=default,google,amazon,azure,github
      --enableReporting=validate,mutate,mutateExisting,imageVerify,generate
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-reports-controller
      KYVERNO_DEPLOYMENT:           kyverno-reports-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_POD_NAME:             kyverno-reports-controller-77c6546f48-q2xdc (v1:metadata.name)
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      TUF_ROOT:                     /.sigstore
    Mounts:
      /.sigstore from sigstore (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jcfw9 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  sigstore:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jcfw9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             kyverno-reports-controller-77c6546f48-ql2zz
Namespace:        kyverno
Priority:         0
Service Account:  kyverno-reports-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:15 +0000
Labels:           app.kubernetes.io/component=reports-controller
                  app.kubernetes.io/instance=kyverno
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/part-of=kyverno
                  app.kubernetes.io/version=3.6.1
                  helm.sh/chart=kyverno-3.6.1
                  pod-template-hash=77c6546f48
Annotations:      cni.projectcalico.org/containerID: 17eba0744a660c1e2bd1dbe56d68ffb6ae06355dbe9bf4d3b2912ceb35c40275
                  cni.projectcalico.org/podIP: 10.42.65.95/32
                  cni.projectcalico.org/podIPs: 10.42.65.95/32
Status:           Running
IP:               10.42.65.95
IPs:
  IP:           10.42.65.95
Controlled By:  ReplicaSet/kyverno-reports-controller-77c6546f48
Containers:
  controller:
    Container ID:    containerd://d6698326da3e6c6f627884130d80aa0605021495ad1f1bcf39d446382ab926cf
    Image:           reg.kyverno.io/kyverno/reports-controller:v1.16.1
    Image ID:        reg.kyverno.io/kyverno/reports-controller@sha256:84291952cc15b3968c29a8a91370d6e8a4b9749136427c1f1be6a03f42703c6a
    Ports:           9443/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --disableMetrics=false
      --openreportsEnabled=false
      --otelConfig=prometheus
      --metricsPort=8000
      --resyncPeriod=15m
      --admissionReports=true
      --aggregateReports=true
      --policyReports=true
      --validatingAdmissionPolicyReports=true
      --mutatingAdmissionPolicyReports=false
      --backgroundScan=true
      --backgroundScanWorkers=2
      --backgroundScanInterval=1h
      --skipResourceFilters=true
      --enableConfigMapCaching=true
      --enableDeferredLoading=true
      --maxAPICallResponseLength=2000000
      --loggingFormat=text
      --v=2
      --omitEvents=PolicyApplied,PolicySkipped
      --enablePolicyException=true
      --exceptionNamespace=kyverno
      --allowInsecureRegistry=false
      --registryCredentialHelpers=default,google,amazon,azure,github
      --enableReporting=validate,mutate,mutateExisting,imageVerify,generate
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      KYVERNO_SERVICEACCOUNT_NAME:  kyverno-reports-controller
      KYVERNO_DEPLOYMENT:           kyverno-reports-controller
      INIT_CONFIG:                  kyverno
      METRICS_CONFIG:               kyverno-metrics
      KYVERNO_POD_NAME:             kyverno-reports-controller-77c6546f48-ql2zz (v1:metadata.name)
      KYVERNO_NAMESPACE:            kyverno (v1:metadata.namespace)
      TUF_ROOT:                     /.sigstore
    Mounts:
      /.sigstore from sigstore (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gdwg8 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  sigstore:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-gdwg8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             metallb-controller-6c6859987b-n2lqp
Namespace:        metallb-system
Priority:         0
Service Account:  metallb-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:09 +0000
Labels:           app.kubernetes.io/component=controller
                  app.kubernetes.io/instance=metallb
                  app.kubernetes.io/name=metallb
                  pod-template-hash=6c6859987b
Annotations:      cni.projectcalico.org/containerID: 609ca505af2f365176e55a6772e1ea5515f43afc9dc2ddddde8065473d120b3a
                  cni.projectcalico.org/podIP: 10.42.65.94/32
                  cni.projectcalico.org/podIPs: 10.42.65.94/32
Status:           Running
IP:               10.42.65.94
IPs:
  IP:           10.42.65.94
Controlled By:  ReplicaSet/metallb-controller-6c6859987b
Containers:
  controller:
    Container ID:  containerd://9b7aa388648e4f685fd7de9c7293f260083167291794c89507a1d1eaeedec425
    Image:         quay.io/metallb/controller:v0.15.2
    Image ID:      quay.io/metallb/controller@sha256:417cdb6d6f9f2c410cceb84047d3a4da3bfb78b5ddfa30f4cf35ea5c667e8c2e
    Ports:         7472/TCP, 9443/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      --port=7472
      --log-level=info
      --tls-min-version=VersionTLS12
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:12 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:monitoring/metrics delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:monitoring/metrics delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      METALLB_ML_SECRET_NAME:  metallb-memberlist
      METALLB_DEPLOYMENT:      metallb-controller
    Mounts:
      /tmp/k8s-webhook-server/serving-certs from cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fmfpl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  metallb-webhook-cert
    Optional:    false
  kube-api-access-fmfpl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             metallb-speaker-ct7vb
Namespace:        metallb-system
Priority:         0
Service Account:  metallb-speaker
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:09 +0000
Labels:           app.kubernetes.io/component=speaker
                  app.kubernetes.io/instance=metallb
                  app.kubernetes.io/name=metallb
                  controller-revision-hash=744b9955d
                  pod-template-generation=1
Annotations:      <none>
Status:           Running
IP:               192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  DaemonSet/metallb-speaker
Containers:
  speaker:
    Container ID:  containerd://84fe34e57298fe5bb836d6edf723249669dc82e3188768b3aed8b11cea7b5d3f
    Image:         quay.io/metallb/speaker:v0.15.2
    Image ID:      quay.io/metallb/speaker@sha256:260c9406f957c0830d4e6cd2e9ac8c05e51ac959dd2462c4c2269ac43076665a
    Ports:         7472/TCP, 7946/TCP, 7946/UDP
    Host Ports:    7472/TCP, 7946/TCP, 7946/UDP
    Args:
      --port=7472
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:16 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:monitoring/metrics delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:monitoring/metrics delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      METALLB_NODE_NAME:            (v1:spec.nodeName)
      METALLB_HOST:                 (v1:status.hostIP)
      METALLB_ML_BIND_ADDR:         (v1:status.podIP)
      METALLB_ML_LABELS:           app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker
      METALLB_ML_BIND_PORT:        7946
      METALLB_ML_SECRET_KEY_PATH:  /etc/ml_secret_key
      METALLB_POD_NAME:            metallb-speaker-ct7vb (v1:metadata.name)
    Mounts:
      /etc/metallb from metallb-excludel2 (rw)
      /etc/ml_secret_key from memberlist (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xkttv (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  memberlist:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  metallb-memberlist
    Optional:    false
  metallb-excludel2:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      metallb-excludel2
    Optional:  false
  kube-api-access-xkttv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/master:NoSchedule op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  52m (x2 over 52m)  kubelet  Readiness probe failed: Get "http://192.168.99.10:7472/metrics": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  51m (x2 over 52m)  kubelet  Liveness probe failed: Get "http://192.168.99.10:7472/metrics": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             namespace-label-78321e26-dvpfr
Namespace:        ns-label
Priority:         0
Service Account:  namespace-label
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:30:48 +0000
Labels:           batch.kubernetes.io/controller-uid=aef797ed-e65d-4ffa-b427-c8305c02d414
                  batch.kubernetes.io/job-name=namespace-label-78321e26
                  controller-uid=aef797ed-e65d-4ffa-b427-c8305c02d414
                  job-name=namespace-label-78321e26
Annotations:      checksum/job: 78321e26cbb7b4bb2da9a716b12f451f7870c790c735e333231fbd2d6fd205a0
                  cni.projectcalico.org/containerID: 07009b4e4ab10cd469965e1999f4000512824dce9299dd93423c9f52a1720752
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.87
IPs:
  IP:           10.42.65.87
Controlled By:  Job/namespace-label-78321e26
Containers:
  ns-label-container:
    Container ID:    containerd://bd4348d1fe2336bf69b35c24f7b1dee93f7c9b94a37930edf3fc6f7813896627
    Image:           alpine/kubectl:1.34.1
    Image ID:        docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
      -c
    Args:
      
      set -e;
      kubectl create namespace orch-infra --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-infra istio-injection=enabled --overwrite;
      kubectl create namespace orch-iam --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-iam istio-injection=enabled --overwrite;
      kubectl create namespace orch-app --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-app istio-injection=enabled --overwrite;
      kubectl create namespace orch-cluster --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-cluster istio-injection=enabled --overwrite;
      kubectl create namespace orch-ui --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-ui istio-injection=enabled --overwrite;
      kubectl create namespace orch-platform --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-platform istio-injection=enabled --overwrite;
      kubectl create namespace orch-harbor --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-harbor istio-injection=enabled --overwrite;
      kubectl create namespace orch-gateway --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-gateway istio-injection=enabled --overwrite;
      kubectl create namespace orch-sre --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-sre istio-injection=enabled --overwrite;
      kubectl create namespace orch-database --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-database istio-injection=enabled --overwrite;
      kubectl create namespace orch-boots --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-boots istio-injection=enabled --overwrite;
      kubectl create namespace orch-secret --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace orch-secret istio-injection=enabled --overwrite;
      kubectl create namespace fleet-default --dry-run=client -o yaml | kubectl apply -f -;
      kubectl label namespace fleet-default app.edge-orchestrator.intel.com/fleet-rs-secret=true --overwrite;
      kubectl create namespace capi-variables --dry-run=client -o yaml | kubectl apply -f -;
      echo "Namespaces have been processed successfully.";
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:30:50 +0000
      Finished:     Tue, 03 Feb 2026 17:30:56 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8m782 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-8m782:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             wait-istio-job-77372bdf-pcrb7
Namespace:        ns-label
Priority:         0
Service Account:  job-wait-namespace-label
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:32:29 +0000
Labels:           batch.kubernetes.io/controller-uid=95516611-02ee-4121-be6a-7982ec2e1ee4
                  batch.kubernetes.io/job-name=wait-istio-job-77372bdf
                  controller-uid=95516611-02ee-4121-be6a-7982ec2e1ee4
                  job-name=wait-istio-job-77372bdf
Annotations:      checksum/job: 77372bdfa81a019951ae7ebe3ad75d676ed3b37f922dfc64eebb58164425e3b7
                  cni.projectcalico.org/containerID: d9271a4dcf412c797683f1414f7686c39a8581f11092d445acb16d8f03d5fc6c
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.105
IPs:
  IP:           10.42.65.105
Controlled By:  Job/wait-istio-job-77372bdf
Containers:
  job-wait-container:
    Container ID:    containerd://e4c51eaf97726f7cda6bede774571f8c10c9783c504cffe98d99677f657f0d49
    Image:           alpine/kubectl:1.34.1
    Image ID:        docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
      -c
    Args:
      
      set -e;
      
      jobs=""
      until [ -n "$jobs" ]; do
        jobs=$(kubectl get jobs -n ns-label -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -E "^namespace-label")
        sleep 1
      done
      echo "Found jobs with prefix namespace-label: $(echo $jobs |tr '\n' ' ')"
      
      for job in $jobs; do
        echo "Waiting for job $job to complete"
        until kubectl wait --for=condition=complete --timeout=1s job/$job -n ns-label; do
          sleep 1
        done
        echo "Job $job is completed"
      done;
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:32:30 +0000
      Finished:     Tue, 03 Feb 2026 17:32:33 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:        10m
      memory:     16Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-snzz6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-snzz6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             openebs-localpv-localpv-provisioner-59fd56b64c-5blfw
Namespace:        openebs-system
Priority:         0
Service Account:  openebs-localpv-localpv-provisioner
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:24:31 +0000
Labels:           app=localpv-provisioner
                  chart=localpv-provisioner-4.3.0
                  component=localpv-provisioner
                  heritage=Helm
                  name=openebs-localpv-provisioner
                  openebs.io/component-name=openebs-localpv-provisioner
                  openebs.io/logging=true
                  pod-template-hash=59fd56b64c
                  release=openebs-localpv
Annotations:      cni.projectcalico.org/containerID: b89b2dadf42f522142eea84120da99d0868c88c755dc274d2d27e73c8e908d47
                  cni.projectcalico.org/podIP: 10.42.65.72/32
                  cni.projectcalico.org/podIPs: 10.42.65.72/32
Status:           Running
IP:               10.42.65.72
IPs:
  IP:           10.42.65.72
Controlled By:  ReplicaSet/openebs-localpv-localpv-provisioner-59fd56b64c
Containers:
  openebs-localpv-localpv-provisioner:
    Container ID:   containerd://baf79ca3b6cf5a0cd5ec446e1aaeafa8cca7e7ea239b8294a76994e138bd59e0
    Image:          openebs/provisioner-localpv:4.3.0
    Image ID:       docker.io/openebs/provisioner-localpv@sha256:e898af5631b64cba55be0d478cee3f422cdaf03e95c55f95d38193cac5dc1fe6
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:24:33 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       exec [sh -c test `pgrep -c "^provisioner-loc.*"` = 1] delay=30s timeout=1s period=60s #success=1 #failure=3
    Environment:
      OPENEBS_NAMESPACE:                   openebs-system (v1:metadata.namespace)
      NODE_NAME:                            (v1:spec.nodeName)
      OPENEBS_SERVICE_ACCOUNT:              (v1:spec.serviceAccountName)
      OPENEBS_IO_ENABLE_ANALYTICS:         true
      OPENEBS_IO_BASE_PATH:                /var/openebs/local
      OPENEBS_IO_HELPER_IMAGE:             openebs/linux-utils:4.2.0
      OPENEBS_IO_HELPER_POD_HOST_NETWORK:  false
      OPENEBS_IO_INSTALLER_TYPE:           localpv-charts-helm
      LEADER_ELECTION_ENABLED:             true
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9vkvb (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-9vkvb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 5s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 5s
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: runAsNonRoot != true, field error list: [spec.containers[0].securityContext.runAsNonRoot: Required value])(Forbidden reason: seccompProfile, field error list: [spec.containers[0].securityContext.seccompProfile.type: Required value])(Forbidden reason: allowPrivilegeEscalation != false, field error list: [spec.containers[0].securityContext.allowPrivilegeEscalation: Required value])(Forbidden reason: unrestricted capabilities, field error list: [spec.containers[0].securityContext.capabilities.drop: Required value])
  Warning  PolicyViolation  13m   kyverno-scan  policy require-ro-rootfs/validate-readOnlyRootFilesystem fail: validation error: Root filesystem must be read-only. rule validate-readOnlyRootFilesystem failed at path /spec/containers/0/securityContext/


Name:             app-deployment-api-65c68f8bf6-4fr2h
Namespace:        orch-app
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:21 +0000
Labels:           app=app-deployment-api
                  pod-template-hash=65c68f8bf6
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=app-deployment-api
                  service.istio.io/canonical-revision=2.6.0
                  version=2.6.0
Annotations:      cni.projectcalico.org/containerID: 664395faef1809339292b9441723a8cb514e48758d3da5b81cf7f1e76e6eca7a
                  cni.projectcalico.org/podIP: 10.42.65.229/32
                  cni.projectcalico.org/podIPs: 10.42.65.229/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: app-deployment-api-rest-proxy
                  kubectl.kubernetes.io/default-logs-container: app-deployment-api-rest-proxy
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.229
IPs:
  IP:           10.42.65.229
Controlled By:  ReplicaSet/app-deployment-api-65c68f8bf6
Init Containers:
  istio-init:
    Container ID:  containerd://b95adf370aac4e3a7ac4cadfcefa0ff389db7cada347112e4ca06543405e2b2f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:25 +0000
      Finished:     Tue, 03 Feb 2026 17:53:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xn69r (ro)
  istio-proxy:
    Container ID:  containerd://9b472c054dfda17709d16ef458029d1ebfdcb618f07f2e28a9b43d7936d0eece
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:30 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      app-deployment-api-65c68f8bf6-4fr2h (v1:metadata.name)
      POD_NAMESPACE:                 orch-app (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8081,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     app-deployment-api-rest-proxy,app-deployment-api,openpolicyagent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      app-deployment-api
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-app/deployments/app-deployment-api
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/app-deployment-api-rest-proxy/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/app-deployment-api-rest-proxy/readyz":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xn69r (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  app-deployment-api-rest-proxy:
    Container ID:    containerd://ea809df944d86a86f94ebd3235fa85bba8ad6c3a965835af272567142ad7376b
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-gateway:2.6.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-gateway@sha256:8ccb4946b06fb4cf665607a287f3fdeff56a47d99b72e2cccf9e37c79c2ffdf3
    Port:            8081/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/local/bin/rest-proxy
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/app-deployment-api-rest-proxy/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/app-deployment-api-rest-proxy/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MSG_SIZE_LIMIT:                 1
      OIDC_TLS_INSECURE_SKIP_VERIFY:  false
      GIN_MODE:                       release
    Mounts:
      /etc/dazl from logging (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xn69r (ro)
  app-deployment-api:
    Container ID:    containerd://2a24bbcb12b2c4354587f79f464e8b8402070de4eb9e4bcbf98627e94886fc26
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-gateway:2.6.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-gateway@sha256:8ccb4946b06fb4cf665607a287f3fdeff56a47d99b72e2cccf9e37c79c2ffdf3
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/local/bin/app-deployment-manager
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      USE_M2M_TOKEN:              true
      SECRET_SERVICE_ENABLED:     true
      MSG_SIZE_LIMIT:             1
      CATALOG_SERVICE_ADDRESS:    app-orch-catalog-grpc-server:8080
      SECRET_SERVICE_ENDPOINT:    http://vault.orch-platform.svc.cluster.local:8200
      KEYCLOAK_SERVICE_ENDPOINT:  http://platform-keycloak.orch-platform.svc.cluster.local:8080
      SERVICE_ACCOUNT:            orch-svc
      RATE_LIMITER_QPS:           30
      RATE_LIMITER_BURST:         2000
      OIDC_SERVER_URL:            http://platform-keycloak.orch-platform.svc/realms/master
      OPA_PORT:                   8181
      OPA_ENABLED:                true
      RS_PROXY_REPO:              oci://rs-proxy.orch-platform.svc.cluster.local:8443
      RS_PROXY_REPO_SECRET:       fleet-rs-secret
      RS_PROXY_REMOTE_NS:         orch-platform
    Mounts:
      /etc/dazl from logging (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xn69r (ro)
  openpolicyagent:
    Container ID:    containerd://4c7047517bb20fa2ed82d98859f0ba21420cc0dd29ab8bf6e9c1c420dac7605d
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            8181/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      /opt/app-deployment-manager-api/rego
      --log-level
      info
      --addr
      :8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /opt/app-deployment-manager-api/rego from openpolicyagent (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xn69r (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  logging:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-deployment-api-logging
    Optional:  false
  openpolicyagent:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-deployment-api-opa-rego
    Optional:  false
  kube-api-access-xn69r:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  52m                 default-scheduler  Successfully assigned orch-app/app-deployment-api-65c68f8bf6-4fr2h to orch-tf
  Normal   Pulled     52m                 kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                 kubelet            Created container: istio-init
  Normal   Started    52m                 kubelet            Started container istio-init
  Normal   Pulled     52m                 kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                 kubelet            Created container: istio-proxy
  Normal   Started    52m                 kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x14 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.229:15021/healthz/ready": dial tcp 10.42.65.229:15021: connect: connection refused
  Normal   Pulling    52m                 kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-gateway:2.6.0"
  Normal   Pulled     52m                 kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-gateway:2.6.0" in 17.902s (17.902s including waiting). Image size: 36949965 bytes.
  Normal   Created    52m                 kubelet            Created container: app-deployment-api-rest-proxy
  Normal   Started    52m                 kubelet            Started container app-deployment-api-rest-proxy
  Normal   Pulled     52m                 kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-gateway:2.6.0" already present on machine
  Normal   Created    52m                 kubelet            Created container: app-deployment-api
  Normal   Started    52m                 kubelet            Started container app-deployment-api
  Normal   Pulled     52m                 kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    52m                 kubelet            Created container: openpolicyagent
  Normal   Started    52m                 kubelet            Started container openpolicyagent


Name:             app-deployment-manager-65db96c7cf-77fqn
Namespace:        orch-app
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:22 +0000
Labels:           app=app-deployment-manager
                  app.kubernetes.io/component=controller
                  app.kubernetes.io/instance=app-deployment-manager
                  app.kubernetes.io/name=app-deployment-manager
                  pod-template-hash=65db96c7cf
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=app-deployment-manager
                  service.istio.io/canonical-revision=2.6.0
                  version=2.6.0
Annotations:      cni.projectcalico.org/containerID: d88b749a01b37e86111832b2e294b4c774f4f2fd953ce6b4c8995ec8d27b3b82
                  cni.projectcalico.org/podIP: 10.42.65.230/32
                  cni.projectcalico.org/podIPs: 10.42.65.230/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: controller
                  kubectl.kubernetes.io/default-logs-container: controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.230
IPs:
  IP:           10.42.65.230
Controlled By:  ReplicaSet/app-deployment-manager-65db96c7cf
Init Containers:
  istio-init:
    Container ID:  containerd://dba0b92c5dacda238a05388864a70936480a8240fe91887930662a1ce5b13da4
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:32 +0000
      Finished:     Tue, 03 Feb 2026 17:53:35 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-njzpg (ro)
  istio-proxy:
    Container ID:  containerd://0a597b86d2131a40c8a5100c3c36a4be9f5cad545059c6049d578c7479536c29
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      app-deployment-manager-65db96c7cf-77fqn (v1:metadata.name)
      POD_NAMESPACE:                 orch-app (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      app-deployment-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-app/deployments/app-deployment-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/controller/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/controller/readyz":{"httpGet":{"path":"/readyz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-njzpg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  controller:
    Container ID:    containerd://93ef998bfa32af71c172032ea89d9d9e98413bff2d8167caf260959358c6547f
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-controller:2.6.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-controller@sha256:ef46871cca78cd42bdd9a509be7189ac05446824e82d7cf9e4612e42a4eb1c37
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /manager
    Args:
      --health-probe-bind-address=:8081
      --metrics-bind-address=:8080
      --zap-log-level=2
      --git-ca-cert-folder=/tmp/ssl/certs/
      --git-ca-cert-file=ca.crt
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:10 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/controller/livez delay=15s timeout=1s period=20s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/controller/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DEFAULT_NAMESPACE:                          orch-apps
      SECRET_SERVICE_ENABLED:                     true
      SECRET_SERVICE_ENDPOINT:                    http://vault.orch-platform.svc.cluster.local:8200
      KEYCLOAK_SERVICE_ENDPOINT:                  http://platform-keycloak.orch-platform.svc.cluster.local:8080
      GIT_SERVER:                                 https://gitea-http.gitea.svc.cluster.local
      GIT_PROVIDER:                               gitea
      GIT_CA_CERT:                                <set to the key 'ca.crt' in secret 'gitea-ca-cert'>  Optional: false
      SERVICE_ACCOUNT:                            orch-svc
      SECRET_SERVICE_MOUNT:                       secret
      SECRET_GIT_SERVICE_PATH:                    ma_git_service
      SECRET_GIT_SERVICE_USERNAME_KVKEY:          username
      SECRET_GIT_SERVICE_PASSWORD_KVKEY:          password
      SECRET_AWS_SERVICE_PATH:                    ma_aws_service
      SECRET_AWS_SERVICE_REGION_KVKEY:            region
      SECRET_AWS_SERVICE_ACCESSKEY_KVKEY:         accessKeyID
      SECRET_AWS_SERVICE_SECRET_ACCESSKEY_KVKEY:  secretAccessKey
      SECRET_AWS_SERVICE_SECRET_SSHKEY_KVKEY:     sshKeyID
      SECRET_HARBOR_SERVICE_PATH:                 ma_harbor_service
      SECRET_HARBOR_SERVICE_CERT_KVKEY:           cacerts
      CATALOG_SERVICE_ADDRESS:                    app-orch-catalog-grpc-server:8080
      GITEA_DELETE_REPO_ON_TERMINATE:             true
      FLEET_ADD_GLOBAL_VARS:                      true
      FLEET_GIT_REMOTE_TYPE:                      http
      FLEET_AGENT_CHECKIN:                        32
      REDEPLOY_AFTER_UPDATE:                      false
      USE_M2M_TOKEN:                              true
      CAPI_ENABLED:                               true
      FLEET_GIT_POLLING_INTERVAL:                 30
      DELETE_CRD_RESOURCES:                       true
    Mounts:
      /etc/dazl from logging (rw)
      /tmp from tmp (rw)
      /tmp/k8s-webhook-server/serving-certs from cert (ro)
      /tmp/mocks from mock-ca (rw)
      /tmp/ssl/certs/ from git-ca-cert-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-njzpg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  mock-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-deployment-manager-mock-ca
    Optional:  false
  logging:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-deployment-manager-logging
    Optional:  false
  cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  webhook-server-cert
    Optional:    false
  git-ca-cert-volume:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gitea-ca-cert
    Optional:    false
  kube-api-access-njzpg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                 From               Message
  ----     ------       ----                ----               -------
  Normal   Scheduled    52m                 default-scheduler  Successfully assigned orch-app/app-deployment-manager-65db96c7cf-77fqn to orch-tf
  Warning  FailedMount  52m (x3 over 52m)   kubelet            MountVolume.SetUp failed for volume "cert" : secret "webhook-server-cert" not found
  Normal   Pulled       52m                 kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created      52m                 kubelet            Created container: istio-init
  Normal   Started      52m                 kubelet            Started container istio-init
  Normal   Pulled       52m                 kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created      52m                 kubelet            Created container: istio-proxy
  Normal   Started      52m                 kubelet            Started container istio-proxy
  Warning  Unhealthy    52m (x10 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.230:15021/healthz/ready": dial tcp 10.42.65.230:15021: connect: connection refused
  Warning  Unhealthy    52m (x4 over 52m)   kubelet            Startup probe failed: HTTP probe failed with statuscode: 503
  Normal   Pulling      52m                 kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-controller:2.6.0"
  Normal   Pulled       52m                 kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/app/adm-controller:2.6.0" in 11.766s (11.766s including waiting). Image size: 19491179 bytes.
  Normal   Created      52m                 kubelet            Created container: controller
  Normal   Started      52m                 kubelet            Started container controller


Name:             app-orch-catalog-66bc49cb8-rvngj
Namespace:        orch-app
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:41:46 +0000
Labels:           app.kubernetes.io/instance=app-orch-catalog
                  app.kubernetes.io/name=app-orch-catalog
                  pod-template-hash=66bc49cb8
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=app-orch-catalog
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: f03a87c35be9882322d8db3f4f81b6c399d966437ea56c94df1ae059d1363e8a
                  cni.projectcalico.org/podIP: 10.42.65.142/32
                  cni.projectcalico.org/podIPs: 10.42.65.142/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: app-orch-catalog-rest-proxy
                  kubectl.kubernetes.io/default-logs-container: app-orch-catalog-rest-proxy
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.142
IPs:
  IP:           10.42.65.142
Controlled By:  ReplicaSet/app-orch-catalog-66bc49cb8
Init Containers:
  istio-init:
    Container ID:  containerd://0159dfea5bedc76c2027963a522d9d7231a3e680a4b912e1a555284d80fd9c50
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:41:48 +0000
      Finished:     Tue, 03 Feb 2026 17:41:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ltkpp (ro)
  istio-proxy:
    Container ID:  containerd://e6f86194b45ffeaaa4c621a7a2a7632c7d876af62b186866063e43093f3d1bc2
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:41:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      app-orch-catalog-66bc49cb8-rvngj (v1:metadata.name)
      POD_NAMESPACE:                 orch-app (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8081,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     app-orch-catalog-rest-proxy,app-orch-catalog-server,openpolicyagent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      app-orch-catalog
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-app/deployments/app-orch-catalog
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/app-orch-catalog-rest-proxy/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/app-orch-catalog-rest-proxy/readyz":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ltkpp (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  app-orch-catalog-rest-proxy:
    Container ID:    containerd://c095e042dcd8a7f89eb9dd0ec81b152ce61187cb5389e47546358351af608354
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/app-orch-catalog:0.16.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/app-orch-catalog@sha256:f18df8da4521934116778dcacca0eb0adc6179a2e9c58e83f8140313e7952180
    Port:            8081/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/local/bin/rest-proxy
    Args:
      --openidc-external=https://keycloak.cluster.onprem/realms/master
    State:          Running
      Started:      Tue, 03 Feb 2026 17:41:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:      10m
      memory:   64Mi
    Liveness:   http-get http://:15020/app-health/app-orch-catalog-rest-proxy/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/app-orch-catalog-rest-proxy/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      LOGGING_CONFIG:  /opt/rest-proxy/logging.yaml
      GIN_MODE:        release
    Mounts:
      /opt/rest-proxy from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ltkpp (ro)
  app-orch-catalog-server:
    Container ID:    containerd://b90494ba4e4c3cba33654cbf63bf71c9937bed0bbacdc0812c3c7f6498145867
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/app-orch-catalog:0.16.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/app-orch-catalog@sha256:f18df8da4521934116778dcacca0eb0adc6179a2e9c58e83f8140313e7952180
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/local/bin/application-catalog
    Args:
      -databaseHostname=$(PGHOST)
      -databasePort=$(PGPORT)
      -databaseName=$(PGDATABASE)
      -databaseSslMode=false
      -databaseDisableMigration=false
      -useSecretsService=$(USESECRET)
      -defaultProjectUUID=$(MT_UPGRADE_PROJECT_ID)
      -vaultServerAddress=$(VAULT_SERVER_ADDRESS)
    State:          Running
      Started:      Tue, 03 Feb 2026 17:41:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:     10m
      memory:  64Mi
    Environment Variables from:
      app-orch-catalog-local-postgresql  Secret  Optional: false
    Environment:
      DATABASE_USER:                  <set to the key 'PGUSER' in secret 'app-orch-catalog-local-postgresql'>      Optional: false
      DATABASE_PWD:                   <set to the key 'PGPASSWORD' in secret 'app-orch-catalog-local-postgresql'>  Optional: false
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_SERVER_URL_EXTERNAL:       https://keycloak.cluster.onprem/realms/master
      VAULT_SERVER_ADDRESS:           http://vault.orch-platform.svc.cluster.local:8200
      TLS_CERT_NAME:                  tls-orch
      SERVICE_ACCOUNT:                orch-svc
      OIDC_TLS_INSECURE_SKIP_VERIFY:  false
      USESECRET:                      true
      LOGGING_CONFIG:                 /opt/application-catalog/logging.yaml
    Mounts:
      /opt/application-catalog from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ltkpp (ro)
  openpolicyagent:
    Container ID:    containerd://531a22370b7a0698242b2234d40d1be2054db9fadf9c2f089161c3ecc8767bda
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            8181/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      /etc/app-orch-catalog/rego
      --log-level
      info
      --addr
      :8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:41:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        10m
      memory:     64Mi
    Liveness:     http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/app-orch-catalog/rego from openpolicyagent (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ltkpp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-orch-catalog
    Optional:  false
  tmpfs-1:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  openpolicyagent:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-orch-catalog-opa-rego
    Optional:  false
  kube-api-access-ltkpp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             app-orch-tenant-controller-d7b66768c-r8x48
Namespace:        orch-app
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:34 +0000
Labels:           app=app-orch-tenant-controller
                  app.kubernetes.io/instance=app-orch-tenant-controller
                  app.kubernetes.io/name=app-orch-tenant-controller
                  pod-template-hash=d7b66768c
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=app-orch-tenant-controller
                  service.istio.io/canonical-revision=0.5.0
                  version=0.5.0
Annotations:      cni.projectcalico.org/containerID: ef01ef578b2c8867d455cb034aec34a0709c1ae22bab8a811dc2ab9144469670
                  cni.projectcalico.org/podIP: 10.42.65.178/32
                  cni.projectcalico.org/podIPs: 10.42.65.178/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: config-provisioner
                  kubectl.kubernetes.io/default-logs-container: config-provisioner
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.178
IPs:
  IP:           10.42.65.178
Controlled By:  ReplicaSet/app-orch-tenant-controller-d7b66768c
Init Containers:
  istio-init:
    Container ID:  containerd://fee54cc7f318d9755f93e093c871a385ff38f3074a4396c8c1b8ab782efe61ce
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:39:37 +0000
      Finished:     Tue, 03 Feb 2026 17:39:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z4ttm (ro)
  istio-proxy:
    Container ID:  containerd://e7adf77102535c89eb170aeee4a97333fe3fa7ad786391e0b974b991bf087bdb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      app-orch-tenant-controller-d7b66768c-r8x48 (v1:metadata.name)
      POD_NAMESPACE:                 orch-app (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"grpc-health","containerPort":8081,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     config-provisioner
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      app-orch-tenant-controller
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-app/deployments/app-orch-tenant-controller
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/config-provisioner/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/config-provisioner/readyz":{"httpGet":{"path":"/readyz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z4ttm (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  config-provisioner:
    Container ID:    containerd://bfc525761f3573619b4c910941f4ef67ccd08a374e20785c0af4f80ff67e2a96
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/app-orch-tenant-controller:0.5.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/app-orch-tenant-controller@sha256:384ada0481cdfd8ad1e8f294a746cc14d645c6fe84334f9b8299526b707a10aa
    Port:            8081/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:53:45 +0000
    Last State:      Terminated
      Reason:        Error
      Exit Code:     1
      Started:       Tue, 03 Feb 2026 17:39:46 +0000
      Finished:      Tue, 03 Feb 2026 17:53:41 +0000
    Ready:           True
    Restart Count:   1
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:      10m
      memory:   64Mi
    Liveness:   http-get http://:15020/app-health/config-provisioner/livez delay=15s timeout=1s period=20s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/config-provisioner/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      USE_M2M_TOKEN:            true
      HARBOR_SERVER:            http://harbor-oci-core.orch-harbor.svc.cluster.local:80
      HARBOR_NAMESPACE:         orch-harbor
      HARBOR_ADMIN_CREDENTIAL:  harbor-admin-credential
      CATALOG_SERVER:           app-orch-catalog-grpc-server.orch-app.svc.cluster.local:8080
      RELEASE_SERVICE_BASE:     rs-proxy.orch-platform.svc.cluster.local:8081
      KEYCLOAK_SERVER:          https://keycloak.cluster.onprem
      KEYCLOAK_SERVICE_BASE:    http://platform-keycloak.orch-platform.svc.cluster.local:8080
      KEYCLOAK_NAMESPACE:       orch-platform
      KEYCLOAK_SECRET:          platform-keycloak
      ADM_SERVER:               app-deployment-api-grpc-server.orch-app.svc.cluster.local:8080
      VAULT_SERVER:             http://vault.orch-platform.svc.cluster.local:8200
      SERVICE_ACCOUNT:          orch-svc
      REGISTRY_HOST_EXTERNAL:   https://registry-oci.cluster.onprem
      REGISTRY_HOST:            http://harbor-oci-core.orch-harbor.svc.cluster.local:80
      RS_ROOT_URL:              oci://registry-rs.edgeorchestration.intel.com
      RS_PROXY_ROOT_URL:        oci://rs-proxy.orch-platform.svc.cluster.local:8443
      MANIFEST_PATH:            /edge-orch/en/file/cluster-extension-manifest
      MANIFEST_TAG:             v1.5.2
      INITIAL_SLEEP_INTERVAL:   15
      MAX_WAIT_TIME:            600
      NUMBER_WORKER_THREADS:    2
      http_proxy:               
      https_proxy:              
      no_proxy:                 
      USE_LOCAL_MANIFEST:       
    Mounts:
      /etc/dazl from logging (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z4ttm (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  logging:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-orch-tenant-controller
    Optional:  false
  kube-api-access-z4ttm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age                From     Message
  ----    ------   ----               ----     -------
  Normal  Pulled   52m                kubelet  Container image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-orch-tenant-controller:0.5.0" already present on machine
  Normal  Created  52m (x2 over 66m)  kubelet  Created container: config-provisioner
  Normal  Started  52m (x2 over 66m)  kubelet  Started container config-provisioner


Name:             app-resource-manager-7b6fb55dd5-79mmz
Namespace:        orch-app
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:06 +0000
Labels:           app=app-resource-manager
                  app.kubernetes.io/instance=app-resource-manager
                  app.kubernetes.io/name=app-resource-manager
                  pod-template-hash=7b6fb55dd5
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=app-resource-manager
                  service.istio.io/canonical-revision=2.6.0
                  version=2.6.0
Annotations:      cni.projectcalico.org/containerID: c140129e0405c87c5d7bb5a4780580c870fb43baf2d5677757785c283f9754e0
                  cni.projectcalico.org/podIP: 10.42.65.228/32
                  cni.projectcalico.org/podIPs: 10.42.65.228/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: openpolicyagent
                  kubectl.kubernetes.io/default-logs-container: openpolicyagent
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.228
IPs:
  IP:           10.42.65.228
Controlled By:  ReplicaSet/app-resource-manager-7b6fb55dd5
Init Containers:
  istio-init:
    Container ID:  containerd://930be3b178c9643f3ea7b001387e284a710f07c9512c2c55c1bb09df51a4d8a1
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:11 +0000
      Finished:     Tue, 03 Feb 2026 17:53:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5m5g9 (ro)
  istio-proxy:
    Container ID:  containerd://c73a6504b4410f95e37ccd88dbd544c9792a34a7006e267b6e0d12edde98b014
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      app-resource-manager-7b6fb55dd5-79mmz (v1:metadata.name)
      POD_NAMESPACE:                 orch-app (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"opa","containerPort":8181,"protocol":"TCP"}
                                         ,{"name":"http","containerPort":8081,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     openpolicyagent,app-resource-manager-rest-proxy,app-resource-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      app-resource-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-app/deployments/app-resource-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/app-resource-manager-rest-proxy/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/app-resource-manager-rest-proxy/readyz":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/app-resource-manager/livez":{"tcpSocket":{"port":8080},"timeoutSeconds":1},"/app-health/app-resource-manager/readyz":{"tcpSocket":{"port":8080},"timeoutSeconds":1},"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5m5g9 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  openpolicyagent:
    Container ID:    containerd://006feb83e835e1d1f97eeafedd3e37b1f76254a9c06643dcc8d710096c7a2efd
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            8181/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      /opt/app-resource-manager/rego/v2
      --log-level
      info
      --addr
      :8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /opt/app-resource-manager/rego/v2 from openpolicyagent-v2 (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5m5g9 (ro)
  app-resource-manager-rest-proxy:
    Container ID:    containerd://8206614d585b7f4e1843f8cd7d13d2d96d9c93d0169c122869892b417c0fd6c3
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-rest-proxy:2.6.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-rest-proxy@sha256:8f0ffe290978cce70d64454c1ceee551139632de1125c16764cc10aae8162728
    Port:            8081/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/local/bin/rest-proxy
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/app-resource-manager-rest-proxy/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/app-resource-manager-rest-proxy/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MSG_SIZE_LIMIT:   1
      LOGGING_CONFIG:   /opt/app-resource-manager-rest-proxy/logging.yaml
      GIN_MODE:         release
      USE_M2M_TOKEN:    true
      VAULT_SERVER:     http://vault.orch-platform.svc.cluster.local:8200
      KEYCLOAK_SERVER:  http://platform-keycloak.orch-platform.svc.cluster.local:8080
      SERVICE_ACCOUNT:  orch-svc
    Mounts:
      /opt/app-resource-manager-rest-proxy from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5m5g9 (ro)
  app-resource-manager:
    Container ID:    containerd://ad3f52574776891740047a3d6e6d89bfabd8ff1c37c13eb7a670459f7c80bf04
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-manager:2.6.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-manager@sha256:75378d78e8dac5d0aad753501e6b3a2b31af40f0aadd5e5c4a8ec14dc5e40d56
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/local/bin/app-resource-manager
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/app-resource-manager/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/app-resource-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MSG_SIZE_LIMIT:                 1
      LOGGING_CONFIG:                 /opt/app-resource-manager/logging.yaml
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OPA_PORT:                       8181
      OIDC_TLS_INSECURE_SKIP_VERIFY:  false
      SERVICE_PROXY_DOMAIN_NAME:      https://app-service-proxy.cluster.onprem
      RATE_LIMITER_QPS:               30
      RATE_LIMITER_BURST:             2000
      USE_M2M_TOKEN:                  true
      VAULT_SERVER:                   http://vault.orch-platform.svc.cluster.local:8200
      KEYCLOAK_SERVER:                http://platform-keycloak.orch-platform.svc.cluster.local:8080
      SERVICE_ACCOUNT:                orch-svc
      OPA_ENABLED:                    true
    Mounts:
      /opt/app-resource-manager from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5m5g9 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-resource-manager
    Optional:  false
  openpolicyagent-v2:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-resource-manager-opa-rego-v2
    Optional:  false
  kube-api-access-5m5g9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-app/app-resource-manager-7b6fb55dd5-79mmz to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-proxy
  Normal   Started    52m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x3 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.228:15021/healthz/ready": dial tcp 10.42.65.228:15021: connect: connection refused
  Normal   Pulled     52m                kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    52m                kubelet            Created container: openpolicyagent
  Normal   Started    52m                kubelet            Started container openpolicyagent
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-rest-proxy:2.6.0"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-rest-proxy:2.6.0" in 21.13s (21.13s including waiting). Image size: 40650820 bytes.
  Normal   Created    52m                kubelet            Created container: app-resource-manager-rest-proxy
  Normal   Started    52m                kubelet            Started container app-resource-manager-rest-proxy
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-manager:2.6.0"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-manager:2.6.0" in 12.911s (12.911s including waiting). Image size: 16745917 bytes.
  Normal   Created    52m                kubelet            Created container: app-resource-manager
  Normal   Started    52m                kubelet            Started container app-resource-manager


Name:             app-service-proxy-6d99d6b949-nb886
Namespace:        orch-app
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:52:55 +0000
Labels:           app=app-service-proxy
                  app.kubernetes.io/instance=app-service-proxy
                  app.kubernetes.io/name=app-service-proxy
                  pod-template-hash=6d99d6b949
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=app-service-proxy
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: c4cb596ec407522f7a506d31c5b43493863dd19531306c74538e644d59c0b826
                  cni.projectcalico.org/podIP: 10.42.65.220/32
                  cni.projectcalico.org/podIPs: 10.42.65.220/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: app-service-proxy
                  kubectl.kubernetes.io/default-logs-container: app-service-proxy
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.220
IPs:
  IP:           10.42.65.220
Controlled By:  ReplicaSet/app-service-proxy-6d99d6b949
Init Containers:
  istio-init:
    Container ID:  containerd://d7eabecf0fdcce8831650288aa530c6e4e92304ebe194185c4c47227180f424a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:52:58 +0000
      Finished:     Tue, 03 Feb 2026 17:52:59 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbtlp (ro)
  istio-proxy:
    Container ID:  containerd://b3fcc992b9b63e069c0d0d4ed33eff7e2200ed10bae1c2a206c0e796048de4a5
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:01 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      app-service-proxy-6d99d6b949-nb886 (v1:metadata.name)
      POD_NAMESPACE:                 orch-app (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     app-service-proxy,openpolicyagent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      app-service-proxy
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-app/deployments/app-service-proxy
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbtlp (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  app-service-proxy:
    Container ID:    containerd://dcea034b9b5993321b9fc392750d826eba113f8403172ffd9527f5452e44b2e5
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/app-service-proxy:1.6.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/app-service-proxy@sha256:284461da352f6cbd5f710b84a78162c22fb5e4c73099d810114a7b1f3d8b4714
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /app-service-proxy
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      MAX_BODY_SIZE_BYTES_LIMIT:      100
      API_POSITION_IN_URL:            5
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  false
      OPA_PORT:                       8181
      USE_M2M_TOKEN:                  true
      OPA_ENABLED:                    true
      ASP_LOG_LEVEL:                  debug
      ADM_ADDRESS:                    app-deployment-api-grpc-server:8080
      CCG_ADDRESS:                    cluster-connect-gateway.orch-cluster.svc:8080
      RATE_LIMITER_QPS:               30
      RATE_LIMITER_BURST:             2000
    Mounts:
      /tmp from asptmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbtlp (ro)
  openpolicyagent:
    Container ID:    containerd://82ad3edc01b85d597544cbc8935f4a6f5667eb332111353c0c7771166b9f7dba
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            8181/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      /opt/app-service-proxy/rego
      --log-level
      info
      --addr
      :8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /opt/app-service-proxy/rego from openpolicyagent (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbtlp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  asptmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  openpolicyagent:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-service-proxy-opa-rego
    Optional:  false
  kube-api-access-sbtlp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-app/app-service-proxy-6d99d6b949-nb886 to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-proxy
  Normal   Started    53m                kubelet            Started container istio-proxy
  Warning  Unhealthy  53m (x4 over 53m)  kubelet            Startup probe failed: Get "http://10.42.65.220:15021/healthz/ready": dial tcp 10.42.65.220:15021: connect: connection refused
  Normal   Pulling    53m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-service-proxy:1.6.0"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-service-proxy:1.6.0" in 7.496s (7.496s including waiting). Image size: 14965299 bytes.
  Normal   Created    52m                kubelet            Created container: app-service-proxy
  Normal   Started    52m                kubelet            Started container app-service-proxy
  Normal   Pulled     52m                kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    52m                kubelet            Created container: openpolicyagent
  Normal   Started    52m                kubelet            Started container openpolicyagent


Name:             vnc-proxy-app-resource-manager-74f95dbdf9-hlbwh
Namespace:        orch-app
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:05 +0000
Labels:           app=vnc-proxy-app-resource-manager
                  app.kubernetes.io/instance=app-resource-manager
                  app.kubernetes.io/name=vnc-proxy-app-resource-manager
                  pod-template-hash=74f95dbdf9
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=vnc-proxy-app-resource-manager
                  service.istio.io/canonical-revision=2.6.0
                  version=2.6.0
Annotations:      cni.projectcalico.org/containerID: 0b6473957f99d6dffa7964cf6b3c51ccba4a6798c74ca4c096468b1a1359a4ef
                  cni.projectcalico.org/podIP: 10.42.65.226/32
                  cni.projectcalico.org/podIPs: 10.42.65.226/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: openpolicyagent
                  kubectl.kubernetes.io/default-logs-container: openpolicyagent
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.226
IPs:
  IP:           10.42.65.226
Controlled By:  ReplicaSet/vnc-proxy-app-resource-manager-74f95dbdf9
Init Containers:
  istio-init:
    Container ID:  containerd://e394e07ae2074d6b160103b659879b686ee6fc96c57b64ad65729a0c0d8cd53f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:10 +0000
      Finished:     Tue, 03 Feb 2026 17:53:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sv9zg (ro)
  istio-proxy:
    Container ID:  containerd://d261fcc0202c27e464284ec48d291e1e9c59ae0e9694a095f656799b59e2173c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      vnc-proxy-app-resource-manager-74f95dbdf9-hlbwh (v1:metadata.name)
      POD_NAMESPACE:                 orch-app (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     openpolicyagent,vncproxy
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      vnc-proxy-app-resource-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-app/deployments/vnc-proxy-app-resource-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/vncproxy/livez":{"tcpSocket":{"port":5900},"timeoutSeconds":1},"/app-health/vncproxy/readyz":{"tcpSocket":{"port":5900},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sv9zg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  openpolicyagent:
    Container ID:    containerd://2662a03911f0705d02fbf5cbd776a8534369b550aa374f9e490941afeff90f81
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            8181/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      /opt/vnc-proxy/rego/v2
      --log-level
      info
      --addr
      :8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /opt/vnc-proxy/rego/v2 from openpolicyagent-v2 (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sv9zg (ro)
  vncproxy:
    Container ID:    containerd://90d17e02e3cddbf55f08396c66d7614e33a99909879a6276e85612a773afc805
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-vnc-proxy:2.6.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-vnc-proxy@sha256:f49f6c0dfa98d4c2276cca5092867fa9ddb990c6b21e3fae159c58c7d4222451
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /usr/local/bin/vnc-proxy
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:33 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/vncproxy/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/vncproxy/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MSG_SIZE_LIMIT:                 1
      RATE_LIMITER_QPS:               30
      RATE_LIMITER_BURST:             2000
      LOGGING_CONFIG:                 /opt/vnc-proxy/logging.yaml
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OPA_PORT:                       8181
      OIDC_TLS_INSECURE_SKIP_VERIFY:  false
      USE_M2M_TOKEN:                  true
      VAULT_SERVER:                   http://vault.orch-platform.svc.cluster.local:8200
      KEYCLOAK_SERVER:                http://platform-keycloak.orch-platform.svc.cluster.local:8080
      SERVICE_ACCOUNT:                orch-svc
      OPA_ENABLED:                    true
    Mounts:
      /opt/vnc-proxy from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sv9zg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-resource-manager
    Optional:  false
  openpolicyagent-v2:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      app-resource-manager-opa-rego-v2
    Optional:  false
  kube-api-access-sv9zg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-app/vnc-proxy-app-resource-manager-74f95dbdf9-hlbwh to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-proxy
  Normal   Started    52m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x3 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.226:15021/healthz/ready": dial tcp 10.42.65.226:15021: connect: connection refused
  Normal   Pulled     52m                kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    52m                kubelet            Created container: openpolicyagent
  Normal   Started    52m                kubelet            Started container openpolicyagent
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-vnc-proxy:2.6.0"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/app/app-resource-vnc-proxy:2.6.0" in 9.623s (9.623s including waiting). Image size: 17032092 bytes.
  Normal   Created    52m                kubelet            Created container: vncproxy
  Normal   Started    52m                kubelet            Started container vncproxy


Name:             ingress-nginx-controller-kubernetes-ingress-7d87dc5676-8zml7
Namespace:        orch-boots
Priority:         0
Service Account:  ingress-nginx-controller-kubernetes-ingress
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:54 +0000
Labels:           app.kubernetes.io/instance=ingress-nginx-controller
                  app.kubernetes.io/name=kubernetes-ingress
                  pod-template-hash=7d87dc5676
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=kubernetes-ingress
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 8c2d9e5e56cbed550b8abc4f65ed599a74d7fba9f83d310306b673aba344dd4e
                  cni.projectcalico.org/podIP: 10.42.65.151/32
                  cni.projectcalico.org/podIPs: 10.42.65.151/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: kubernetes-ingress-controller
                  kubectl.kubernetes.io/default-logs-container: kubernetes-ingress-controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.151
IPs:
  IP:           10.42.65.151
Controlled By:  ReplicaSet/ingress-nginx-controller-kubernetes-ingress-7d87dc5676
Init Containers:
  istio-init:
    Container ID:  containerd://12d4696e0bb07fd98007d66ae2311631aab38cdd66dcd80ad6acd708556d20bb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:39:56 +0000
      Finished:     Tue, 03 Feb 2026 17:39:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-65vl2 (ro)
  istio-proxy:
    Container ID:  containerd://a2d4860d89929c58dce674b0c73c7a36eae1e9088a71021626eaf74345762a39
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      ingress-nginx-controller-kubernetes-ingress-7d87dc5676-8zml7 (v1:metadata.name)
      POD_NAMESPACE:                 orch-boots (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"https","containerPort":8443,"protocol":"TCP"}
                                         ,{"name":"stat","containerPort":1024,"protocol":"TCP"}
                                         ,{"name":"quic","containerPort":8443,"protocol":"UDP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     kubernetes-ingress-controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      ingress-nginx-controller-kubernetes-ingress
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-boots/deployments/ingress-nginx-controller-kubernetes-ingress
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/kubernetes-ingress-controller/livez":{"httpGet":{"path":"/healthz","port":1042,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/kubernetes-ingress-controller/readyz":{"httpGet":{"path":"/healthz","port":1042,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/kubernetes-ingress-controller/startupz":{"httpGet":{"path":"/healthz","port":1042,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-65vl2 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  kubernetes-ingress-controller:
    Container ID:    containerd://da60c78fb89614afb48e6b0de0fce61757182d22ee01ba3d945c06296c11c83b
    Image:           haproxytech/kubernetes-ingress:3.0.1
    Image ID:        docker.io/haproxytech/kubernetes-ingress@sha256:d98dba660b179140a60068a8a4feb8f471d45623c946eb87ba24367c7602c654
    Ports:           8080/TCP, 8443/TCP, 1024/TCP, 8443/UDP
    Host Ports:      0/TCP, 0/TCP, 0/TCP, 0/UDP
    SeccompProfile:  RuntimeDefault
    Args:
      --default-ssl-certificate=orch-boots/ingress-nginx-controller-kubernetes-ingress-default-cert
      --configmap=orch-boots/ingress-nginx-controller-kubernetes-ingress
      --http-bind-port=8080
      --https-bind-port=8443
      --quic-bind-port=8443
      --quic-announce-port=443
      --ingress.class=haproxy
      --publish-service=orch-boots/ingress-nginx-controller-kubernetes-ingress
      --log=info
      --default-ssl-certificate=orch-boots/tls-boots
    State:          Running
      Started:      Tue, 03 Feb 2026 17:40:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/kubernetes-ingress-controller/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/kubernetes-ingress-controller/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Startup:    http-get http://:15020/app-health/kubernetes-ingress-controller/startupz delay=0s timeout=1s period=1s #success=1 #failure=20
    Environment:
      POD_NAME:       ingress-nginx-controller-kubernetes-ingress-7d87dc5676-8zml7 (v1:metadata.name)
      POD_NAMESPACE:  orch-boots (v1:metadata.namespace)
      POD_IP:          (v1:status.podIP)
    Mounts:
      /run from tmp (rw,path="run")
      /tmp from tmp (rw,path="tmp")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-65vl2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  64Mi
  kube-api-access-65vl2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             ingress-nginx-controller-kubernetes-ingress-7d87dc5676-fkr98
Namespace:        orch-boots
Priority:         0
Service Account:  ingress-nginx-controller-kubernetes-ingress
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:54 +0000
Labels:           app.kubernetes.io/instance=ingress-nginx-controller
                  app.kubernetes.io/name=kubernetes-ingress
                  pod-template-hash=7d87dc5676
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=kubernetes-ingress
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 1479ada8f02ab33ff2369284158915663ba49c2b47dbc4246a68f89099363ddd
                  cni.projectcalico.org/podIP: 10.42.65.188/32
                  cni.projectcalico.org/podIPs: 10.42.65.188/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: kubernetes-ingress-controller
                  kubectl.kubernetes.io/default-logs-container: kubernetes-ingress-controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.188
IPs:
  IP:           10.42.65.188
Controlled By:  ReplicaSet/ingress-nginx-controller-kubernetes-ingress-7d87dc5676
Init Containers:
  istio-init:
    Container ID:  containerd://0697f8d9475330ab72ef4d1d15494a73a76a7ac809bf24d356cdd9b6d13c132c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:39:56 +0000
      Finished:     Tue, 03 Feb 2026 17:39:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2lvl5 (ro)
  istio-proxy:
    Container ID:  containerd://bae331be70fffa113a01c840d03d323785eff81583549b9e2a4eee384fb75c34
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      ingress-nginx-controller-kubernetes-ingress-7d87dc5676-fkr98 (v1:metadata.name)
      POD_NAMESPACE:                 orch-boots (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"https","containerPort":8443,"protocol":"TCP"}
                                         ,{"name":"stat","containerPort":1024,"protocol":"TCP"}
                                         ,{"name":"quic","containerPort":8443,"protocol":"UDP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     kubernetes-ingress-controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      ingress-nginx-controller-kubernetes-ingress
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-boots/deployments/ingress-nginx-controller-kubernetes-ingress
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/kubernetes-ingress-controller/livez":{"httpGet":{"path":"/healthz","port":1042,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/kubernetes-ingress-controller/readyz":{"httpGet":{"path":"/healthz","port":1042,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/kubernetes-ingress-controller/startupz":{"httpGet":{"path":"/healthz","port":1042,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2lvl5 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  kubernetes-ingress-controller:
    Container ID:    containerd://37462c3bc89158c9a6867b7362d65c64a97e2db3719e4d029373647ef4ad6abf
    Image:           haproxytech/kubernetes-ingress:3.0.1
    Image ID:        docker.io/haproxytech/kubernetes-ingress@sha256:d98dba660b179140a60068a8a4feb8f471d45623c946eb87ba24367c7602c654
    Ports:           8080/TCP, 8443/TCP, 1024/TCP, 8443/UDP
    Host Ports:      0/TCP, 0/TCP, 0/TCP, 0/UDP
    SeccompProfile:  RuntimeDefault
    Args:
      --default-ssl-certificate=orch-boots/ingress-nginx-controller-kubernetes-ingress-default-cert
      --configmap=orch-boots/ingress-nginx-controller-kubernetes-ingress
      --http-bind-port=8080
      --https-bind-port=8443
      --quic-bind-port=8443
      --quic-announce-port=443
      --ingress.class=haproxy
      --publish-service=orch-boots/ingress-nginx-controller-kubernetes-ingress
      --log=info
      --default-ssl-certificate=orch-boots/tls-boots
    State:          Running
      Started:      Tue, 03 Feb 2026 17:40:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/kubernetes-ingress-controller/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/kubernetes-ingress-controller/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Startup:    http-get http://:15020/app-health/kubernetes-ingress-controller/startupz delay=0s timeout=1s period=1s #success=1 #failure=20
    Environment:
      POD_NAME:       ingress-nginx-controller-kubernetes-ingress-7d87dc5676-fkr98 (v1:metadata.name)
      POD_NAMESPACE:  orch-boots (v1:metadata.namespace)
      POD_IP:          (v1:status.podIP)
    Mounts:
      /run from tmp (rw,path="run")
      /tmp from tmp (rw,path="tmp")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2lvl5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  64Mi
  kube-api-access-2lvl5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             secret-wait-tls-boots-67a6b1d971-xmfrp
Namespace:        orch-boots
Priority:         0
Service Account:  secret-wait-tls-boots
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:46:37 +0000
Labels:           batch.kubernetes.io/controller-uid=66c5224f-aaa0-4063-b430-748801be6cf3
                  batch.kubernetes.io/job-name=secret-wait-tls-boots-67a6b1d971
                  controller-uid=66c5224f-aaa0-4063-b430-748801be6cf3
                  job-name=secret-wait-tls-boots-67a6b1d971
Annotations:      cni.projectcalico.org/containerID: f3c32d18e9afff71f33e118b67ec2c8f1d068d6f2b2fb61836945b33e9b326b7
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.141
IPs:
  IP:           10.42.65.141
Controlled By:  Job/secret-wait-tls-boots-67a6b1d971
Containers:
  secret-wait:
    Container ID:    containerd://e69fed0634a2c3585c2e91bb383706f7cc01f5057d221a94f3e4127b660ec0aa
    Image:           alpine/kubectl:1.34.1
    Image ID:        docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      timeout
      240h
      sh
      -c
      until kubectl get secret -n orch-boots tls-boots; do sleep 10; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:46:40 +0000
      Finished:     Tue, 03 Feb 2026 17:46:41 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rbv7l (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-rbv7l:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  59m   default-scheduler  Successfully assigned orch-boots/secret-wait-tls-boots-67a6b1d971-xmfrp to orch-tf
  Normal  Pulled     59m   kubelet            Container image "alpine/kubectl:1.34.1" already present on machine
  Normal  Created    59m   kubelet            Created container: secret-wait
  Normal  Started    59m   kubelet            Started container secret-wait


Name:             cluster-connect-gateway-controller-66d6658cc8-gh5c4
Namespace:        orch-cluster
Priority:         0
Service Account:  cluster-connect-gateway
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:06 +0000
Labels:           app.kubernetes.io/component=controller
                  app.kubernetes.io/instance=cluster-connect-gateway
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=cluster-connect-gateway
                  app.kubernetes.io/version=1.2.5
                  helm.sh/chart=cluster-connect-gateway-1.2.5
                  pod-template-hash=66d6658cc8
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=cluster-connect-gateway
                  service.istio.io/canonical-revision=1.2.5
Annotations:      cni.projectcalico.org/containerID: f4e1366c2932a593942c1bff4fe1686764c5922dcd1d29e258ad788c3a96773f
                  cni.projectcalico.org/podIP: 10.42.65.206/32
                  cni.projectcalico.org/podIPs: 10.42.65.206/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: connect-controller
                  kubectl.kubernetes.io/default-logs-container: connect-controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.206
IPs:
  IP:           10.42.65.206
Controlled By:  ReplicaSet/cluster-connect-gateway-controller-66d6658cc8
Init Containers:
  istio-init:
    Container ID:  containerd://9c8a1e5900fb2407c8eec462fd99adcf3bc09598c454f42c66e3922a32f993d2
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:08 +0000
      Finished:     Tue, 03 Feb 2026 17:48:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24jnt (ro)
  istio-proxy:
    Container ID:  containerd://c527cf6a3965d04e0bfc86d76c21117498dc5138cac16f4693a7595a9bfc46aa
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:09 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      cluster-connect-gateway-controller-66d6658cc8-gh5c4 (v1:metadata.name)
      POD_NAMESPACE:                 orch-cluster (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"metrics","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     connect-controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      cluster-connect-gateway-controller
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-cluster/deployments/cluster-connect-gateway-controller
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/connect-controller/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/connect-controller/readyz":{"httpGet":{"path":"/readyz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24jnt (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  connect-controller:
    Container ID:    containerd://4bfd54cb7dc90badc145a900ad8234683ccded0d7575bba71d71d8a595cebfb8
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-controller:1.2.5
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-controller@sha256:ec84f965fb28025f0e2a794ad5372d3f9395506a29dc16d2b566f0b7078c1905
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /connect-controller
    Args:
      --leader-elect
      --health-probe-bind-address=:8081
      --connection-probe-timeout=5m
      --metrics-bind-address=:8080
      --metrics-secure=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:22 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/connect-controller/livez delay=15s timeout=1s period=20s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/connect-controller/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PRIVATE_CA_ENABLED:           true
      PRIVATE_CA_SECRET_NAME:       tls-orch
      PRIVATE_CA_SECRET_NAMESPACE:  orch-gateway
      SECRET_NAMESPACE:             orch-cluster
      GATEWAY_EXTERNAL_URL:         wss://connect-gateway.cluster.onprem:443
      GATEWAY_INTERNAL_URL:         http://cluster-connect-gateway.orch-cluster.svc:8080
      AGENT_JWT_TOKEN_PATH:         /etc/intel_edge_node/tokens/connect-agent/access_token
      AGENT_AUTH_MODE:              jwt
      AGENT_IMAGE:                  <set to the key 'AGENT_IMAGE' of config map 'connect-agent-config'>      Optional: false
      AGENT_LOG_LEVEL:              <set to the key 'AGENT_LOG_LEVEL' of config map 'connect-agent-config'>  Optional: false
      AGENT_TLS_MODE:               <set to the key 'AGENT_TLS_MODE' of config map 'connect-agent-config'>   Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24jnt (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-24jnt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  58m                default-scheduler  Successfully assigned orch-cluster/cluster-connect-gateway-controller-66d6658cc8-gh5c4 to orch-tf
  Normal   Pulled     58m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m                kubelet            Created container: istio-init
  Normal   Started    58m                kubelet            Started container istio-init
  Normal   Pulled     58m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m                kubelet            Created container: istio-proxy
  Normal   Started    58m                kubelet            Started container istio-proxy
  Warning  Unhealthy  58m (x2 over 58m)  kubelet            Startup probe failed: Get "http://10.42.65.206:15021/healthz/ready": dial tcp 10.42.65.206:15021: connect: connection refused
  Normal   Pulling    58m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-controller:1.2.5"
  Normal   Pulled     57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-controller:1.2.5" in 7.79s (7.79s including waiting). Image size: 20101582 bytes.
  Normal   Created    57m                kubelet            Created container: connect-controller
  Normal   Started    57m                kubelet            Started container connect-controller


Name:             cluster-connect-gateway-gateway-5f7b9f899f-rl2pc
Namespace:        orch-cluster
Priority:         0
Service Account:  cluster-connect-gateway
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:06 +0000
Labels:           app=cluster-connect-gateway-gateway
                  app.kubernetes.io/component=gateway
                  app.kubernetes.io/instance=cluster-connect-gateway
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=cluster-connect-gateway
                  app.kubernetes.io/version=1.2.5
                  helm.sh/chart=cluster-connect-gateway-1.2.5
                  pod-template-hash=5f7b9f899f
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=cluster-connect-gateway
                  service.istio.io/canonical-revision=1.2.5
Annotations:      cni.projectcalico.org/containerID: 7bdbdde2ec2b56c9b5526f2657a28058618bc24c141aa17a0686c5a796fa6618
                  cni.projectcalico.org/podIP: 10.42.65.205/32
                  cni.projectcalico.org/podIPs: 10.42.65.205/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: connect-gateway
                  kubectl.kubernetes.io/default-logs-container: connect-gateway
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.205
IPs:
  IP:           10.42.65.205
Controlled By:  ReplicaSet/cluster-connect-gateway-gateway-5f7b9f899f
Init Containers:
  istio-init:
    Container ID:  containerd://59629bc90582bee2e1867abc1a776fcaff3151b840a50d2dde90549393efc17c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:08 +0000
      Finished:     Tue, 03 Feb 2026 17:48:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mj74w (ro)
  istio-proxy:
    Container ID:  containerd://f90e2a96040884dc975786de6d96a64b72a011ed123fd7a5747ea1c29d31bc52
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:09 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      cluster-connect-gateway-gateway-5f7b9f899f-rl2pc (v1:metadata.name)
      POD_NAMESPACE:                 orch-cluster (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     connect-gateway,openpolicyagent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      cluster-connect-gateway-gateway
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-cluster/deployments/cluster-connect-gateway-gateway
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mj74w (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  connect-gateway:
    Container ID:    containerd://99f5dcd90b688a274916ee7316436915bf300f9fcd839a1c8c79fd34af086a98
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-gateway:1.2.5
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-gateway@sha256:564dd8a5c18cb01bdb3d7ffc23691961b1078b3ff7c1562294b191f2df8c95f9
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /connect-gateway
    Args:
      --address=0.0.0.0
      --port=8080
      --enable-auth=true
      --oidc-issuer-url=http://platform-keycloak.orch-platform.svc:8080/realms/master
      --oidc-insecure-skip-verify=true
      --enable-metrics=true
      --log-level=info
      --external-host=connect-gateway.cluster.onprem
      --tunnel-auth-mode=jwt
      --connection-probe-interval=1m
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      CATTLE_PROMETHEUS_METRICS:      true
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc:8080/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
      SECRET_NAMESPACE:               orch-cluster
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mj74w (ro)
  openpolicyagent:
    Container ID:    containerd://9756a92303c3627e5e889f588a4a37d325a388f4826b2ed6e7dec95ecd34e6f6
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            8181/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      --bundle
      /rego/v2
      --log-level
      debug
      --disable-telemetry
      --addr
      0.0.0.0:8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:22 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /rego/v2 from openpolicyagent-v2 (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mj74w (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  openpolicyagent-v2:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cluster-connect-gateway-opa-rego-v2
    Optional:  false
  kube-api-access-mj74w:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  58m   default-scheduler  Successfully assigned orch-cluster/cluster-connect-gateway-gateway-5f7b9f899f-rl2pc to orch-tf
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-init
  Normal   Started    58m   kubelet            Started container istio-init
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-proxy
  Normal   Started    58m   kubelet            Started container istio-proxy
  Warning  Unhealthy  58m   kubelet            Startup probe failed: Get "http://10.42.65.205:15021/healthz/ready": dial tcp 10.42.65.205:15021: connect: connection refused
  Normal   Pulling    58m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-gateway:1.2.5"
  Normal   Pulled     57m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/connect-gateway:1.2.5" in 5.679s (5.679s including waiting). Image size: 13159544 bytes.
  Normal   Created    57m   kubelet            Created container: connect-gateway
  Normal   Started    57m   kubelet            Started container connect-gateway
  Normal   Pulled     57m   kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    57m   kubelet            Created container: openpolicyagent
  Normal   Started    57m   kubelet            Started container openpolicyagent


Name:             cluster-manager-6c49967cdf-bc8db
Namespace:        orch-cluster
Priority:         0
Service Account:  cluster-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:27 +0000
Labels:           app=cluster-manager-cm
                  pod-template-hash=6c49967cdf
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=cluster-manager-cm
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/configmap: 0a5b4f8d54d90ed546d87a06035d2b1898325f401194c8473144e96d2cd54f69
                  cni.projectcalico.org/containerID: 7e427d7a9efa73a00ae91f84645086b0be6f665f872186d4f58f1fa75b5d2af7
                  cni.projectcalico.org/podIP: 10.42.65.212/32
                  cni.projectcalico.org/podIPs: 10.42.65.212/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: cluster-manager
                  kubectl.kubernetes.io/default-logs-container: cluster-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.212
IPs:
  IP:           10.42.65.212
Controlled By:  ReplicaSet/cluster-manager-6c49967cdf
Init Containers:
  istio-init:
    Container ID:  containerd://6ab4a480d555de8d9f4ec70581849dda97fb4e5841a5ef873ab5c84a81b06a14
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:31 +0000
      Finished:     Tue, 03 Feb 2026 17:48:31 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tflxh (ro)
  istio-proxy:
    Container ID:  containerd://d5bf51bbf69f443868d4dbea2339229cf60662905bef9440e330dae750571712
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:33 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      cluster-manager-6c49967cdf-bc8db (v1:metadata.name)
      POD_NAMESPACE:                 orch-cluster (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"rest","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     cluster-manager,openpolicyagent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      cluster-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-cluster/deployments/cluster-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/cluster-manager/readyz":{"httpGet":{"path":"/v2/healthz","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":8181,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tflxh (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  wait-for-template-controller:
    Container ID:    containerd://060558155407bcd9acb5ce90ac74c6ecde847a5d34e667d20a7b58ffdaf36f0d
    Image:           alpine/curl
    Image ID:        docker.io/alpine/curl@sha256:afdd46d18b0f2d5a5bbe399e39dfa50a20ab285f5950dffff9d3cb5e1d0a74f3
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      sh
      -c
      until curl --fail --connect-timeout 3 http://cluster-manager-template-controller:8081/readyz; do echo 'Waiting for template-controller...'; sleep 5; done
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:42 +0000
      Finished:     Tue, 03 Feb 2026 17:49:02 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tflxh (ro)
Containers:
  cluster-manager:
    Container ID:    containerd://84598299cb12f0dbd1c4db7266b569d19e5af5f3eecbf516f09a2c7c7857f242
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/cluster/cluster-manager:2.2.6
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/cluster/cluster-manager@sha256:0317d5ea5b67ecd8074ae839f843762f43391ea4420fd70289b50b30e386dc00
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -loglevel=0
      -logformat=human
      -clusterdomain=cluster.onprem
      -username=admin
      -kubeconfig-ttl-hours=3
      -inventory-endpoint=inventory.orch-infra.svc.cluster.local:50051
      -default-template=baseline-k3s-v0.0.10
      -disable-auth=false
      -disable-inventory=false
      -disable-metrics=false
      -disable-multi-tenancy=false
      -kubeconfig-ttl-hours=3
    State:          Running
      Started:      Tue, 03 Feb 2026 17:49:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/cluster-manager/readyz delay=15s timeout=1s period=20s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  false
      RATE_LIMITER_QPS:               50
      RATE_LIMITER_BURST:             200
      OPA_ENABLED:                    true
      OPA_PORT:                       8181
    Mounts:
      /default-templates from default-templates (rw)
      /pod-security-admission from psa-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tflxh (ro)
  openpolicyagent:
    Container ID:    containerd://f76e24667c5d203fa8615c653758e915db501bbbff738b25b6cc0dc298a38cb3
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            8181/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      --addr=0.0.0.0:8181
      --bundle
      /opt/cluster-manager/rego/v1
      --disable-telemetry
    State:          Running
      Started:      Tue, 03 Feb 2026 17:49:07 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /opt/cluster-manager/rego/v1 from openpolicyagent-v1 (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tflxh (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  default-templates:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cluster-manager-default-templates
    Optional:  false
  openpolicyagent-v1:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cluster-manager-opa-rego-v1
    Optional:  false
  psa-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  pod-security-admission-config
    Optional:    false
  kube-api-access-tflxh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57m                default-scheduler  Successfully assigned orch-cluster/cluster-manager-6c49967cdf-bc8db to orch-tf
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-init
  Normal   Started    57m                kubelet            Started container istio-init
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-proxy
  Normal   Started    57m                kubelet            Started container istio-proxy
  Warning  Unhealthy  57m (x4 over 57m)  kubelet            Startup probe failed: Get "http://10.42.65.212:15021/healthz/ready": dial tcp 10.42.65.212:15021: connect: connection refused
  Normal   Pulling    57m                kubelet            Pulling image "alpine/curl"
  Normal   Pulled     57m                kubelet            Successfully pulled image "alpine/curl" in 3.18s (3.18s including waiting). Image size: 6407906 bytes.
  Normal   Created    57m                kubelet            Created container: wait-for-template-controller
  Normal   Started    57m                kubelet            Started container wait-for-template-controller
  Normal   Pulling    57m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/cluster-manager:2.2.6"
  Normal   Pulled     57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/cluster-manager:2.2.6" in 3.785s (3.785s including waiting). Image size: 24951991 bytes.
  Normal   Created    57m                kubelet            Created container: cluster-manager
  Normal   Started    57m                kubelet            Started container cluster-manager
  Normal   Pulled     57m                kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    57m                kubelet            Created container: openpolicyagent
  Normal   Started    57m                kubelet            Started container openpolicyagent


Name:             cluster-manager-credentials-script-9d6vm
Namespace:        orch-cluster
Priority:         0
Service Account:  cluster-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:29 +0000
Labels:           app.kubernetes.io/component=credentials-m2m
                  app.kubernetes.io/instance=cluster-manager
                  app.kubernetes.io/name=cluster-manager
                  batch.kubernetes.io/controller-uid=acb80a41-4f68-41c3-b45c-7c7cbcc79d34
                  batch.kubernetes.io/job-name=cluster-manager-credentials-script
                  controller-uid=acb80a41-4f68-41c3-b45c-7c7cbcc79d34
                  job-name=cluster-manager-credentials-script
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=cluster-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: c7750b0d61ca4a2895928c061563cb62c05497a20813ac5593db64abf73e1166
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: credentials-m2m
                  kubectl.kubernetes.io/default-logs-container: credentials-m2m
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.213
IPs:
  IP:           10.42.65.213
Controlled By:  Job/cluster-manager-credentials-script
Init Containers:
  istio-init:
    Container ID:  containerd://2c7287b29b674f5b5a6e1cc969af095287eb9b475fc8f30343194fedae02acbd
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:33 +0000
      Finished:     Tue, 03 Feb 2026 17:48:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqdmv (ro)
  istio-proxy:
    Container ID:  containerd://7ed5509eae059876516ac3fe515fdafcd693ad2e4fea2034ededfa23a3f423e9
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:36 +0000
      Finished:     Tue, 03 Feb 2026 17:48:43 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      cluster-manager-credentials-script-9d6vm (v1:metadata.name)
      POD_NAMESPACE:                 orch-cluster (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     credentials-m2m
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      cluster-manager-credentials-script
      ISTIO_META_OWNER:              kubernetes://apis/batch/v1/namespaces/orch-cluster/jobs/cluster-manager-credentials-script
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqdmv (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  credentials-m2m:
    Container ID:    containerd://23fdbe2949a8a1c52f2a785b45ebec5e0fec35ee441aa6e299a4a63f65e82364
    Image:           badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Image ID:        docker.io/badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
    Args:
      /scripts/credentials-m2m.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:41 +0000
      Finished:     Tue, 03 Feb 2026 17:48:43 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  2Gi
    Requests:
      cpu:     10m
      memory:  16Mi
    Environment:
      NAMESPACE:  orch-cluster (v1:metadata.namespace)
    Mounts:
      /scripts from credentials-script (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqdmv (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  credentials-script:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cluster-manager-credentials-script
    Optional:  false
  kube-api-access-nqdmv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57m                default-scheduler  Successfully assigned orch-cluster/cluster-manager-credentials-script-9d6vm to orch-tf
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-init
  Normal   Started    57m                kubelet            Started container istio-init
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-proxy
  Normal   Started    57m                kubelet            Started container istio-proxy
  Warning  Unhealthy  57m (x2 over 57m)  kubelet            Startup probe failed: Get "http://10.42.65.213:15021/healthz/ready": dial tcp 10.42.65.213:15021: connect: connection refused
  Normal   Pulled     57m                kubelet            Container image "badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b" already present on machine
  Normal   Created    57m                kubelet            Created container: credentials-m2m
  Normal   Started    57m                kubelet            Started container credentials-m2m


Name:             cluster-manager-template-controller-6c9596f65b-9twkl
Namespace:        orch-cluster
Priority:         0
Service Account:  cluster-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:24 +0000
Labels:           app=cluster-manager-controller
                  pod-template-hash=6c9596f65b
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=cluster-manager-controller
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: b2c6ffbbf1fe80ad36a6b8caa446164c46b2822e67b0b6a7ab61f3f72b87b606
                  cni.projectcalico.org/podIP: 10.42.65.214/32
                  cni.projectcalico.org/podIPs: 10.42.65.214/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: template-controller
                  kubectl.kubernetes.io/default-logs-container: template-controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.214
IPs:
  IP:           10.42.65.214
Controlled By:  ReplicaSet/cluster-manager-template-controller-6c9596f65b
Init Containers:
  istio-init:
    Container ID:  containerd://139d38939e150576ced5bbf767863e27a60a8c637bcfc5dff5cdb68f24b0686b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:36 +0000
      Finished:     Tue, 03 Feb 2026 17:48:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x7h5p (ro)
  istio-proxy:
    Container ID:  containerd://1038c639a33c675560a51c71063a4a7e35e484433ac6ddd5a39d5bd86008ca23
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      cluster-manager-template-controller-6c9596f65b-9twkl (v1:metadata.name)
      POD_NAMESPACE:                 orch-cluster (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"webhook-server","containerPort":9443,"protocol":"TCP"}
                                         ,{"name":"http-health","containerPort":8081,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     template-controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      cluster-manager-template-controller
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-cluster/deployments/cluster-manager-template-controller
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/template-controller/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/template-controller/readyz":{"httpGet":{"path":"/readyz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x7h5p (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  template-controller:
    Container ID:    containerd://29237a421bc9fffaa88912ea6e73ee1e49a4f31445fe2f14f47817d64c0509ec
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/cluster/template-controller:2.2.6
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/cluster/template-controller@sha256:819ebd605f7b14d942c61c86e7222fd66bb56e5c7b8fa5b697ba3c08e3a96a78
    Ports:           8080/TCP, 9443/TCP, 8081/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /template-controller
    Args:
      --leader-elect
      --health-probe-bind-address=:8081
      --webhook-cert-path=/tmp/k8s-webhook-server/serving-certs
      --metrics-bind-address=:8080
      --metrics-secure=false
      --webhook-enabled=true
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:47 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/template-controller/livez delay=15s timeout=1s period=20s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/template-controller/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp/k8s-webhook-server/serving-certs from webhook-certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x7h5p (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  webhook-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  webhook-server-cert
    Optional:    false
  kube-api-access-x7h5p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                From               Message
  ----     ------       ----               ----               -------
  Normal   Scheduled    57m                default-scheduler  Successfully assigned orch-cluster/cluster-manager-template-controller-6c9596f65b-9twkl to orch-tf
  Warning  FailedMount  57m (x4 over 57m)  kubelet            MountVolume.SetUp failed for volume "webhook-certs" : secret "webhook-server-cert" not found
  Normal   Pulled       57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created      57m                kubelet            Created container: istio-init
  Normal   Started      57m                kubelet            Started container istio-init
  Normal   Pulled       57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created      57m                kubelet            Created container: istio-proxy
  Normal   Started      57m                kubelet            Started container istio-proxy
  Warning  Unhealthy    57m                kubelet            Startup probe failed: Get "http://10.42.65.214:15021/healthz/ready": dial tcp 10.42.65.214:15021: connect: connection refused
  Normal   Pulling      57m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/template-controller:2.2.6"
  Normal   Pulled       57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/template-controller:2.2.6" in 5.028s (5.028s including waiting). Image size: 20412804 bytes.
  Normal   Created      57m                kubelet            Created container: template-controller
  Normal   Started      57m                kubelet            Started container template-controller


Name:             intel-infra-provider-manager-5c7d8cc486-p5dpk
Namespace:        orch-cluster
Priority:         0
Service Account:  intel-infra-provider
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:32 +0000
Labels:           app=intel-infra-provider-manager
                  app.kubernetes.io/instance=intel-infra-provider
                  app.kubernetes.io/name=intel-infra-provider
                  pod-template-hash=5c7d8cc486
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=intel-infra-provider
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 52a4b3831b666708af11b425aff86f136574a837e86c9cb75dd2e6f90dab5630
                  cni.projectcalico.org/podIP: 10.42.65.232/32
                  cni.projectcalico.org/podIPs: 10.42.65.232/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: intel-infra-provider-manager
                  kubectl.kubernetes.io/default-logs-container: intel-infra-provider-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.232
IPs:
  IP:           10.42.65.232
Controlled By:  ReplicaSet/intel-infra-provider-manager-5c7d8cc486
Init Containers:
  istio-init:
    Container ID:  containerd://c595199b2db99f601732e49aab4b6a8ece816fbdb4d8472d133c300c3444c582
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:39 +0000
      Finished:     Tue, 03 Feb 2026 17:53:40 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gg4jw (ro)
  istio-proxy:
    Container ID:  containerd://686e24a7f621444a9370cdb4525eff90f4431157cecde5660d2dc7eae8b1d1c7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      intel-infra-provider-manager-5c7d8cc486-p5dpk (v1:metadata.name)
      POD_NAMESPACE:                 orch-cluster (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"metrics","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     intel-infra-provider-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      intel-infra-provider-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-cluster/deployments/intel-infra-provider-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/intel-infra-provider-manager/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/intel-infra-provider-manager/readyz":{"httpGet":{"path":"/readyz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gg4jw (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  intel-infra-provider-manager:
    Container ID:    containerd://0f052fbf64f579a9bcd7db9a3408c730cd2dee5ce7b31b7c95cbb2adcfa22b10
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-manager:1.3.4
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-manager@sha256:9c2df29109c901161f6f6470a16fe211276c6088f8aa5314f043cec3ee309a7c
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /manager
    Args:
      --leader-elect
      --health-probe-bind-address=:8081
      --metrics-bind-address=:8080
      --metrics-secure=false
      --inventory-endpoint=inventory.orch-infra.svc.cluster.local:50051
      --use-inv-stub=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/intel-infra-provider-manager/livez delay=15s timeout=1s period=20s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/intel-infra-provider-manager/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gg4jw (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-gg4jw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  52m                default-scheduler  Successfully assigned orch-cluster/intel-infra-provider-manager-5c7d8cc486-p5dpk to orch-tf
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-init
  Normal   Started    52m                kubelet            Started container istio-init
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-proxy
  Normal   Started    52m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x7 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.232:15021/healthz/ready": dial tcp 10.42.65.232:15021: connect: connection refused
  Warning  Unhealthy  52m                kubelet            Startup probe failed: Get "http://10.42.65.232:15021/healthz/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  52m                kubelet            Startup probe failed: HTTP probe failed with statuscode: 503
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-manager:1.3.4"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-manager:1.3.4" in 15.518s (15.518s including waiting). Image size: 25370828 bytes.
  Normal   Created    52m                kubelet            Created container: intel-infra-provider-manager
  Normal   Started    51m                kubelet            Started container intel-infra-provider-manager


Name:             intel-infra-provider-southbound-5879db47fb-mnj5t
Namespace:        orch-cluster
Priority:         0
Service Account:  intel-infra-provider
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:30 +0000
Labels:           app=southbound-api
                  app.kubernetes.io/instance=intel-infra-provider
                  app.kubernetes.io/name=intel-infra-provider
                  pod-template-hash=5879db47fb
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=intel-infra-provider
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 4711379410a71cdda7efee79053470f9fae37f0137855e08af4ca6997a84e17d
                  cni.projectcalico.org/podIP: 10.42.65.231/32
                  cni.projectcalico.org/podIPs: 10.42.65.231/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: intel-infra-provider-southbound
                  kubectl.kubernetes.io/default-logs-container: intel-infra-provider-southbound
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.231
IPs:
  IP:           10.42.65.231
Controlled By:  ReplicaSet/intel-infra-provider-southbound-5879db47fb
Init Containers:
  istio-init:
    Container ID:  containerd://4f455959e158188a92332d50a81b1755c5241f78a23cb088a53b924fa7a68d83
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:38 +0000
      Finished:     Tue, 03 Feb 2026 17:53:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-br6gg (ro)
  istio-proxy:
    Container ID:  containerd://ca284c932801301cf1f9aee71b6d8e5840474bfe8e6c7211a77ce117263a5e5e
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:42 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      intel-infra-provider-southbound-5879db47fb-mnj5t (v1:metadata.name)
      POD_NAMESPACE:                 orch-cluster (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"grpc","containerPort":50020,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     intel-infra-provider-southbound
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      intel-infra-provider-southbound
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-cluster/deployments/intel-infra-provider-southbound
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/intel-infra-provider-southbound/livez":{"grpc":{"port":50020,"service":""},"timeoutSeconds":1},"/app-health/intel-infra-provider-southbound/readyz":{"grpc":{"port":50020,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-br6gg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  intel-infra-provider-southbound:
    Container ID:    containerd://6bb64bf0901357356fe885b214c17f6daea868ee9b95eb09e3df250676b7210d
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-southbound:1.3.4
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-southbound@sha256:1b7883afa0fba26588ed2a23f42f157a7437b540d01b45af982abc83d1b4a826
    Port:            50020/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /southbound_handler
    Args:
      --logLevel=info
      --useGrpcStubMiddleware=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/intel-infra-provider-southbound/livez delay=15s timeout=1s period=20s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/intel-infra-provider-southbound/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_CLIENT_ID:                 cluster-management-client
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
      RATE_LIMITER_QPS:               30
      RATE_LIMITER_BURST:             100
      INVENTORY_ADDRESS:              inventory.orch-infra.svc.cluster.local:50051
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-br6gg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-br6gg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  52m                 default-scheduler  Successfully assigned orch-cluster/intel-infra-provider-southbound-5879db47fb-mnj5t to orch-tf
  Normal   Pulled     52m                 kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                 kubelet            Created container: istio-init
  Normal   Started    52m                 kubelet            Started container istio-init
  Normal   Pulled     52m                 kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                 kubelet            Created container: istio-proxy
  Normal   Started    52m                 kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x13 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.231:15021/healthz/ready": dial tcp 10.42.65.231:15021: connect: connection refused
  Normal   Pulling    52m                 kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-southbound:1.3.4"
  Normal   Pulled     52m                 kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/cluster/capi-provider-intel-southbound:1.3.4" in 13.742s (13.742s including waiting). Image size: 25870875 bytes.
  Normal   Created    52m                 kubelet            Created container: intel-infra-provider-southbound
  Normal   Started    51m                 kubelet            Started container intel-infra-provider-southbound
  Warning  Unhealthy  51m                 kubelet            Readiness probe failed: Get "http://10.42.65.231:15020/app-health/intel-infra-provider-southbound/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             postgresql-cluster-1
Namespace:        orch-database
Priority:         0
Service Account:  postgresql-cluster
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:33:18 +0000
Labels:           cnpg.io/cluster=postgresql-cluster
                  cnpg.io/instanceName=postgresql-cluster-1
                  cnpg.io/instanceRole=primary
                  cnpg.io/podRole=instance
                  role=primary
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=postgresql-cluster-1
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: f7223219eff4f6e9f233e58bf0e9ac4af43f0dd60f0a16084537752e770cdd29
                  cni.projectcalico.org/podIP: 10.42.65.117/32
                  cni.projectcalico.org/podIPs: 10.42.65.117/32
                  cnpg.io/nodeSerial: 1
                  cnpg.io/operatorVersion: 1.27.0
                  cnpg.io/podEnvHash: 56997dd48b
                  cnpg.io/podSpec:
                    {"volumes":[{"name":"pgdata","persistentVolumeClaim":{"claimName":"postgresql-cluster-1"}},{"name":"scratch-data","emptyDir":{}},{"name":"...
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: postgres
                  kubectl.kubernetes.io/default-logs-container: postgres
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.117
IPs:
  IP:           10.42.65.117
Controlled By:  Cluster/postgresql-cluster
Init Containers:
  istio-init:
    Container ID:  containerd://e5fe00ad4ecf80f140f1630b4e598e9458401c7f36b45c9fa534886bac8018a3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:33:19 +0000
      Finished:     Tue, 03 Feb 2026 17:33:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fpcfw (ro)
  istio-proxy:
    Container ID:  containerd://aca6c97232722cca4bfa1edd01031ff5f57bfada56780852d0808763e9cb0978
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      postgresql-cluster-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-database (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"postgresql","containerPort":5432,"protocol":"TCP"}
                                         ,{"name":"metrics","containerPort":9187,"protocol":"TCP"}
                                         ,{"name":"status","containerPort":8000,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     postgres
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      postgresql-cluster-1
      ISTIO_META_OWNER:              kubernetes://apis/v1/namespaces/orch-database/pods/postgresql-cluster-1
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/postgres/livez":{"httpGet":{"path":"/healthz","port":8000,"scheme":"HTTPS"},"timeoutSeconds":5},"/app-health/postgres/readyz":{"httpGet":{"path":"/readyz","port":8000,"scheme":"HTTPS"},"timeoutSeconds":5},"/app-health/postgres/startupz":{"httpGet":{"path":"/startupz","port":8000,"scheme":"HTTPS"},"timeoutSeconds":5}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fpcfw (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  bootstrap-controller:
    Container ID:    containerd://36e5c9030ad002d0143857be19f2cb07326fc4902194ad1c7756b65f8283fe73
    Image:           ghcr.io/cloudnative-pg/cloudnative-pg:1.27.0
    Image ID:        ghcr.io/cloudnative-pg/cloudnative-pg@sha256:9e5633b36f1f3ff0bb28b434ce51c95fbb8428a4ab47bc738ea403eb09dbf945
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /manager
      bootstrap
      /controller/manager
      --log-level=info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:33:22 +0000
      Finished:     Tue, 03 Feb 2026 17:33:22 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     256Mi
    Environment:  <none>
    Mounts:
      /controller from scratch-data (rw)
      /dev/shm from shm (rw)
      /run from scratch-data (rw)
      /var/lib/postgresql/data from pgdata (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fpcfw (ro)
Containers:
  postgres:
    Container ID:    containerd://911f09dd370d2d360587ba0d23d58678000ae5887a6c0625a7fb390afd9d68f8
    Image:           ghcr.io/cloudnative-pg/postgresql:17
    Image ID:        ghcr.io/cloudnative-pg/postgresql@sha256:5e2077b0b7a7031ae656c6ab37d9633f674d08f277a382b7f02341c19afc6832
    Ports:           5432/TCP, 9187/TCP, 8000/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /controller/manager
      instance
      run
      --status-port-tls
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   256Mi
    Liveness:   http-get http://:15020/app-health/postgres/livez delay=0s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/postgres/readyz delay=0s timeout=5s period=10s #success=1 #failure=3
    Startup:    http-get http://:15020/app-health/postgres/startupz delay=0s timeout=5s period=10s #success=1 #failure=360
    Environment:
      PGDATA:        /var/lib/postgresql/data/pgdata
      POD_NAME:      postgresql-cluster-1
      NAMESPACE:     orch-database
      CLUSTER_NAME:  postgresql-cluster
      PSQL_HISTORY:  /controller/tmp/.psql_history
      PGPORT:        5432
      PGHOST:        /controller/run
      TMPDIR:        /controller/tmp
    Mounts:
      /controller from scratch-data (rw)
      /dev/shm from shm (rw)
      /run from scratch-data (rw)
      /var/lib/postgresql/data from pgdata (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fpcfw (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  pgdata:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  postgresql-cluster-1
    ReadOnly:   false
  scratch-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  shm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  kube-api-access-fpcfw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             postgresql-cluster-1-initdb-qhs2t
Namespace:        orch-database
Priority:         0
Service Account:  postgresql-cluster
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:32:58 +0000
Labels:           batch.kubernetes.io/controller-uid=82f73ada-4881-4c3e-ac29-5d36fce644bc
                  batch.kubernetes.io/job-name=postgresql-cluster-1-initdb
                  cnpg.io/cluster=postgresql-cluster
                  cnpg.io/instanceName=postgresql-cluster-1
                  cnpg.io/jobRole=initdb
                  controller-uid=82f73ada-4881-4c3e-ac29-5d36fce644bc
                  job-name=postgresql-cluster-1-initdb
Annotations:      cni.projectcalico.org/containerID: 84592688ed906fb0e1863f1316adf846cad060333403ec1ca623f5717834f74b
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  cnpg.io/operatorVersion: 1.27.0
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.110
IPs:
  IP:           10.42.65.110
Controlled By:  Job/postgresql-cluster-1-initdb
Init Containers:
  bootstrap-controller:
    Container ID:    containerd://1b5c2e3326bbbe0abbc2193f657e5bd21e2298b01e0d040981fa81487b4c4e1a
    Image:           ghcr.io/cloudnative-pg/cloudnative-pg:1.27.0
    Image ID:        ghcr.io/cloudnative-pg/cloudnative-pg@sha256:9e5633b36f1f3ff0bb28b434ce51c95fbb8428a4ab47bc738ea403eb09dbf945
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /manager
      bootstrap
      /controller/manager
      --log-level=info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:32:59 +0000
      Finished:     Tue, 03 Feb 2026 17:32:59 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     256Mi
    Environment:  <none>
    Mounts:
      /controller from scratch-data (rw)
      /dev/shm from shm (rw)
      /run from scratch-data (rw)
      /var/lib/postgresql/data from pgdata (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5rmt (ro)
Containers:
  initdb:
    Container ID:    containerd://a2240641693e6c6d439a440e03cd28827c4155146d8eb5d4ce1f6010f2e605af
    Image:           ghcr.io/cloudnative-pg/postgresql:17
    Image ID:        ghcr.io/cloudnative-pg/postgresql@sha256:5e2077b0b7a7031ae656c6ab37d9633f674d08f277a382b7f02341c19afc6832
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /controller/manager
      instance
      init
      --initdb-flags
      --encoding=UTF8 --lc-collate=en_US.UTF-8 --lc-ctype=en_US.UTF-8
      --post-init-sql
      'CREATE DATABASE "orch-app-app-orch-catalog";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-app-app-orch-catalog" FROM PUBLIC;' 'CREATE ROLE "orch-app-app-orch-catalog_user";' 'GRANT CONNECT ON DATABASE "orch-app-app-orch-catalog" TO "orch-app-app-orch-catalog_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-app-app-orch-catalog" TO "orch-app-app-orch-catalog_user";' 'ALTER DATABASE "orch-app-app-orch-catalog" OWNER TO "orch-app-app-orch-catalog_user";' COMMIT\; 'CREATE DATABASE "orch-platform-platform-keycloak";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-platform-platform-keycloak" FROM PUBLIC;' 'CREATE ROLE "orch-platform-platform-keycloak_user";' 'GRANT CONNECT ON DATABASE "orch-platform-platform-keycloak" TO "orch-platform-platform-keycloak_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-platform-platform-keycloak" TO "orch-platform-platform-keycloak_user";' 'ALTER DATABASE "orch-platform-platform-keycloak" OWNER TO "orch-platform-platform-keycloak_user";' COMMIT\; 'CREATE DATABASE "orch-platform-vault";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-platform-vault" FROM PUBLIC;' 'CREATE ROLE "orch-platform-vault_user";' 'GRANT CONNECT ON DATABASE "orch-platform-vault" TO "orch-platform-vault_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-platform-vault" TO "orch-platform-vault_user";' 'ALTER DATABASE "orch-platform-vault" OWNER TO "orch-platform-vault_user";' COMMIT\; 'CREATE DATABASE "orch-infra-inventory";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-infra-inventory" FROM PUBLIC;' 'CREATE ROLE "orch-infra-inventory_user";' 'GRANT CONNECT ON DATABASE "orch-infra-inventory" TO "orch-infra-inventory_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-infra-inventory" TO "orch-infra-inventory_user";' 'ALTER DATABASE "orch-infra-inventory" OWNER TO "orch-infra-inventory_user";' COMMIT\; 'CREATE DATABASE "orch-infra-alerting";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-infra-alerting" FROM PUBLIC;' 'CREATE ROLE "orch-infra-alerting_user";' 'GRANT CONNECT ON DATABASE "orch-infra-alerting" TO "orch-infra-alerting_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-infra-alerting" TO "orch-infra-alerting_user";' 'ALTER DATABASE "orch-infra-alerting" OWNER TO "orch-infra-alerting_user";' COMMIT\; 'CREATE DATABASE "orch-iam-iam-tenancy";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-iam-iam-tenancy" FROM PUBLIC;' 'CREATE ROLE "orch-iam-iam-tenancy_user";' 'GRANT CONNECT ON DATABASE "orch-iam-iam-tenancy" TO "orch-iam-iam-tenancy_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-iam-iam-tenancy" TO "orch-iam-iam-tenancy_user";' 'ALTER DATABASE "orch-iam-iam-tenancy" OWNER TO "orch-iam-iam-tenancy_user";' COMMIT\; 'CREATE DATABASE "orch-infra-mps";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-infra-mps" FROM PUBLIC;' 'CREATE ROLE "orch-infra-mps_user";' 'GRANT CONNECT ON DATABASE "orch-infra-mps" TO "orch-infra-mps_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-infra-mps" TO "orch-infra-mps_user";' 'ALTER DATABASE "orch-infra-mps" OWNER TO "orch-infra-mps_user";' COMMIT\; 'CREATE DATABASE "orch-infra-rps";' BEGIN\; 'REVOKE CREATE ON SCHEMA public FROM PUBLIC;' 'REVOKE ALL ON DATABASE "orch-infra-rps" FROM PUBLIC;' 'CREATE ROLE "orch-infra-rps_user";' 'GRANT CONNECT ON DATABASE "orch-infra-rps" TO "orch-infra-rps_user";' 'GRANT ALL PRIVILEGES ON DATABASE "orch-infra-rps" TO "orch-infra-rps_user";' 'ALTER DATABASE "orch-infra-rps" OWNER TO "orch-infra-rps_user";' COMMIT\;
      --app-db-name
      postgres
      --app-user
      orch-database-postgresql_user
      --log-level=info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:33:11 +0000
      Finished:     Tue, 03 Feb 2026 17:33:14 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  256Mi
    Environment:
      PGDATA:        /var/lib/postgresql/data/pgdata
      POD_NAME:      postgresql-cluster-1-initdb
      NAMESPACE:     orch-database
      CLUSTER_NAME:  postgresql-cluster
      PSQL_HISTORY:  /controller/tmp/.psql_history
      PGPORT:        5432
      PGHOST:        /controller/run
      TMPDIR:        /controller/tmp
    Mounts:
      /controller from scratch-data (rw)
      /dev/shm from shm (rw)
      /run from scratch-data (rw)
      /var/lib/postgresql/data from pgdata (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5rmt (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  pgdata:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  postgresql-cluster-1
    ReadOnly:   false
  scratch-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  shm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  kube-api-access-f5rmt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             auth-service-57bdbd8d9d-mt6s6
Namespace:        orch-gateway
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:52:31 +0000
Labels:           app.kubernetes.io/instance=auth-service
                  app.kubernetes.io/name=auth-service
                  pod-template-hash=57bdbd8d9d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=auth-service
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 42441fc5ae8ee21506f99781223f6525cfa94d25d5e3bb6cb70eb0c2df6f4b70
                  cni.projectcalico.org/podIP: 10.42.65.218/32
                  cni.projectcalico.org/podIPs: 10.42.65.218/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: auth-service
                  kubectl.kubernetes.io/default-logs-container: auth-service
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.218
IPs:
  IP:           10.42.65.218
Controlled By:  ReplicaSet/auth-service-57bdbd8d9d
Init Containers:
  istio-init:
    Container ID:  containerd://afc46d150f4021c59a0410e9acb7079029827764055ad72c271a0a9d31ab8b9b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:52:33 +0000
      Finished:     Tue, 03 Feb 2026 17:52:33 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bldqz (ro)
  istio-proxy:
    Container ID:  containerd://f84f69f236cce4933ef725bf7275ed03ea895aa92148259604821e277635dd31
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:52:35 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      auth-service-57bdbd8d9d-mt6s6 (v1:metadata.name)
      POD_NAMESPACE:                 orch-gateway (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     auth-service
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      auth-service
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-gateway/deployments/auth-service
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/auth-service/livez":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/auth-service/readyz":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bldqz (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  auth-service:
    Container ID:    containerd://ded65e8ae062e04ead1760d2576f9ebe06e86627ab42209c93dfc28480e3e510
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/common/auth-service:26.0.1
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/common/auth-service@sha256:8319cb7252d1b64ea28f4e8683e20da6b09b68fb956fcc7d5eb7eee7b0b1d4fa
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -jwksURL=http://platform-keycloak.orch-platform.svc:8080/realms/master/protocol/openid-connect/certs
      -rolesFile=/config/roles.txt
    State:          Running
      Started:      Tue, 03 Feb 2026 17:52:42 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/auth-service/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/auth-service/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /config from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bldqz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      auth-service
    Optional:  false
  kube-api-access-bldqz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-gateway/auth-service-57bdbd8d9d-mt6s6 to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-proxy
  Normal   Started    53m                kubelet            Started container istio-proxy
  Warning  Unhealthy  53m (x2 over 53m)  kubelet            Startup probe failed: Get "http://10.42.65.218:15021/healthz/ready": dial tcp 10.42.65.218:15021: connect: connection refused
  Normal   Pulling    53m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/common/auth-service:26.0.1"
  Normal   Pulled     53m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/common/auth-service:26.0.1" in 2.531s (2.531s including waiting). Image size: 5219736 bytes.
  Normal   Created    53m                kubelet            Created container: auth-service
  Normal   Started    53m                kubelet            Started container auth-service


Name:             certificate-file-server-7d9bc8987d-hmpx4
Namespace:        orch-gateway
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:57:26 +0000
Labels:           app.kubernetes.io/instance=certificate-file-server
                  app.kubernetes.io/name=certificate-file-server
                  pod-template-hash=7d9bc8987d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=certificate-file-server
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 3218441d37ef00fa0f6795a4d20f71a56f8030867c6eb5251fb42b4af5c1e636
                  cni.projectcalico.org/podIP: 10.42.65.250/32
                  cni.projectcalico.org/podIPs: 10.42.65.250/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: certificate-file-server
                  kubectl.kubernetes.io/default-logs-container: certificate-file-server
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.250
IPs:
  IP:           10.42.65.250
Controlled By:  ReplicaSet/certificate-file-server-7d9bc8987d
Init Containers:
  istio-init:
    Container ID:  containerd://5389ccc0ebe91114cc1d516a4005351fccbbc68784bf4ddfe73e4961e7251601
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:57:32 +0000
      Finished:     Tue, 03 Feb 2026 17:57:32 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kk5d7 (ro)
  istio-proxy:
    Container ID:  containerd://bc065a125e32560b163b0913296913bcb29e5eb84ab2607d52e474cdd7c26e6e
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:57:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      certificate-file-server-7d9bc8987d-hmpx4 (v1:metadata.name)
      POD_NAMESPACE:                 orch-gateway (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     certificate-file-server
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      certificate-file-server
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-gateway/deployments/certificate-file-server
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kk5d7 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  certificate-file-server:
    Container ID:    containerd://6058733fcb533b1d67af8264c60d97a2e4db808f910dd26294ebb9289ac242a5
    Image:           caddy:2.10.2
    Image ID:        docker.io/library/caddy@sha256:f20f80e1fb627294fb84b8515b7593aff8018c840f1396dc942a50ed0c2db648
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:57:44 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/caddy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kk5d7 (ro)
      /www/ from data (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      certificate-file-server
    Optional:  false
  data:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  boots-ca-cert
    Optional:    false
  kube-api-access-kk5d7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  48m   default-scheduler  Successfully assigned orch-gateway/certificate-file-server-7d9bc8987d-hmpx4 to orch-tf
  Normal   Pulled     48m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m   kubelet            Created container: istio-init
  Normal   Started    48m   kubelet            Started container istio-init
  Normal   Pulled     48m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m   kubelet            Created container: istio-proxy
  Normal   Started    48m   kubelet            Started container istio-proxy
  Warning  Unhealthy  48m   kubelet            Startup probe failed: Get "http://10.42.65.250:15021/healthz/ready": dial tcp 10.42.65.250:15021: connect: connection refused
  Warning  Unhealthy  48m   kubelet            Startup probe failed: Get "http://10.42.65.250:15021/healthz/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Normal   Pulled     48m   kubelet            Container image "caddy:2.10.2" already present on machine
  Normal   Created    48m   kubelet            Created container: certificate-file-server
  Normal   Started    48m   kubelet            Started container certificate-file-server


Name:             secret-wait-tls-orch-67a6b1d971-5qkf8
Namespace:        orch-gateway
Priority:         0
Service Account:  secret-wait-tls-orch
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:11 +0000
Labels:           batch.kubernetes.io/controller-uid=14982e6e-0c41-4297-aa7a-8a53e7ceb870
                  batch.kubernetes.io/job-name=secret-wait-tls-orch-67a6b1d971
                  controller-uid=14982e6e-0c41-4297-aa7a-8a53e7ceb870
                  job-name=secret-wait-tls-orch-67a6b1d971
Annotations:      cni.projectcalico.org/containerID: 259b23a6862a45099b86ecc287a3fef9dc643a538e254b9bf8e500dbc851cbda
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.123
IPs:
  IP:           10.42.65.123
Controlled By:  Job/secret-wait-tls-orch-67a6b1d971
Containers:
  secret-wait:
    Container ID:    containerd://2631af43130a5fa2708e540cd0fe7d38f13c0c09e69183a9c0a4e53b13ee07ff
    Image:           alpine/kubectl:1.34.1
    Image ID:        docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      timeout
      240h
      sh
      -c
      until kubectl get secret -n orch-gateway tls-orch; do sleep 10; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:13 +0000
      Finished:     Tue, 03 Feb 2026 17:35:14 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vm8fq (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-vm8fq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             traefik-574fb4cb85-wh58l
Namespace:        orch-gateway
Priority:         0
Service Account:  traefik
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:39:42 +0000
Labels:           app.kubernetes.io/instance=traefik-orch-gateway
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=traefik
                  helm.sh/chart=traefik-37.2.0
                  pod-template-hash=574fb4cb85
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=traefik
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 6f48c2a5b306d45c899bea614eeacff4aab52a7708977127282b1ce96c03f52e
                  cni.projectcalico.org/podIP: 10.42.65.181/32
                  cni.projectcalico.org/podIPs: 10.42.65.181/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: traefik
                  kubectl.kubernetes.io/default-logs-container: traefik
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
                  traffic.sidecar.istio.io/excludeInboundPorts: 8000,8443,8080,4433
                  traffic.sidecar.istio.io/excludeOutboundIPRanges: 10.96.0.1/32
                  traffic.sidecar.istio.io/excludeOutboundPorts: 4433
Status:           Running
IP:               10.42.65.181
IPs:
  IP:           10.42.65.181
Controlled By:  ReplicaSet/traefik-574fb4cb85
Init Containers:
  istio-init:
    Container ID:  containerd://0c0200e0468d20afae032a1740658f677b509a1af75265c0798b56667af669e0
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      10.96.0.1/32
      -b
      *
      -d
      15090,15021,8000,8443,8080,4433,15020
      -o
      4433
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:39:43 +0000
      Finished:     Tue, 03 Feb 2026 17:39:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbnwn (ro)
  istio-proxy:
    Container ID:  containerd://86cbbbd883acf30ed14bbe433906e6fca333435f24142f4e0a8fe702d75d9828
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      traefik-574fb4cb85-wh58l (v1:metadata.name)
      POD_NAMESPACE:                 orch-gateway (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"metrics","containerPort":9100,"protocol":"TCP"}
                                         ,{"name":"tcpamt","containerPort":4433,"protocol":"TCP"}
                                         ,{"name":"traefik","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"web","containerPort":8000,"protocol":"TCP"}
                                         ,{"name":"websecure","containerPort":8443,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     traefik
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      traefik
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-gateway/deployments/traefik
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_PROMETHEUS_ANNOTATIONS:  {"scrape":"true","path":"/metrics","port":"9100"}
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/traefik/livez":{"httpGet":{"path":"/ping","port":8080,"scheme":"HTTP"},"timeoutSeconds":2},"/app-health/traefik/readyz":{"httpGet":{"path":"/ping","port":8080,"scheme":"HTTP"},"timeoutSeconds":2}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbnwn (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  traefik:
    Container ID:    containerd://b98010b4f08452473001ddfb54741225d934a157cd22a82f674e7846bfe3a888
    Image:           docker.io/traefik:v3.5.3
    Image ID:        docker.io/library/traefik@sha256:d6be8725d21b45bdd84b93ea01438256e0e3c94aa8fa51834fe87f37cd5d4af8
    Ports:           9100/TCP, 4433/TCP, 8080/TCP, 8000/TCP, 8443/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --entryPoints.metrics.address=:9100/tcp
      --entryPoints.tcpamt.address=:4433/tcp
      --entryPoints.traefik.address=:8080/tcp
      --entryPoints.web.address=:8000/tcp
      --entryPoints.websecure.address=:8443/tcp
      --api.dashboard=true
      --ping=true
      --metrics.prometheus=true
      --metrics.prometheus.entrypoint=metrics
      --metrics.prometheus.buckets=0.1,0.3,0.6,0.9,1.2,1.5,2.0,2.5,5.0,10.0
      --providers.kubernetescrd
      --providers.kubernetescrd.allowCrossNamespace=true
      --providers.kubernetescrd.allowEmptyServices=true
      --providers.kubernetesingress
      --providers.kubernetesingress.allowEmptyServices=true
      --providers.kubernetesingress.ingressendpoint.publishedservice=orch-gateway/traefik
      --providers.kubernetescrd.namespaces=default,orch-gateway,orch-platform,orch-app,orch-infra,orch-cluster,orch-harbor,cattle-system,connect-gateway,orch-ui,orch-secret,orch-iam
      --entryPoints.tcpamt.http.middlewares=orch-gateway-tcp-rate-limit@kubernetescrd
      --entryPoints.websecure.http.middlewares=orch-gateway-rate-limit@kubernetescrd,orch-gateway-cors@kubernetescrd
      --entryPoints.websecure.http.tls=true
      --log.format=json
      --log.level=INFO
      --accesslog=true
      --accesslog.format=json
      --accesslog.fields.defaultmode=keep
      --accesslog.fields.headers.defaultmode=drop
      --experimental.localPlugins.jwt.moduleName=github.com/team-carepay/traefik-jwt-plugin
    State:          Running
      Started:      Tue, 03 Feb 2026 17:39:57 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/traefik/livez delay=2s timeout=2s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/traefik/readyz delay=2s timeout=2s period=10s #success=1 #failure=1
    Environment:
      POD_NAME:       traefik-574fb4cb85-wh58l (v1:metadata.name)
      POD_NAMESPACE:  orch-gateway (v1:metadata.namespace)
      USER:           traefik
      GOMAXPROCS:     64 (limits.cpu)
      GOMEMLIMIT:     58982MiB
    Mounts:
      ./plugins-local/src/github.com/team-carepay/traefik-jwt-plugin/ from jwt-plugin (rw)
      ./traefik-jwt-plugin/ from jwt-plugin (ro)
      /data from data (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbnwn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  jwt-plugin:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      jwt-plugin
    Optional:  false
  kube-api-access-sbnwn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             harbor-oci-core-7f68c85654-svqt4
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:27 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=core
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  chart=harbor
                  component=core
                  heritage=Helm
                  pod-template-hash=7f68c85654
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
Annotations:      checksum/configmap: 84762ba6937a8ee22db768a2f286b7b4a6ed5d3fc2bec551ef36f488fa1a619b
                  checksum/secret: e0febdf37db3ea222f785e089f9c8da70683a9dd98c9131df8480b1aaa82c3cb
                  checksum/secret-jobservice: c6f4fd5521c21cfd11fc3223319c14558258e80a70998e7b68e41e743e263f83
                  cni.projectcalico.org/containerID: 1ac590236ee39e4751084669edfb683239cdbf50d7a1b850b4ac6d69ff205b80
                  cni.projectcalico.org/podIP: 10.42.65.126/32
                  cni.projectcalico.org/podIPs: 10.42.65.126/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: core
                  kubectl.kubernetes.io/default-logs-container: core
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.126
IPs:
  IP:           10.42.65.126
Controlled By:  ReplicaSet/harbor-oci-core-7f68c85654
Init Containers:
  istio-init:
    Container ID:  containerd://225d39f20568144cfd8ef1af998499de4da9943c68372371d1e965ca053b8024
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:28 +0000
      Finished:     Tue, 03 Feb 2026 17:35:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://be0f198d7cea3a552900ad9e6eaa362732269b50bf00dd26e67e9fe013ee6f41
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-core-7f68c85654-svqt4 (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     core
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-core
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/deployments/harbor-oci-core
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/core/livez":{"httpGet":{"path":"/api/v2.0/ping","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/core/readyz":{"httpGet":{"path":"/api/v2.0/ping","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/core/startupz":{"httpGet":{"path":"/api/v2.0/ping","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  core:
    Container ID:    containerd://338491b03d8c48b3fb264f95e1eafcb2ef256b0ccd2f1172a3035a0f33d82506
    Image:           goharbor/harbor-core:v2.13.2
    Image ID:        docker.io/goharbor/harbor-core@sha256:89114e3fb5c95aefa3285d5d627f89c46c3eaa2f4da138cedee887e58d297f2a
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:31 +0000
    Last State:      Terminated
      Reason:        Error
      Exit Code:     1
      Started:       Tue, 03 Feb 2026 17:36:10 +0000
      Finished:      Tue, 03 Feb 2026 17:36:10 +0000
    Ready:           True
    Restart Count:   2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/core/livez delay=0s timeout=1s period=10s #success=1 #failure=2
    Readiness:  http-get http://:15020/app-health/core/readyz delay=0s timeout=1s period=10s #success=1 #failure=2
    Startup:    http-get http://:15020/app-health/core/startupz delay=10s timeout=1s period=10s #success=1 #failure=360
    Environment Variables from:
      harbor-oci-core  ConfigMap  Optional: false
      harbor-oci-core  Secret     Optional: false
    Environment:
      CORE_SECRET:            <set to the key 'secret' in secret 'harbor-oci-core'>                       Optional: false
      JOBSERVICE_SECRET:      <set to the key 'JOBSERVICE_SECRET' in secret 'harbor-oci-jobservice'>      Optional: false
      HARBOR_ADMIN_PASSWORD:  <set to the key 'HARBOR_ADMIN_PASSWORD' in secret 'harbor-admin-password'>  Optional: false
    Mounts:
      /etc/core/app.conf from config (rw,path="app.conf")
      /etc/core/key from secret-key (rw,path="key")
      /etc/core/private_key.pem from token-service-private-key (rw,path="tls.key")
      /etc/core/token from psc (rw)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      harbor-oci-core
    Optional:  false
  secret-key:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  harbor-oci-core
    Optional:    false
  token-service-private-key:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  harbor-oci-core
    Optional:    false
  psc:
    Type:        EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:      
    SizeLimit:   <unset>
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             harbor-oci-database-0
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:34 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=database
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  apps.kubernetes.io/pod-index=0
                  chart=harbor
                  component=database
                  controller-revision-hash=harbor-oci-database-6d65c7dd67
                  heritage=Helm
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
                  statefulset.kubernetes.io/pod-name=harbor-oci-database-0
Annotations:      checksum/secret: 704f80d51e1bc44fe8de4ecd667591e128c9cf535d8814c55862fc1fb5d67068
                  cni.projectcalico.org/containerID: 311664044e1207f15e29810b53cf8ca739ea7f65d72b2ee84c7afd1c1dbda6da
                  cni.projectcalico.org/podIP: 10.42.65.109/32
                  cni.projectcalico.org/podIPs: 10.42.65.109/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: database
                  kubectl.kubernetes.io/default-logs-container: database
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.109
IPs:
  IP:           10.42.65.109
Controlled By:  StatefulSet/harbor-oci-database
Init Containers:
  istio-init:
    Container ID:  containerd://04427d51358dcf01d00fb53aff4a78c955f08d966950870d16641929769c4ca5
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:36 +0000
      Finished:     Tue, 03 Feb 2026 17:35:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://d0be5d518b8484af43bcf62621e5c7e93be6ad1c70a91f13e19e550d1198e760
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-database-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     database
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-database
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/statefulsets/harbor-oci-database
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  data-permissions-ensurer:
    Container ID:    containerd://28de454b6f4277dc45ec03bf8efe6aae6c2489fd9f8493c48d3d11f16c91a43b
    Image:           goharbor/harbor-db:v2.13.2
    Image ID:        docker.io/goharbor/harbor-db@sha256:fcf1f3dbc8f10183abe336712560a94fb2804ba8d3ce3ca634342e5a8df7da2c
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
    Args:
      -c
      chmod -R 700 /var/lib/postgresql/data/pgdata || true
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:13 +0000
      Finished:     Tue, 03 Feb 2026 17:36:13 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/lib/postgresql/data from database-data (rw)
Containers:
  database:
    Container ID:    containerd://9d5d1ca7baaac4c6e94f7d2556e1023bfbbb1408d71f14f67231869c9bcc11f6
    Image:           goharbor/harbor-db:v2.13.2
    Image ID:        docker.io/goharbor/harbor-db@sha256:fcf1f3dbc8f10183abe336712560a94fb2804ba8d3ce3ca634342e5a8df7da2c
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:15 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   exec [/docker-healthcheck.sh] delay=300s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/docker-healthcheck.sh] delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment Variables from:
      harbor-oci-database  Secret  Optional: false
    Environment:
      PGDATA:  /var/lib/postgresql/data/pgdata
    Mounts:
      /dev/shm from shm-volume (rw)
      /var/lib/postgresql/data from database-data (rw)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  database-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  database-data-harbor-oci-database-0
    ReadOnly:   false
  shm-volume:
    Type:        EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:      Memory
    SizeLimit:   512Mi
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             harbor-oci-jobservice-568f8466c4-47lgc
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:31 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=jobservice
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  chart=harbor
                  component=jobservice
                  heritage=Helm
                  pod-template-hash=568f8466c4
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
Annotations:      checksum/configmap: 8b5f22f41b10068dd263c04287236a7c3cf99c1b7252fb30fdb3f718f1dd2dda
                  checksum/configmap-env: c3bd9e567a155e04e657c36866f2069fb600cf14a3a07f4a2a4c656935c8577b
                  checksum/secret: 653b774b3b9f7dffd2f0841b7df367c9df3dc447acdfc48b00c275946a1b67b2
                  checksum/secret-core: f57c0cb4d62868b3df0754bc777e52e9a20501add02ad053ba489bced1e0f862
                  cni.projectcalico.org/containerID: e04ce745339d2011f99531930922a55071dc2d305a685bea02af10771a178484
                  cni.projectcalico.org/podIP: 10.42.65.87/32
                  cni.projectcalico.org/podIPs: 10.42.65.87/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: jobservice
                  kubectl.kubernetes.io/default-logs-container: jobservice
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.87
IPs:
  IP:           10.42.65.87
Controlled By:  ReplicaSet/harbor-oci-jobservice-568f8466c4
Init Containers:
  istio-init:
    Container ID:  containerd://75b05e3f3f974da6efa061663ecc8e5d366fa0c2da09b3b6cdc6cad06ad3e97c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:32 +0000
      Finished:     Tue, 03 Feb 2026 17:35:32 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://e099ffb3f1e1cbb9355ac84f43f6c501dfe051f6c588f2c3276806cb90bc650f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:33 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-jobservice-568f8466c4-47lgc (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     jobservice
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-jobservice
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/deployments/harbor-oci-jobservice
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/jobservice/livez":{"httpGet":{"path":"/api/v1/stats","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/jobservice/readyz":{"httpGet":{"path":"/api/v1/stats","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  jobservice:
    Container ID:    containerd://5cbb237f2cd0c5f9a1b687bdb055ae76fac2dfa430251c1413b9193d70ddee5e
    Image:           goharbor/harbor-jobservice:v2.13.2
    Image ID:        docker.io/goharbor/harbor-jobservice@sha256:12fe1d4271dfa60fdcd90396efcf39afe05f2316e5b3de50413bde06560cc550
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:37:31 +0000
    Last State:      Terminated
      Reason:        Error
      Exit Code:     2
      Started:       Tue, 03 Feb 2026 17:36:43 +0000
      Finished:      Tue, 03 Feb 2026 17:36:43 +0000
    Ready:           True
    Restart Count:   4
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/jobservice/livez delay=300s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/jobservice/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment Variables from:
      harbor-oci-jobservice-env  ConfigMap  Optional: false
      harbor-oci-jobservice      Secret     Optional: false
    Environment:
      CORE_SECRET:  <set to the key 'secret' in secret 'harbor-oci-core'>  Optional: false
    Mounts:
      /etc/jobservice/config.yml from jobservice-config (rw,path="config.yml")
      /var/log/jobs from job-logs (rw)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  jobservice-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      harbor-oci-jobservice
    Optional:  false
  job-logs:
    Type:        PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:   harbor-oci-jobservice
    ReadOnly:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             harbor-oci-nginx-d885fb48d-5hbdn
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:27 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=nginx
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  chart=harbor
                  component=nginx
                  heritage=Helm
                  pod-template-hash=d885fb48d
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
Annotations:      checksum/configmap: 8f2a64bd103f51e1fb0e1ad08846d6c8c2d20217c23256d2fac87ef6be639455
                  cni.projectcalico.org/containerID: 8cca316a84d61531540b9350c0c356f8ba982ce1f5e7e67d1d11379cf37527be
                  cni.projectcalico.org/podIP: 10.42.65.65/32
                  cni.projectcalico.org/podIPs: 10.42.65.65/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: nginx
                  kubectl.kubernetes.io/default-logs-container: nginx
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.65
IPs:
  IP:           10.42.65.65
Controlled By:  ReplicaSet/harbor-oci-nginx-d885fb48d
Init Containers:
  istio-init:
    Container ID:  containerd://bcf8ed2efaea83a77fad0c6ddb6ccaee204ec52aa1cec16b7c25238dc992566c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:28 +0000
      Finished:     Tue, 03 Feb 2026 17:35:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://0e0e015176d3552648161c2939f13211ca45edf8aef71039543babf0cb47bbac
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-nginx-d885fb48d-5hbdn (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     nginx
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-nginx
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/deployments/harbor-oci-nginx
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/nginx/livez":{"httpGet":{"path":"/","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/nginx/readyz":{"httpGet":{"path":"/","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  nginx:
    Container ID:    containerd://076e35657c6f5cc6a51636fc03b15d0c817e54351a2a148b1c0bbbd2b13eb31e
    Image:           goharbor/nginx-photon:v2.13.2
    Image ID:        docker.io/goharbor/nginx-photon@sha256:41aa68bd94aed9b780e7f7b8afe419fd3b0a340c8ac39a2aed25fce1993b8fea
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:35:39 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/nginx/livez delay=300s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/nginx/readyz delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/nginx/nginx.conf from config (rw,path="nginx.conf")
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        harbor-oci-nginx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From     Message
  ----     ------     ----  ----     -------
  Warning  Unhealthy  52m   kubelet  Liveness probe failed: Get "http://10.42.65.65:15020/app-health/nginx/livez": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             harbor-oci-portal-5d4cd6dccd-skfbn
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:27 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=portal
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  chart=harbor
                  component=portal
                  heritage=Helm
                  pod-template-hash=5d4cd6dccd
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
Annotations:      checksum/configmap: f58653a2ca05efac4968afcd06d2292f6dc296642db2b224116df2062a22d795
                  cni.projectcalico.org/containerID: 2c540b234916d3718f3b38c034a82e2bd47d1cd8e56e184e664413b33048cc5b
                  cni.projectcalico.org/podIP: 10.42.65.127/32
                  cni.projectcalico.org/podIPs: 10.42.65.127/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: portal
                  kubectl.kubernetes.io/default-logs-container: portal
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.127
IPs:
  IP:           10.42.65.127
Controlled By:  ReplicaSet/harbor-oci-portal-5d4cd6dccd
Init Containers:
  istio-init:
    Container ID:  containerd://5ce8f7d41af3fd3f1780004fd7436fefc5622e174725aaf388c05163bf2341d8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:28 +0000
      Finished:     Tue, 03 Feb 2026 17:35:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://82fe532ccac983146ed4a8259d1dd9cdaebe23e99e9be99d14bb11f4f36b9f6f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-portal-5d4cd6dccd-skfbn (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     portal
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-portal
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/deployments/harbor-oci-portal
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/portal/livez":{"httpGet":{"path":"/","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/portal/readyz":{"httpGet":{"path":"/","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  portal:
    Container ID:    containerd://921d020844f7abc10fd47c5c1f662d14a17921dd4589f0cb1549ed1cb1e7252e
    Image:           goharbor/harbor-portal:v2.13.2
    Image ID:        docker.io/goharbor/harbor-portal@sha256:ac9c2444e1045ac6fc23ae3adbfb43443cdb31dc937be354410b6328a9768bb8
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:35:37 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/portal/livez delay=300s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/portal/readyz delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/nginx/nginx.conf from portal-config (rw,path="nginx.conf")
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  portal-config:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        harbor-oci-portal
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             harbor-oci-redis-0
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:34 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=redis
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  apps.kubernetes.io/pod-index=0
                  chart=harbor
                  component=redis
                  controller-revision-hash=harbor-oci-redis-7dd97bf94c
                  heritage=Helm
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
                  statefulset.kubernetes.io/pod-name=harbor-oci-redis-0
Annotations:      cni.projectcalico.org/containerID: 25431d186ae773447fe87931056822f460d200c57b1d98f41e8d687ffe066670
                  cni.projectcalico.org/podIP: 10.42.65.108/32
                  cni.projectcalico.org/podIPs: 10.42.65.108/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: redis
                  kubectl.kubernetes.io/default-logs-container: redis
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.108
IPs:
  IP:           10.42.65.108
Controlled By:  StatefulSet/harbor-oci-redis
Init Containers:
  istio-init:
    Container ID:  containerd://97a7c5b8e7adaca6c044aa1f51d33004c3790c350524d99d876c50d3f79e73a7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:36 +0000
      Finished:     Tue, 03 Feb 2026 17:35:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://22fccbb31910362dd994cb202762127c97c2d80189e93026fb9890db3d676ee1
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-redis-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     redis
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-redis
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/statefulsets/harbor-oci-redis
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/redis/livez":{"tcpSocket":{"port":6379},"timeoutSeconds":1},"/app-health/redis/readyz":{"tcpSocket":{"port":6379},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  redis:
    Container ID:    containerd://9748166c0a2d01a70a4eeecbe5e7bea167c03c027c0ef3450df010128b14f93b
    Image:           goharbor/redis-photon:v2.13.2
    Image ID:        docker.io/goharbor/redis-photon@sha256:7b3e7ea34477cb1df592594fb0d0ea9168945fa9f72aab9586ec357cbe935ae0
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:05 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/redis/livez delay=300s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/redis/readyz delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/lib/redis from data (rw)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:        PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:   data-harbor-oci-redis-0
    ReadOnly:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             harbor-oci-registry-df67bbdb5-25fdt
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:33 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=registry
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  chart=harbor
                  component=registry
                  heritage=Helm
                  pod-template-hash=df67bbdb5
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
Annotations:      checksum/configmap: 3d6430c2bf294b53c5017453ce6e58a0ef906296e8b05a1bdfa7bbe7e1fdfe57
                  checksum/secret: be3fc12244aa98cab4a8f1583d22bb2c65397ace718da36fce9c077e0429f851
                  checksum/secret-core: 21399b9b4de6fbdfc2b0f32f4d92e357cf9cd65edcd30755cd66be583132d89c
                  checksum/secret-jobservice: e47be7da640207568babcea311442f8fafef72a140c2549cd46022ad8e58de66
                  cni.projectcalico.org/containerID: a86f60ff244869917475d9f18ce23556fddb63312eec90ffca5981e1f555eb67
                  cni.projectcalico.org/podIP: 10.42.65.107/32
                  cni.projectcalico.org/podIPs: 10.42.65.107/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: registry
                  kubectl.kubernetes.io/default-logs-container: registry
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.107
IPs:
  IP:           10.42.65.107
Controlled By:  ReplicaSet/harbor-oci-registry-df67bbdb5
Init Containers:
  istio-init:
    Container ID:  containerd://ec749ac4944125d40c7c9edb32b218e3b2ec0b02b187e9984326b4426c9ec85b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:34 +0000
      Finished:     Tue, 03 Feb 2026 17:35:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://82b6f33f9597ee8a551899f0cef3498502d4e049a9c70c3043c11457f43a6708
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-registry-df67bbdb5-25fdt (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":5000,"protocol":"TCP"}
                                         ,{"containerPort":5001,"protocol":"TCP"}
                                         ,{"containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     registry,registryctl
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-registry
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/deployments/harbor-oci-registry
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/registry/livez":{"httpGet":{"path":"/","port":5000,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/registry/readyz":{"httpGet":{"path":"/","port":5000,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/registryctl/livez":{"httpGet":{"path":"/api/health","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/registryctl/readyz":{"httpGet":{"path":"/api/health","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  registry:
    Container ID:    containerd://decb6e1024dac9d2eff077130215f93a63244517ed2836c1bb5dd45698c8a96c
    Image:           goharbor/registry-photon:v2.13.2
    Image ID:        docker.io/goharbor/registry-photon@sha256:1fc840dda17e4bb99cb58edc2a4c2435b9413f770b9146ac716a4593061bc136
    Ports:           5000/TCP, 5001/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:35:58 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/registry/livez delay=300s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/registry/readyz delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment Variables from:
      harbor-oci-registry  Secret  Optional: false
    Environment:           <none>
    Mounts:
      /etc/registry/config.yml from registry-config (rw,path="config.yml")
      /etc/registry/passwd from registry-htpasswd (rw,path="passwd")
      /storage from registry-data (rw)
  registryctl:
    Container ID:    containerd://ec9038cc93edb9118b855d7ea084a333b2603479ede0a0d83944b4ec420ac6f1
    Image:           goharbor/harbor-registryctl:v2.13.2
    Image ID:        docker.io/goharbor/harbor-registryctl@sha256:3d82ef5215c55953540aa08b460785f9d0457a10276365ab9f7750027609295b
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:15 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/registryctl/livez delay=300s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/registryctl/readyz delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment Variables from:
      harbor-oci-registryctl  ConfigMap  Optional: false
      harbor-oci-registry     Secret     Optional: false
      harbor-oci-registryctl  Secret     Optional: false
    Environment:
      CORE_SECRET:        <set to the key 'secret' in secret 'harbor-oci-core'>                   Optional: false
      JOBSERVICE_SECRET:  <set to the key 'JOBSERVICE_SECRET' in secret 'harbor-oci-jobservice'>  Optional: false
    Mounts:
      /etc/registry/config.yml from registry-config (rw,path="config.yml")
      /etc/registryctl/config.yml from registry-config (rw,path="ctl-config.yml")
      /storage from registry-data (rw)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  registry-htpasswd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  harbor-oci-registry-htpasswd
    Optional:    false
  registry-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      harbor-oci-registry
    Optional:  false
  registry-data:
    Type:        PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:   harbor-oci-registry
    ReadOnly:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             harbor-oci-trivy-0
Namespace:        orch-harbor
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:37 +0000
Labels:           app=harbor
                  app.kubernetes.io/component=trivy
                  app.kubernetes.io/instance=harbor-oci
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=harbor
                  app.kubernetes.io/part-of=harbor
                  app.kubernetes.io/version=2.13.2
                  apps.kubernetes.io/pod-index=0
                  chart=harbor
                  component=trivy
                  controller-revision-hash=harbor-oci-trivy-5c9c5788f5
                  heritage=Helm
                  release=harbor-oci
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=harbor
                  service.istio.io/canonical-revision=2.13.2
                  statefulset.kubernetes.io/pod-name=harbor-oci-trivy-0
Annotations:      checksum/secret: e9e2625fd065d9b470ad6ae6b3dada3046ed2d341a5805532f820a46bb382776
                  cni.projectcalico.org/containerID: 30c89f5afe3a3c20410dcb5195d0d15fd9674d75ae7faeab8523c239eecc3317
                  cni.projectcalico.org/podIP: 10.42.65.112/32
                  cni.projectcalico.org/podIPs: 10.42.65.112/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: trivy
                  kubectl.kubernetes.io/default-logs-container: trivy
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.112
IPs:
  IP:           10.42.65.112
Controlled By:  StatefulSet/harbor-oci-trivy
Init Containers:
  istio-init:
    Container ID:  containerd://41530c34e8a8516dc3825320ca0330b6f520e8df12879c95a10eef782e9e086c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:43 +0000
      Finished:     Tue, 03 Feb 2026 17:35:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:                   <none>
  istio-proxy:
    Container ID:  containerd://4f8c9097bd57abced3548836e2f629f8f113d8466931387b0d858b04f3b60db4
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      harbor-oci-trivy-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-harbor (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"api-server","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     trivy
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      harbor-oci-trivy
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-harbor/statefulsets/harbor-oci-trivy
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/trivy/livez":{"httpGet":{"path":"/probe/healthy","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/trivy/readyz":{"httpGet":{"path":"/probe/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  trivy:
    Container ID:    containerd://6fe6f8c6576e093001f9bf288e758bbec69e2badea5d685bb53370ae61696aed
    Image:           goharbor/trivy-adapter-photon:v2.13.2
    Image ID:        docker.io/goharbor/trivy-adapter-photon@sha256:7a6d2284992ee5739ef0a76a9bed722bd87ec9e9ad286c6d72aa7919a606e106
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:18 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/trivy/livez delay=5s timeout=1s period=10s #success=1 #failure=10
    Readiness:  http-get http://:15020/app-health/trivy/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      HTTP_PROXY:                         
      HTTPS_PROXY:                        
      NO_PROXY:                           harbor-oci-core,harbor-oci-jobservice,harbor-oci-database,harbor-oci-registry,harbor-oci-portal,harbor-oci-trivy,harbor-oci-exporter,
      SCANNER_LOG_LEVEL:                  info
      SCANNER_TRIVY_CACHE_DIR:            /home/scanner/.cache/trivy
      SCANNER_TRIVY_REPORTS_DIR:          /home/scanner/.cache/reports
      SCANNER_TRIVY_DEBUG_MODE:           false
      SCANNER_TRIVY_VULN_TYPE:            os,library
      SCANNER_TRIVY_TIMEOUT:              5m0s
      SCANNER_TRIVY_GITHUB_TOKEN:         <set to the key 'gitHubToken' in secret 'harbor-oci-trivy'>  Optional: false
      SCANNER_TRIVY_SEVERITY:             UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL
      SCANNER_TRIVY_IGNORE_UNFIXED:       false
      SCANNER_TRIVY_SKIP_UPDATE:          false
      SCANNER_TRIVY_SKIP_JAVA_DB_UPDATE:  false
      SCANNER_TRIVY_OFFLINE_SCAN:         false
      SCANNER_TRIVY_SECURITY_CHECKS:      vuln
      SCANNER_TRIVY_INSECURE:             false
      SCANNER_API_SERVER_ADDR:            :8080
      SCANNER_REDIS_URL:                  <set to the key 'redisURL' in secret 'harbor-oci-trivy'>  Optional: false
      SCANNER_STORE_REDIS_URL:            <set to the key 'redisURL' in secret 'harbor-oci-trivy'>  Optional: false
      SCANNER_JOB_QUEUE_REDIS_URL:        <set to the key 'redisURL' in secret 'harbor-oci-trivy'>  Optional: false
    Mounts:
      /home/scanner/.cache from data (rw)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:        PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:   data-harbor-oci-trivy-0
    ReadOnly:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:          <none>


Name:             nexus-api-gw-8599d87bdb-n2rsd
Namespace:        orch-iam
Priority:         0
Service Account:  api-gw-k8s-api-service-account
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:42:18 +0000
Labels:           app=nexus-api-gw
                  pod-template-hash=8599d87bdb
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=nexus-api-gw
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: fbd74beb5da2bb251ae4d9f8da478bd7119776a9b42c1854d9f6a2c7ef8316a3
                  cni.projectcalico.org/podIP: 10.42.65.138/32
                  cni.projectcalico.org/podIPs: 10.42.65.138/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: nexus-api-gw
                  kubectl.kubernetes.io/default-logs-container: nexus-api-gw
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.138
IPs:
  IP:           10.42.65.138
Controlled By:  ReplicaSet/nexus-api-gw-8599d87bdb
Init Containers:
  istio-init:
    Container ID:  containerd://64c7b0c5770de687ad9b0d479190851fc8768616cfc04322da8eca4e10e0cb77
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:42:20 +0000
      Finished:     Tue, 03 Feb 2026 17:42:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hnhlz (ro)
  istio-proxy:
    Container ID:  containerd://666636ec1f5a25dc9e82e63e38722d61b3498b7a1bc1cd66600669ec0a2c5579
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:42:21 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      nexus-api-gw-8599d87bdb-n2rsd (v1:metadata.name)
      POD_NAMESPACE:                 orch-iam (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     nexus-api-gw
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      nexus-api-gw
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-iam/deployments/nexus-api-gw
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hnhlz (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  wait-for-job:
    Container ID:  containerd://a22641cc70ad402d36d1a234530ae391ca01bc1a44c12b22c27fc410d4689ffa
    Image:         alpine/kubectl:1.34.1
    Image ID:      docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
    Args:
      set -e
      echo "edge-orchestrator.intel.com CRD is available, waiting for CR..."
      echo "Waiting for CR 'apimappingconfigs.apimappingconfig.edge-orchestrator.intel.com' to be available..."
      until kubectl get apimappingconfigs.apimappingconfig.edge-orchestrator.intel.com; do
        echo "edge-orchestrator.intel.com CRD not found, waiting..."
        sleep 10
      done
      while true; do
        OUTPUT=$(kubectl get apimappingconfigs.apimappingconfig.edge-orchestrator.intel.com 2>&1)
        if echo "$OUTPUT" | grep -q "No resources found"; then
          echo "edge-orchestrator.intel.com CRD not found, waiting..."
          sleep 10
        else
          break
        fi
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:42:24 +0000
      Finished:     Tue, 03 Feb 2026 17:43:16 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hnhlz (ro)
Containers:
  nexus-api-gw:
    Container ID:   containerd://99c5c70e03720d7b54cedf109bc1bee9b9ce0965301810d33b3663f59a4b1b8e
    Image:          registry-rs.edgeorchestration.intel.com/edge-orch/common/nexus-api-gw:26.0.1
    Image ID:       registry-rs.edgeorchestration.intel.com/edge-orch/common/nexus-api-gw@sha256:cc9ab82486f083f9fb46acc8d82efd8c43adbad7870c5383bb62c03a266caf6f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:43:21 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      OIDC_SERVER_URL:                <set to the key 'oidc_server_url' of config map 'keycloak-api'>                      Optional: false
      OIDC_TLS_INSECURE_SKIP_VERIFY:  <set to the key 'oidc_tls_insecure_skip_verify_value' of config map 'keycloak-api'>  Optional: false
      ALLOW_MISSING_AUTH_CLIENTS:     common-metric-query-metrics,ecm-api,eim-config
      APIGWCONFIG:                    /etc/config/iam-config.yaml
      LOG_LEVEL:                      error
    Mounts:
      /etc/config from nexus-api-gw (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hnhlz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  nexus-api-gw:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nexus-api-gw
    Optional:  false
  kube-api-access-hnhlz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             tenancy-api-mapping-45g8p
Namespace:        orch-iam
Priority:         0
Service Account:  api-mapping-k8s-api-service-account
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 18:45:14 +0000
Labels:           batch.kubernetes.io/controller-uid=76e9d3c1-c856-4389-99ed-deea2811227f
                  batch.kubernetes.io/job-name=tenancy-api-mapping
                  controller-uid=76e9d3c1-c856-4389-99ed-deea2811227f
                  job-name=tenancy-api-mapping
Annotations:      cni.projectcalico.org/containerID: a5a199578a9c325decf04b4aebca043f971f03e77d1a849127c9266d46640cf5
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  sidecar.istio.io/inject: false
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.247
IPs:
  IP:           10.42.65.247
Controlled By:  Job/tenancy-api-mapping
Init Containers:
  job-wait-for-apimappingconfigs:
    Container ID:  containerd://6f1973766691b92c329d4c98c273ad7b9c4d0f2a8ff84f425c400907deb1a65d
    Image:         alpine/kubectl:1.34.1
    Image ID:      docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
    Args:
      set -e
      echo "Waiting for CRD 'apimappingconfigs.apimappingconfig.edge-orchestrator.intel.com' to be available..."
      until kubectl get crd apimappingconfigs.apimappingconfig.edge-orchestrator.intel.com; do
        echo "CRD not found, waiting..."
        sleep 50
      done
      echo "CRD is available, Exiting init container..."
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:45:16 +0000
      Finished:     Tue, 03 Feb 2026 18:45:16 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l4lzp (ro)
Containers:
  tenancy-api-mapping:
    Container ID:   containerd://5de1bfe6360d4cf1a76ae49129196a9003c868596f1b955e632cac6a781874d5
    Image:          registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-api-mapping:26.0.1
    Image ID:       registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-api-mapping@sha256:53c207e7f1c413107f7fe052a4a0179b695595d907f78966653384a5cf603f27
    Port:           <none>
    Host Port:      <none>
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:45:18 +0000
      Finished:     Tue, 03 Feb 2026 18:45:19 +0000
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l4lzp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-l4lzp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  59s   default-scheduler  Successfully assigned orch-iam/tenancy-api-mapping-45g8p to orch-tf
  Normal  Pulled     57s   kubelet            Container image "alpine/kubectl:1.34.1" already present on machine
  Normal  Created    57s   kubelet            Created container: job-wait-for-apimappingconfigs
  Normal  Started    57s   kubelet            Started container job-wait-for-apimappingconfigs
  Normal  Pulled     56s   kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-api-mapping:26.0.1" already present on machine
  Normal  Created    56s   kubelet            Created container: tenancy-api-mapping
  Normal  Started    55s   kubelet            Started container tenancy-api-mapping


Name:             tenancy-datamodel-w7p77
Namespace:        orch-iam
Priority:         0
Service Account:  tenancy-k8s-api-service-account
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 18:43:13 +0000
Labels:           batch.kubernetes.io/controller-uid=ef554f25-0c97-4f3e-8a08-d948e9adab8d
                  batch.kubernetes.io/job-name=tenancy-datamodel
                  controller-uid=ef554f25-0c97-4f3e-8a08-d948e9adab8d
                  job-name=tenancy-datamodel
Annotations:      cni.projectcalico.org/containerID: 7372fb864a166464d4c7518a67630d706730e65fdf8f16980e93576bdfdf8d97
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  sidecar.istio.io/inject: false
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.243
IPs:
  IP:           10.42.65.243
Controlled By:  Job/tenancy-datamodel
Containers:
  tenancy-datamodel:
    Container ID:   containerd://e22165a9893ca083debb03ee0f9beb683c03ad124b6c5f3db0961609b080badf
    Image:          registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-datamodel:25.2.2
    Image ID:       registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-datamodel@sha256:d116999bac4ca2c753443703ef7d07d74707db8f295bd24dae5b4600617a75eb
    Port:           <none>
    Host Port:      <none>
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:43:15 +0000
      Finished:     Tue, 03 Feb 2026 18:43:18 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      NAME:  tenancy-datamodel
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w9rkq (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-w9rkq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m     default-scheduler  Successfully assigned orch-iam/tenancy-datamodel-w7p77 to orch-tf
  Normal  Pulled     2m59s  kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-datamodel:25.2.2" already present on machine
  Normal  Created    2m59s  kubelet            Created container: tenancy-datamodel
  Normal  Started    2m58s  kubelet            Started container tenancy-datamodel


Name:             tenancy-manager-84f99749cc-bdljx
Namespace:        orch-iam
Priority:         0
Service Account:  tenancy-manager-service-account
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:42:17 +0000
Labels:           app=tenancy-manager
                  pod-template-hash=84f99749cc
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=tenancy-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 069ee7807e2ba62baa8c1065a5c1318623916324331749b389dde8378d58633f
                  cni.projectcalico.org/podIP: 10.42.65.183/32
                  cni.projectcalico.org/podIPs: 10.42.65.183/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: tenancy-manager
                  kubectl.kubernetes.io/default-logs-container: tenancy-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.183
IPs:
  IP:           10.42.65.183
Controlled By:  ReplicaSet/tenancy-manager-84f99749cc
Init Containers:
  istio-init:
    Container ID:  containerd://18b04afb22f82d5f5fc61c58b95f671c37268e5b30f8aad12dbe703b47e90081
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:42:19 +0000
      Finished:     Tue, 03 Feb 2026 17:42:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhjbj (ro)
  istio-proxy:
    Container ID:  containerd://a2212796afdb36a76cd6a784afb01b84acd34c760a001e94b5d50ad679f18b80
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:42:21 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      tenancy-manager-84f99749cc-bdljx (v1:metadata.name)
      POD_NAMESPACE:                 orch-iam (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     tenancy-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      tenancy-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-iam/deployments/tenancy-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhjbj (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  wait-for-job:
    Container ID:  containerd://36c1ad72d65b6a540088bff19004ba57bc64b90ccf64574ad7b80173c335732f
    Image:         alpine/kubectl:1.34.1
    Image ID:      docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
    Args:
      set -e
      echo "admin.nexus.com CRD is available, waiting for CR..."
      echo "Waiting for CRD 'orgs.org.edge-orchestrator.intel.com' to be available..."
      until kubectl get orgs.org.edge-orchestrator.intel.com; do
        echo "edge-orchestrator.intel.com CRD not found, waiting..."
        sleep 10
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:42:23 +0000
      Finished:     Tue, 03 Feb 2026 17:42:24 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhjbj (ro)
Containers:
  tenancy-manager:
    Container ID:  containerd://52b90454c8be1e676d5b641633ef04a9c22fd7a71123c344f0136de98b3b76b3
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-manager:26.0.1
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/common/tenancy-manager@sha256:a463565fd38165dadbadb8bb80da7886132351854f137be2d860419b6bd964e5
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/tenancy-manager
      -serviceaccount
    State:          Running
      Started:      Tue, 03 Feb 2026 17:42:28 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      LOG_LEVEL:        error
      NEXUS_LOG_LEVEL:  error
    Mounts:
      /etc/config from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jhjbj (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tenancy-config
    Optional:  false
  kube-api-access-jhjbj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             alerting-monitor-5cb47f87f4-5mdg6
Namespace:        orch-infra
Priority:         0
Service Account:  alerting-monitor
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:17 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor
                  app.kubernetes.io/name=alerting-monitor
                  pod-template-hash=5cb47f87f4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alerting-monitor
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 7a76193299940cd9765cf035784b4444dd9b7591fe22c11f3a3081f594ac6e87
                  cni.projectcalico.org/podIP: 10.42.65.194/32
                  cni.projectcalico.org/podIPs: 10.42.65.194/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: alerting-monitor
                  kubectl.kubernetes.io/default-logs-container: alerting-monitor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.194
IPs:
  IP:           10.42.65.194
Controlled By:  ReplicaSet/alerting-monitor-5cb47f87f4
Init Containers:
  istio-init:
    Container ID:  containerd://f0a85660b4df03badf2e72e7cd8da0f1a6f49401549ac07a66b9d5737d78cea6
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:19 +0000
      Finished:     Tue, 03 Feb 2026 17:47:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rb55k (ro)
  istio-proxy:
    Container ID:  containerd://36b3674ef2c6604d8397df5a6b88b6c272ce68215bc9151063f9a4a0145659fb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-5cb47f87f4-5mdg6 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     alerting-monitor,open-policy-agent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/alerting-monitor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/alerting-monitor/livez":{"httpGet":{"path":"/api/v1/status","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rb55k (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  alerting-monitor:
    Container ID:  containerd://a990cdf0592e3b4560e5d3abc8b51e7bb9b2e7fabc12671ec79c10155b699596
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor@sha256:e4c435f0121a34007b9049917cc59d2d3e3ec55e0832cc9877ebf1e7bfed8cb3
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      -config=/etc/config/config.yaml
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:24 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   128Mi
    Liveness:   http-get http://:15020/app-health/alerting-monitor/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/sh -c curl --silent -X GET http://localhost:8080/api/v1/status | grep -q -e '^{\"state\":\"ready\"}$'] delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PGDATABASE:     <set to the key 'PGDATABASE' in secret 'alerting-local-postgresql'>  Optional: false
      PGHOST:         <set to the key 'PGHOST' in secret 'alerting-local-postgresql'>      Optional: false
      PGPORT:         <set to the key 'PGPORT' in secret 'alerting-local-postgresql'>      Optional: false
      PGPASSWORD:     <set to the key 'PGPASSWORD' in secret 'alerting-local-postgresql'>  Optional: false
      PGUSER:         <set to the key 'PGUSER' in secret 'alerting-local-postgresql'>      Optional: false
      POD_UID:         (v1:metadata.uid)
      FROM_MAIL:      <set to the key 'from' in secret 'smtp'>           Optional: false
      SMART_HOST:     <set to the key 'smartHost' in secret 'smtp'>      Optional: false
      SMART_PORT:     <set to the key 'smartPort' in secret 'smtp'>      Optional: false
      SMTP_USERNAME:  <set to the key 'authUsername' in secret 'smtp'>   Optional: false
      SMTP_PASSWORD:  <set to the key 'password' in secret 'smtp-auth'>  Optional: false
    Mounts:
      /etc/config from config (ro)
      /etc/ssl/certs from destination-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rb55k (ro)
  open-policy-agent:
    Container ID:  containerd://a3fc2a4b531968b92564f93eaf186ece6d3aa3df3f79ca840a18fa753c1a3b59
    Image:         openpolicyagent/opa:1.10.1-static
    Image ID:      docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:          8181/TCP
    Host Port:     0/TCP
    Args:
      run
      --server
      /etc/opa/rego
      --log-level
      info
      --addr
      localhost:8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:24 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        10m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /etc/opa/rego from alerting-monitor-opa-cm (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rb55k (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alert-monitor-config
    Optional:  false
  alerting-monitor-opa-cm:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alerting-monitor-opa-cm
    Optional:  false
  destination-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gateway-ca-cert
    Optional:    false
  kube-api-access-rb55k:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  58m   default-scheduler  Successfully assigned orch-infra/alerting-monitor-5cb47f87f4-5mdg6 to orch-tf
  Normal  Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal  Created    58m   kubelet            Created container: istio-init
  Normal  Started    58m   kubelet            Started container istio-init
  Normal  Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal  Created    58m   kubelet            Created container: istio-proxy
  Normal  Started    58m   kubelet            Started container istio-proxy
  Normal  Pulled     58m   kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4" already present on machine
  Normal  Created    58m   kubelet            Created container: alerting-monitor
  Normal  Started    58m   kubelet            Started container alerting-monitor
  Normal  Pulled     58m   kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal  Created    58m   kubelet            Created container: open-policy-agent
  Normal  Started    58m   kubelet            Started container open-policy-agent


Name:             alerting-monitor-5cb47f87f4-cjslh
Namespace:        orch-infra
Priority:         0
Service Account:  alerting-monitor
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:02 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor
                  app.kubernetes.io/name=alerting-monitor
                  pod-template-hash=5cb47f87f4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alerting-monitor
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: d7fa686a625ce8ffd87aea8659b0d16297b82747b8ecea5a38f2de2e3e2b422e
                  cni.projectcalico.org/podIP: 10.42.65.203/32
                  cni.projectcalico.org/podIPs: 10.42.65.203/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: alerting-monitor
                  kubectl.kubernetes.io/default-logs-container: alerting-monitor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.203
IPs:
  IP:           10.42.65.203
Controlled By:  ReplicaSet/alerting-monitor-5cb47f87f4
Init Containers:
  istio-init:
    Container ID:  containerd://5fe5aa6960de2d77163280e9e6d71763eec1f0e7965c596a336d8d831359f1e3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:04 +0000
      Finished:     Tue, 03 Feb 2026 17:48:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7l6dm (ro)
  istio-proxy:
    Container ID:  containerd://565e81ce1de9d8396a1d698d2d975568696240083c8c2ebe2717858c214df603
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-5cb47f87f4-cjslh (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     alerting-monitor,open-policy-agent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/alerting-monitor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/alerting-monitor/livez":{"httpGet":{"path":"/api/v1/status","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7l6dm (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  alerting-monitor:
    Container ID:  containerd://b03862783b197bacc1b07586ecac060ffd8b487c0761fcb4e5c9a5b1c754d283
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor@sha256:e4c435f0121a34007b9049917cc59d2d3e3ec55e0832cc9877ebf1e7bfed8cb3
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      -config=/etc/config/config.yaml
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   128Mi
    Liveness:   http-get http://:15020/app-health/alerting-monitor/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/sh -c curl --silent -X GET http://localhost:8080/api/v1/status | grep -q -e '^{\"state\":\"ready\"}$'] delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PGDATABASE:     <set to the key 'PGDATABASE' in secret 'alerting-local-postgresql'>  Optional: false
      PGHOST:         <set to the key 'PGHOST' in secret 'alerting-local-postgresql'>      Optional: false
      PGPORT:         <set to the key 'PGPORT' in secret 'alerting-local-postgresql'>      Optional: false
      PGPASSWORD:     <set to the key 'PGPASSWORD' in secret 'alerting-local-postgresql'>  Optional: false
      PGUSER:         <set to the key 'PGUSER' in secret 'alerting-local-postgresql'>      Optional: false
      POD_UID:         (v1:metadata.uid)
      FROM_MAIL:      <set to the key 'from' in secret 'smtp'>           Optional: false
      SMART_HOST:     <set to the key 'smartHost' in secret 'smtp'>      Optional: false
      SMART_PORT:     <set to the key 'smartPort' in secret 'smtp'>      Optional: false
      SMTP_USERNAME:  <set to the key 'authUsername' in secret 'smtp'>   Optional: false
      SMTP_PASSWORD:  <set to the key 'password' in secret 'smtp-auth'>  Optional: false
    Mounts:
      /etc/config from config (ro)
      /etc/ssl/certs from destination-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7l6dm (ro)
  open-policy-agent:
    Container ID:  containerd://1f4687810fc35be84989d68e88bdf99cbde6cced0d202fdf92906eecdaac81a5
    Image:         openpolicyagent/opa:1.10.1-static
    Image ID:      docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:          8181/TCP
    Host Port:     0/TCP
    Args:
      run
      --server
      /etc/opa/rego
      --log-level
      info
      --addr
      localhost:8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        10m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /etc/opa/rego from alerting-monitor-opa-cm (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7l6dm (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alert-monitor-config
    Optional:  false
  alerting-monitor-opa-cm:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alerting-monitor-opa-cm
    Optional:  false
  destination-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gateway-ca-cert
    Optional:    false
  kube-api-access-7l6dm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  58m   default-scheduler  Successfully assigned orch-infra/alerting-monitor-5cb47f87f4-cjslh to orch-tf
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-init
  Normal   Started    58m   kubelet            Started container istio-init
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-proxy
  Normal   Started    58m   kubelet            Started container istio-proxy
  Warning  Unhealthy  58m   kubelet            Startup probe failed: Get "http://10.42.65.203:15021/healthz/ready": dial tcp 10.42.65.203:15021: connect: connection refused
  Normal   Pulled     58m   kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4" already present on machine
  Normal   Created    58m   kubelet            Created container: alerting-monitor
  Normal   Started    58m   kubelet            Started container alerting-monitor
  Normal   Pulled     58m   kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    58m   kubelet            Created container: open-policy-agent
  Normal   Started    58m   kubelet            Started container open-policy-agent


Name:             alerting-monitor-5cb47f87f4-mftgs
Namespace:        orch-infra
Priority:         0
Service Account:  alerting-monitor
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:02 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor
                  app.kubernetes.io/name=alerting-monitor
                  pod-template-hash=5cb47f87f4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alerting-monitor
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 1f1b332e4d785226beea35beb1c7a3f85c20a3e9ebcaf3cb46e7e24c866d2868
                  cni.projectcalico.org/podIP: 10.42.65.204/32
                  cni.projectcalico.org/podIPs: 10.42.65.204/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: alerting-monitor
                  kubectl.kubernetes.io/default-logs-container: alerting-monitor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.204
IPs:
  IP:           10.42.65.204
Controlled By:  ReplicaSet/alerting-monitor-5cb47f87f4
Init Containers:
  istio-init:
    Container ID:  containerd://67b444e33c10f9566bf76fa627ae2489e613094bcead99c86b59b796f739feb6
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:04 +0000
      Finished:     Tue, 03 Feb 2026 17:48:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gm8f2 (ro)
  istio-proxy:
    Container ID:  containerd://d2eddd8d6dd2ab60a5c5de693eeebecbeb85fde89683ac33021128093b9e4bf2
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-5cb47f87f4-mftgs (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     alerting-monitor,open-policy-agent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/alerting-monitor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/alerting-monitor/livez":{"httpGet":{"path":"/api/v1/status","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gm8f2 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  alerting-monitor:
    Container ID:  containerd://b2148d6e0102c9b7063823d950798ce251a94577411e4e48be865a7f8fb365b6
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor@sha256:e4c435f0121a34007b9049917cc59d2d3e3ec55e0832cc9877ebf1e7bfed8cb3
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      -config=/etc/config/config.yaml
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   128Mi
    Liveness:   http-get http://:15020/app-health/alerting-monitor/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/sh -c curl --silent -X GET http://localhost:8080/api/v1/status | grep -q -e '^{\"state\":\"ready\"}$'] delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PGDATABASE:     <set to the key 'PGDATABASE' in secret 'alerting-local-postgresql'>  Optional: false
      PGHOST:         <set to the key 'PGHOST' in secret 'alerting-local-postgresql'>      Optional: false
      PGPORT:         <set to the key 'PGPORT' in secret 'alerting-local-postgresql'>      Optional: false
      PGPASSWORD:     <set to the key 'PGPASSWORD' in secret 'alerting-local-postgresql'>  Optional: false
      PGUSER:         <set to the key 'PGUSER' in secret 'alerting-local-postgresql'>      Optional: false
      POD_UID:         (v1:metadata.uid)
      FROM_MAIL:      <set to the key 'from' in secret 'smtp'>           Optional: false
      SMART_HOST:     <set to the key 'smartHost' in secret 'smtp'>      Optional: false
      SMART_PORT:     <set to the key 'smartPort' in secret 'smtp'>      Optional: false
      SMTP_USERNAME:  <set to the key 'authUsername' in secret 'smtp'>   Optional: false
      SMTP_PASSWORD:  <set to the key 'password' in secret 'smtp-auth'>  Optional: false
    Mounts:
      /etc/config from config (ro)
      /etc/ssl/certs from destination-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gm8f2 (ro)
  open-policy-agent:
    Container ID:  containerd://6fee5fd588a1d2755ea8942798dc0dce053dd202425868b1ceb15092e992cfad
    Image:         openpolicyagent/opa:1.10.1-static
    Image ID:      docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:          8181/TCP
    Host Port:     0/TCP
    Args:
      run
      --server
      /etc/opa/rego
      --log-level
      info
      --addr
      localhost:8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        10m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /etc/opa/rego from alerting-monitor-opa-cm (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gm8f2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alert-monitor-config
    Optional:  false
  alerting-monitor-opa-cm:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alerting-monitor-opa-cm
    Optional:  false
  destination-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gateway-ca-cert
    Optional:    false
  kube-api-access-gm8f2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  58m   default-scheduler  Successfully assigned orch-infra/alerting-monitor-5cb47f87f4-mftgs to orch-tf
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-init
  Normal   Started    58m   kubelet            Started container istio-init
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-proxy
  Normal   Started    58m   kubelet            Started container istio-proxy
  Warning  Unhealthy  58m   kubelet            Startup probe failed: Get "http://10.42.65.204:15021/healthz/ready": dial tcp 10.42.65.204:15021: connect: connection refused
  Normal   Pulled     58m   kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4" already present on machine
  Normal   Created    58m   kubelet            Created container: alerting-monitor
  Normal   Started    58m   kubelet            Started container alerting-monitor
  Normal   Pulled     58m   kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    58m   kubelet            Created container: open-policy-agent
  Normal   Started    58m   kubelet            Started container open-policy-agent
  Warning  Unhealthy  52m   kubelet            Liveness probe failed: Get "http://10.42.65.204:15020/app-health/alerting-monitor/livez": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             alerting-monitor-5cb47f87f4-vvsfc
Namespace:        orch-infra
Priority:         0
Service Account:  alerting-monitor
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:02 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor
                  app.kubernetes.io/name=alerting-monitor
                  pod-template-hash=5cb47f87f4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alerting-monitor
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 7d47ce6949063e2e2017bc88b876b6f73b7fe4bea15900516e2a67f4eab57cac
                  cni.projectcalico.org/podIP: 10.42.65.122/32
                  cni.projectcalico.org/podIPs: 10.42.65.122/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: alerting-monitor
                  kubectl.kubernetes.io/default-logs-container: alerting-monitor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.122
IPs:
  IP:           10.42.65.122
Controlled By:  ReplicaSet/alerting-monitor-5cb47f87f4
Init Containers:
  istio-init:
    Container ID:  containerd://e6a8f47e877926c72c3bb95e276d1a623ea8c6c5fe6006e36b2afd683eae1624
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:04 +0000
      Finished:     Tue, 03 Feb 2026 17:47:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d6kbt (ro)
  istio-proxy:
    Container ID:  containerd://d6c89074c85ef1c89353b26c93e96141b17e89e3904b67c27e6e1bee5d9dc167
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-5cb47f87f4-vvsfc (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     alerting-monitor,open-policy-agent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/alerting-monitor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/alerting-monitor/livez":{"httpGet":{"path":"/api/v1/status","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d6kbt (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  alerting-monitor:
    Container ID:  containerd://5d8b2fefa63f05fa5e506b62ff021b521c4403f1fddf8aa570e9f96dc3475e13
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor@sha256:e4c435f0121a34007b9049917cc59d2d3e3ec55e0832cc9877ebf1e7bfed8cb3
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      -config=/etc/config/config.yaml
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:13 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   128Mi
    Liveness:   http-get http://:15020/app-health/alerting-monitor/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/sh -c curl --silent -X GET http://localhost:8080/api/v1/status | grep -q -e '^{\"state\":\"ready\"}$'] delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PGDATABASE:     <set to the key 'PGDATABASE' in secret 'alerting-local-postgresql'>  Optional: false
      PGHOST:         <set to the key 'PGHOST' in secret 'alerting-local-postgresql'>      Optional: false
      PGPORT:         <set to the key 'PGPORT' in secret 'alerting-local-postgresql'>      Optional: false
      PGPASSWORD:     <set to the key 'PGPASSWORD' in secret 'alerting-local-postgresql'>  Optional: false
      PGUSER:         <set to the key 'PGUSER' in secret 'alerting-local-postgresql'>      Optional: false
      POD_UID:         (v1:metadata.uid)
      FROM_MAIL:      <set to the key 'from' in secret 'smtp'>           Optional: false
      SMART_HOST:     <set to the key 'smartHost' in secret 'smtp'>      Optional: false
      SMART_PORT:     <set to the key 'smartPort' in secret 'smtp'>      Optional: false
      SMTP_USERNAME:  <set to the key 'authUsername' in secret 'smtp'>   Optional: false
      SMTP_PASSWORD:  <set to the key 'password' in secret 'smtp-auth'>  Optional: false
    Mounts:
      /etc/config from config (ro)
      /etc/ssl/certs from destination-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d6kbt (ro)
  open-policy-agent:
    Container ID:  containerd://75d5f5d3882f9777ab2c37f787b20a78717e7f7872821c3620391b59ab7f9781
    Image:         openpolicyagent/opa:1.10.1-static
    Image ID:      docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:          8181/TCP
    Host Port:     0/TCP
    Args:
      run
      --server
      /etc/opa/rego
      --log-level
      info
      --addr
      localhost:8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        10m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /etc/opa/rego from alerting-monitor-opa-cm (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d6kbt (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alert-monitor-config
    Optional:  false
  alerting-monitor-opa-cm:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alerting-monitor-opa-cm
    Optional:  false
  destination-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gateway-ca-cert
    Optional:    false
  kube-api-access-d6kbt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  59m   default-scheduler  Successfully assigned orch-infra/alerting-monitor-5cb47f87f4-vvsfc to orch-tf
  Normal   Pulled     59m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    59m   kubelet            Created container: istio-init
  Normal   Started    59m   kubelet            Started container istio-init
  Normal   Pulled     59m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    59m   kubelet            Created container: istio-proxy
  Normal   Started    59m   kubelet            Started container istio-proxy
  Warning  Unhealthy  59m   kubelet            Startup probe failed: Get "http://10.42.65.122:15021/healthz/ready": dial tcp 10.42.65.122:15021: connect: connection refused
  Normal   Pulling    59m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4"
  Normal   Pulled     59m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4" in 3.562s (3.562s including waiting). Image size: 24093555 bytes.
  Normal   Created    59m   kubelet            Created container: alerting-monitor
  Normal   Started    59m   kubelet            Started container alerting-monitor
  Normal   Pulled     59m   kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    58m   kubelet            Created container: open-policy-agent
  Normal   Started    58m   kubelet            Started container open-policy-agent
  Warning  Unhealthy  58m   kubelet            Readiness probe failed:


Name:             alerting-monitor-5cb47f87f4-zsfkc
Namespace:        orch-infra
Priority:         0
Service Account:  alerting-monitor
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:02 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor
                  app.kubernetes.io/name=alerting-monitor
                  pod-template-hash=5cb47f87f4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alerting-monitor
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 4f6fded6fb7d4384b76295cff9bf0408ef0a2507940152e2d2d671daf55bdbd0
                  cni.projectcalico.org/podIP: 10.42.65.202/32
                  cni.projectcalico.org/podIPs: 10.42.65.202/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: alerting-monitor
                  kubectl.kubernetes.io/default-logs-container: alerting-monitor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.202
IPs:
  IP:           10.42.65.202
Controlled By:  ReplicaSet/alerting-monitor-5cb47f87f4
Init Containers:
  istio-init:
    Container ID:  containerd://4bcfeb0e09d46bc964cf18e20161ff47c54d9f36a4d36c222ac8979477c8a9ff
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:04 +0000
      Finished:     Tue, 03 Feb 2026 17:48:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ff9pp (ro)
  istio-proxy:
    Container ID:  containerd://ee271edd48a96ff06d1f9941c88a16513d3a876a91a31fceece2e9874f77884d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-5cb47f87f4-zsfkc (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":8181,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     alerting-monitor,open-policy-agent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/alerting-monitor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/alerting-monitor/livez":{"httpGet":{"path":"/api/v1/status","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ff9pp (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  alerting-monitor:
    Container ID:  containerd://c2502f6607863a06614bbda277d5828b94e9c9dcd858318614cbd7770a62d9f9
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor@sha256:e4c435f0121a34007b9049917cc59d2d3e3ec55e0832cc9877ebf1e7bfed8cb3
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      -config=/etc/config/config.yaml
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   128Mi
    Liveness:   http-get http://:15020/app-health/alerting-monitor/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/sh -c curl --silent -X GET http://localhost:8080/api/v1/status | grep -q -e '^{\"state\":\"ready\"}$'] delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PGDATABASE:     <set to the key 'PGDATABASE' in secret 'alerting-local-postgresql'>  Optional: false
      PGHOST:         <set to the key 'PGHOST' in secret 'alerting-local-postgresql'>      Optional: false
      PGPORT:         <set to the key 'PGPORT' in secret 'alerting-local-postgresql'>      Optional: false
      PGPASSWORD:     <set to the key 'PGPASSWORD' in secret 'alerting-local-postgresql'>  Optional: false
      PGUSER:         <set to the key 'PGUSER' in secret 'alerting-local-postgresql'>      Optional: false
      POD_UID:         (v1:metadata.uid)
      FROM_MAIL:      <set to the key 'from' in secret 'smtp'>           Optional: false
      SMART_HOST:     <set to the key 'smartHost' in secret 'smtp'>      Optional: false
      SMART_PORT:     <set to the key 'smartPort' in secret 'smtp'>      Optional: false
      SMTP_USERNAME:  <set to the key 'authUsername' in secret 'smtp'>   Optional: false
      SMTP_PASSWORD:  <set to the key 'password' in secret 'smtp-auth'>  Optional: false
    Mounts:
      /etc/config from config (ro)
      /etc/ssl/certs from destination-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ff9pp (ro)
  open-policy-agent:
    Container ID:  containerd://dcf03cbf902fe0c1689fd38fbdf3957c3ce6409a83f62920d52b804e88f8fe7d
    Image:         openpolicyagent/opa:1.10.1-static
    Image ID:      docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:          8181/TCP
    Host Port:     0/TCP
    Args:
      run
      --server
      /etc/opa/rego
      --log-level
      info
      --addr
      localhost:8181
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        10m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /etc/opa/rego from alerting-monitor-opa-cm (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ff9pp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alert-monitor-config
    Optional:  false
  alerting-monitor-opa-cm:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alerting-monitor-opa-cm
    Optional:  false
  destination-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gateway-ca-cert
    Optional:    false
  kube-api-access-ff9pp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  58m                default-scheduler  Successfully assigned orch-infra/alerting-monitor-5cb47f87f4-zsfkc to orch-tf
  Normal   Pulled     58m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m                kubelet            Created container: istio-init
  Normal   Started    58m                kubelet            Started container istio-init
  Normal   Pulled     58m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m                kubelet            Created container: istio-proxy
  Normal   Started    58m                kubelet            Started container istio-proxy
  Warning  Unhealthy  58m (x3 over 58m)  kubelet            Startup probe failed: Get "http://10.42.65.202:15021/healthz/ready": dial tcp 10.42.65.202:15021: connect: connection refused
  Normal   Pulled     58m                kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor:1.7.4" already present on machine
  Normal   Created    58m                kubelet            Created container: alerting-monitor
  Normal   Started    58m                kubelet            Started container alerting-monitor
  Normal   Pulled     58m                kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    58m                kubelet            Created container: open-policy-agent
  Normal   Started    58m                kubelet            Started container open-policy-agent
  Warning  Unhealthy  52m                kubelet            Liveness probe failed: Get "http://10.42.65.202:15020/app-health/alerting-monitor/livez": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             alerting-monitor-alertmanager-0
Namespace:        orch-infra
Priority:         0
Service Account:  alerting-monitor-alertmanager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:09 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor
                  app.kubernetes.io/name=alertmanager
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=alerting-monitor-alertmanager-5f76b4c4d4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alertmanager
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=alerting-monitor-alertmanager-0
Annotations:      cni.projectcalico.org/containerID: 657e2ab5367020579abac8ee9db78ae0a409f87b9b1fec82e26a3095aa044e28
                  cni.projectcalico.org/podIP: 10.42.65.193/32
                  cni.projectcalico.org/podIPs: 10.42.65.193/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: alertmanager-configmap-reload
                  kubectl.kubernetes.io/default-logs-container: alertmanager-configmap-reload
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.193
IPs:
  IP:           10.42.65.193
Controlled By:  StatefulSet/alerting-monitor-alertmanager
Init Containers:
  istio-init:
    Container ID:  containerd://5d28c3ab700e3918326938956f3b5d59e8fece000f370737c2d49b667a217539
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:11 +0000
      Finished:     Tue, 03 Feb 2026 17:47:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hhn8n (ro)
  istio-proxy:
    Container ID:  containerd://f978b552058e6ccd3e83410e9ebd9ac049cf771349e264baa0302dd12d841e79
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-alertmanager-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":9093,"protocol":"TCP"}
                                         ,{"name":"clusterpeer-tcp","containerPort":9094,"protocol":"TCP"}
                                         ,{"name":"clusterpeer-udp","containerPort":9094,"protocol":"UDP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     alertmanager-configmap-reload,alertmanager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor-alertmanager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/alerting-monitor-alertmanager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/alertmanager/livez":{"httpGet":{"path":"/","port":9093,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/alertmanager/readyz":{"httpGet":{"path":"/","port":9093,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hhn8n (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  alertmanager-configmap-reload:
    Container ID:  containerd://3055426d31803f3a409ab0faa122d4ab8d91f8aae04df189ae09582bda122bbd
    Image:         quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
    Image ID:      quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
    Port:          <none>
    Host Port:     <none>
    Args:
      --watched-dir=/etc/alertmanager
      --reload-url=http://127.0.0.1:9093/-/reload
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/alertmanager from config (rw)
      /etc/alertmanager/ from alertmanager-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hhn8n (ro)
  alertmanager:
    Container ID:  containerd://efcafd29ae13906bb5e7ef0aa35897aaf7a30d79f23fb66ef9a57647e656453f
    Image:         quay.io/prometheus/alertmanager:v0.29.0
    Image ID:      quay.io/prometheus/alertmanager@sha256:88743b63b3e09ea6e31e140ced5bf45f4a8e82c617c2a963f78841f4995ad1d7
    Ports:         9093/TCP, 9094/TCP, 9094/UDP
    Host Ports:    0/TCP, 0/TCP, 0/UDP
    Args:
      --storage.path=/alertmanager
      --cluster.advertise-address=[$(POD_IP)]:9094
      --cluster.listen-address=0.0.0.0:9094
      --cluster.peer=alerting-monitor-alertmanager-0.alerting-monitor-alertmanager-headless:9094
      --cluster.peer=alerting-monitor-alertmanager-1.alerting-monitor-alertmanager-headless:9094
      --config.file=/etc/alertmanager/custom.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:18 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/alertmanager/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/alertmanager/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /alertmanager from storage (rw)
      /etc/alertmanager from config (rw)
      /etc/alertmanager/ from alertmanager-config (ro)
      /etc/alertmanager/templates from alertmanager-email-template (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hhn8n (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-alerting-monitor-alertmanager-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alerting-monitor-alertmanager
    Optional:  false
  alertmanager-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alert-monitor-config
    Optional:    false
  alertmanager-email-template:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alertmanager-email-template
    Optional:  false
  kube-api-access-hhn8n:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  59m   default-scheduler  Successfully assigned orch-infra/alerting-monitor-alertmanager-0 to orch-tf
  Normal  Pulled     59m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal  Created    59m   kubelet            Created container: istio-init
  Normal  Started    59m   kubelet            Started container istio-init
  Normal  Pulled     59m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal  Created    59m   kubelet            Created container: istio-proxy
  Normal  Started    59m   kubelet            Started container istio-proxy
  Normal  Pulled     58m   kubelet            Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2" already present on machine
  Normal  Created    58m   kubelet            Created container: alertmanager-configmap-reload
  Normal  Started    58m   kubelet            Started container alertmanager-configmap-reload
  Normal  Pulling    58m   kubelet            Pulling image "quay.io/prometheus/alertmanager:v0.29.0"
  Normal  Pulled     58m   kubelet            Successfully pulled image "quay.io/prometheus/alertmanager:v0.29.0" in 2.26s (2.26s including waiting). Image size: 35538534 bytes.
  Normal  Created    58m   kubelet            Created container: alertmanager
  Normal  Started    58m   kubelet            Started container alertmanager


Name:             alerting-monitor-alertmanager-1
Namespace:        orch-infra
Priority:         0
Service Account:  alerting-monitor-alertmanager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:23 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor
                  app.kubernetes.io/name=alertmanager
                  apps.kubernetes.io/pod-index=1
                  controller-revision-hash=alerting-monitor-alertmanager-5f76b4c4d4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alertmanager
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=alerting-monitor-alertmanager-1
Annotations:      cni.projectcalico.org/containerID: 834257bd8db914c45d20d7eb8f47601cf38e2e432a9a1abd1b6ac3e3fdd27b14
                  cni.projectcalico.org/podIP: 10.42.65.196/32
                  cni.projectcalico.org/podIPs: 10.42.65.196/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: alertmanager-configmap-reload
                  kubectl.kubernetes.io/default-logs-container: alertmanager-configmap-reload
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.196
IPs:
  IP:           10.42.65.196
Controlled By:  StatefulSet/alerting-monitor-alertmanager
Init Containers:
  istio-init:
    Container ID:  containerd://17f5cfffb0573b3b4e988ef3d4df0c70b316030b50eb1a25cd68a3d0bd2fd41b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:25 +0000
      Finished:     Tue, 03 Feb 2026 17:47:25 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8q8jg (ro)
  istio-proxy:
    Container ID:  containerd://e558b00973dea66d44a05508105259d583560e65f88a4d05dc62592cb2ad0205
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-alertmanager-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":9093,"protocol":"TCP"}
                                         ,{"name":"clusterpeer-tcp","containerPort":9094,"protocol":"TCP"}
                                         ,{"name":"clusterpeer-udp","containerPort":9094,"protocol":"UDP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     alertmanager-configmap-reload,alertmanager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor-alertmanager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/alerting-monitor-alertmanager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/alertmanager/livez":{"httpGet":{"path":"/","port":9093,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/alertmanager/readyz":{"httpGet":{"path":"/","port":9093,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8q8jg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  alertmanager-configmap-reload:
    Container ID:  containerd://929b6db23c39242c1497c9f0b816ea2d455d2c3d3f5443326c408233c6db1b2d
    Image:         quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
    Image ID:      quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
    Port:          <none>
    Host Port:     <none>
    Args:
      --watched-dir=/etc/alertmanager
      --reload-url=http://127.0.0.1:9093/-/reload
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/alertmanager from config (rw)
      /etc/alertmanager/ from alertmanager-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8q8jg (ro)
  alertmanager:
    Container ID:  containerd://74013e1357ad3f32f9bc2524ddf33c46248df7d5f140db12ac26be48a684d0f8
    Image:         quay.io/prometheus/alertmanager:v0.29.0
    Image ID:      quay.io/prometheus/alertmanager@sha256:88743b63b3e09ea6e31e140ced5bf45f4a8e82c617c2a963f78841f4995ad1d7
    Ports:         9093/TCP, 9094/TCP, 9094/UDP
    Host Ports:    0/TCP, 0/TCP, 0/UDP
    Args:
      --storage.path=/alertmanager
      --cluster.advertise-address=[$(POD_IP)]:9094
      --cluster.listen-address=0.0.0.0:9094
      --cluster.peer=alerting-monitor-alertmanager-0.alerting-monitor-alertmanager-headless:9094
      --cluster.peer=alerting-monitor-alertmanager-1.alerting-monitor-alertmanager-headless:9094
      --config.file=/etc/alertmanager/custom.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/alertmanager/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/alertmanager/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /alertmanager from storage (rw)
      /etc/alertmanager from config (rw)
      /etc/alertmanager/ from alertmanager-config (ro)
      /etc/alertmanager/templates from alertmanager-email-template (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8q8jg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-alerting-monitor-alertmanager-1
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alerting-monitor-alertmanager
    Optional:  false
  alertmanager-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alert-monitor-config
    Optional:    false
  alertmanager-email-template:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alertmanager-email-template
    Optional:  false
  kube-api-access-8q8jg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  58m   default-scheduler  Successfully assigned orch-infra/alerting-monitor-alertmanager-1 to orch-tf
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-init
  Normal   Started    58m   kubelet            Started container istio-init
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-proxy
  Normal   Started    58m   kubelet            Started container istio-proxy
  Warning  Unhealthy  58m   kubelet            Startup probe failed: Get "http://10.42.65.196:15021/healthz/ready": dial tcp 10.42.65.196:15021: connect: connection refused
  Normal   Pulled     58m   kubelet            Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2" already present on machine
  Normal   Created    58m   kubelet            Created container: alertmanager-configmap-reload
  Normal   Started    58m   kubelet            Started container alertmanager-configmap-reload
  Normal   Pulled     58m   kubelet            Container image "quay.io/prometheus/alertmanager:v0.29.0" already present on machine
  Normal   Created    58m   kubelet            Created container: alertmanager
  Normal   Started    58m   kubelet            Started container alertmanager


Name:             alerting-monitor-management-7c5f79d5f9-2dfwv
Namespace:        orch-infra
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:02 +0000
Labels:           app.kubernetes.io/instance=alerting-monitor-management
                  app.kubernetes.io/name=alerting-monitor-management
                  pod-template-hash=7c5f79d5f9
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=alerting-monitor-management
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 8c8f8d13f8225a13837b45dac564ebcc50da83ff2aba963c8aaeb5cd3a7e9684
                  cni.projectcalico.org/podIP: 10.42.65.141/32
                  cni.projectcalico.org/podIPs: 10.42.65.141/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: management
                  kubectl.kubernetes.io/default-logs-container: management
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.141
IPs:
  IP:           10.42.65.141
Controlled By:  ReplicaSet/alerting-monitor-management-7c5f79d5f9
Init Containers:
  istio-init:
    Container ID:  containerd://f2f79f7e21ab77c6173a07b206bf22a6a43b3923bb91915203d0704c071bd9d9
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:04 +0000
      Finished:     Tue, 03 Feb 2026 17:47:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hsj26 (ro)
  istio-proxy:
    Container ID:  containerd://f15d8b4c160443ad109244dce5b5a00f74415a94c658ea4abc8927085cfd2c69
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:05 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      alerting-monitor-management-7c5f79d5f9-2dfwv (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":51001,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     management
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      alerting-monitor-management
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/alerting-monitor-management
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/management/livez":{"grpc":{"port":51001,"service":""},"timeoutSeconds":1},"/app-health/management/readyz":{"grpc":{"port":51001,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hsj26 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  management:
    Container ID:   containerd://76ab73a358249c3588f69bb0ec379d21d6318cbf89f32d85240b1e3ee2ca8bd7
    Image:          registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor-management:1.7.4
    Image ID:       registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor-management@sha256:aa5dfd9cd32c209a16df4778634b0d699564f88ae2b7b3875ce8c0d9b8d3e94c
    Port:           51001/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:13 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   128Mi
    Liveness:   http-get http://:15020/app-health/management/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/management/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PGDATABASE:  <set to the key 'PGDATABASE' in secret 'alerting-local-postgresql'>  Optional: false
      PGHOST:      <set to the key 'PGHOST' in secret 'alerting-local-postgresql'>      Optional: false
      PGPORT:      <set to the key 'PGPORT' in secret 'alerting-local-postgresql'>      Optional: false
      PGPASSWORD:  <set to the key 'PGPASSWORD' in secret 'alerting-local-postgresql'>  Optional: false
      PGUSER:      <set to the key 'PGUSER' in secret 'alerting-local-postgresql'>      Optional: false
      FROM_MAIL:   <set to the key 'from' in secret 'smtp'>                             Optional: false
      SMART_HOST:  <set to the key 'smartHost' in secret 'smtp'>                        Optional: false
      SMART_PORT:  <set to the key 'smartPort' in secret 'smtp'>                        Optional: false
    Mounts:
      /config/rules.yaml from rules-volume (rw,path="rules.yaml")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hsj26 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  rules-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      default-rules
    Optional:  false
  kube-api-access-hsj26:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  59m                default-scheduler  Successfully assigned orch-infra/alerting-monitor-management-7c5f79d5f9-2dfwv to orch-tf
  Normal   Pulled     59m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    59m                kubelet            Created container: istio-init
  Normal   Started    59m                kubelet            Started container istio-init
  Normal   Pulled     59m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    59m                kubelet            Created container: istio-proxy
  Normal   Started    59m                kubelet            Started container istio-proxy
  Warning  Unhealthy  59m (x2 over 59m)  kubelet            Startup probe failed: Get "http://10.42.65.141:15021/healthz/ready": dial tcp 10.42.65.141:15021: connect: connection refused
  Normal   Pulling    59m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor-management:1.7.4"
  Normal   Pulled     59m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/alerting-monitor-management:1.7.4" in 3.729s (3.729s including waiting). Image size: 11622484 bytes.
  Normal   Created    59m                kubelet            Created container: management
  Normal   Started    59m                kubelet            Started container management


Name:             amt-dbpassword-secret-job-5zxzt
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:58 +0000
Labels:           batch.kubernetes.io/controller-uid=bc5aa84e-41ac-4d91-84b9-f8ffbfe68487
                  batch.kubernetes.io/job-name=amt-dbpassword-secret-job
                  controller-uid=bc5aa84e-41ac-4d91-84b9-f8ffbfe68487
                  job-name=amt-dbpassword-secret-job
Annotations:      cni.projectcalico.org/containerID: 0a3b7b2ca84ee95e4d476068e2f4cc57bae880216fb3fedf5b100d07823ab00b
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.243
IPs:
  IP:           10.42.65.243
Controlled By:  Job/amt-dbpassword-secret-job
Containers:
  db-password-secret:
    Container ID:  containerd://a378127cd98bc9968c05d2c0695e8f28fe9d38182be9e78a27fc3b598864dc23
    Image:         alpine/kubectl:1.34.1
    Image ID:      docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      cp /scripts/db-script.sh /test/db-script.sh && chmod +x /test/db-script.sh && /test/db-script.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:12 +0000
      Finished:     Tue, 03 Feb 2026 17:54:33 +0000
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /scripts from cm-vol (rw)
      /test from exec-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4mb4r (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  cm-vol:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      mps-db-configmap
    Optional:  false
  exec-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-4mb4r:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  52m   default-scheduler  Successfully assigned orch-infra/amt-dbpassword-secret-job-5zxzt to orch-tf
  Normal  Pulled     52m   kubelet            Container image "alpine/kubectl:1.34.1" already present on machine
  Normal  Created    52m   kubelet            Created container: db-password-secret
  Normal  Started    52m   kubelet            Started container db-password-secret


Name:             amt-vault-job-29502360-kw9nc
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 18:00:00 +0000
Labels:           app=mps
                  app.kubernetes.io/instance=infra-external
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mps
                  app.kubernetes.io/version=0.0.17
                  batch.kubernetes.io/controller-uid=2d1347b7-9021-4dc2-8367-5aa2227d944b
                  batch.kubernetes.io/job-name=amt-vault-job-29502360
                  controller-uid=2d1347b7-9021-4dc2-8367-5aa2227d944b
                  helm.sh/chart=mps-0.0.29
                  job-name=amt-vault-job-29502360
Annotations:      cni.projectcalico.org/containerID: 7889b7f9812262cee48094379893e3426b89af119f15b6173d449bacafc59dfd
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  traffic.sidecar.istio.io/excludeInboundPorts: 4433
                  traffic.sidecar.istio.io/excludeOutboundPorts: 4433
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.192
IPs:
  IP:           10.42.65.192
Controlled By:  Job/amt-vault-job-29502360
Containers:
  amt:
    Container ID:  containerd://3385fda8cb70961a84216e1e9c5bf61a38e373f40287419fdf015037ec0b4dd2
    Image:         badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Image ID:      docker.io/badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      /amt.sh
      http://platform-keycloak.orch-platform:8080
      http://vault.orch-platform.svc:8200
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:00:02 +0000
      Finished:     Tue, 03 Feb 2026 18:00:02 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      ADMIN_USER:    admin
      ADMIN_PASS:    <set to the key 'admin-password' in secret 'platform-keycloak'>  Optional: false
      ADMIN_CLIENT:  system-client
    Mounts:
      /amt.sh from amt-vol (rw,path="amt.sh")
      /tmp from tmp-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nwwmm (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  amt-vol:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      amt-vault-script
    Optional:  false
  tmp-vol:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-nwwmm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  46m   default-scheduler  Successfully assigned orch-infra/amt-vault-job-29502360-kw9nc to orch-tf
  Normal  Pulled     46m   kubelet            Container image "badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b" already present on machine
  Normal  Created    46m   kubelet            Created container: amt
  Normal  Started    46m   kubelet            Started container amt


Name:             api-5946f44bb6-925nj
Namespace:        orch-infra
Priority:         0
Service Account:  api
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:18 +0000
Labels:           app.kubernetes.io/instance=infra-core
                  app.kubernetes.io/name=api
                  pod-template-hash=5946f44bb6
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=api
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: d89e364758a7371ed6ce6e9071430bce3f42c1286f4c9bba939f27e043540bb5
                  cni.projectcalico.org/podIP: 10.42.65.211/32
                  cni.projectcalico.org/podIPs: 10.42.65.211/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: api
                  kubectl.kubernetes.io/default-logs-container: api
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.211
IPs:
  IP:           10.42.65.211
Controlled By:  ReplicaSet/api-5946f44bb6
Init Containers:
  istio-init:
    Container ID:  containerd://6b9e8ed9c2b33982c34ef51f55b0ef4f59eebb6c7bca06da45695f8ffeb0a8cb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:25 +0000
      Finished:     Tue, 03 Feb 2026 17:48:25 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gl6l2 (ro)
  istio-proxy:
    Container ID:  containerd://b9c93c62ba16111e2d6ea35ea8fd4215b745564ec742f1d3a8eb6f7149afc21b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:30 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      api-5946f44bb6-925nj (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     api
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      api
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/api
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/api/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/api/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gl6l2 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  api:
    Container ID:  containerd://00a312c5b07d804996b8ffa129d9e8c8821cc0288e0a0b0ec4ea7a6277838262
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/api:1.36.0
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/api@sha256:080fa9e2b3fce38a76d3b29278a8c8b2e5372767bb77e9fd414359f1c00bdb08
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      -allowedCorsOrigins=http://localhost:3000
      -baseRESTURL=/edge-infra.orchestrator.apis/v1
      -echoDebug=false
      -enableAuditing=true
      -enableAuth=true
      -enableTracing=false
      -globalLogLevel=info
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress=0.0.0.0:2379
      -serverAddress=0.0.0.0:8080
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:49:02 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:48:44 +0000
      Finished:     Tue, 03 Feb 2026 17:48:46 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/api/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/api/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                <set to the key 'oidc_server_url' of config map 'keycloak-api'>                      Optional: false
      OIDC_TLS_INSECURE_SKIP_VERIFY:  <set to the key 'oidc_tls_insecure_skip_verify_value' of config map 'keycloak-api'>  Optional: false
      ALLOW_MISSING_AUTH_CLIENTS:     
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gl6l2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-gl6l2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57m                default-scheduler  Successfully assigned orch-infra/api-5946f44bb6-925nj to orch-tf
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-init
  Normal   Started    57m                kubelet            Started container istio-init
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-proxy
  Normal   Started    57m                kubelet            Started container istio-proxy
  Warning  Unhealthy  57m (x2 over 57m)  kubelet            Startup probe failed: Get "http://10.42.65.211:15021/healthz/ready": dial tcp 10.42.65.211:15021: connect: connection refused
  Warning  Unhealthy  57m                kubelet            Startup probe failed: HTTP probe failed with statuscode: 503
  Normal   Pulling    57m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/api:1.36.0"
  Normal   Pulled     57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/api:1.36.0" in 5.446s (5.446s including waiting). Image size: 16364353 bytes.
  Warning  BackOff    57m (x2 over 57m)  kubelet            Back-off restarting failed container api in pod api-5946f44bb6-925nj_orch-infra(5196e28f-131f-48d8-8a2c-0e145b024376)
  Normal   Created    57m (x3 over 57m)  kubelet            Created container: api
  Normal   Pulled     57m (x2 over 57m)  kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/api:1.36.0" already present on machine
  Normal   Started    57m (x3 over 57m)  kubelet            Started container api


Name:             apiv2-595d8cb5c-lzwzx
Namespace:        orch-infra
Priority:         0
Service Account:  apiv2
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:18 +0000
Labels:           app.kubernetes.io/instance=infra-core
                  app.kubernetes.io/name=apiv2
                  pod-template-hash=595d8cb5c
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=apiv2
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 8937675529fb40b509f2d3ee3c9627292b8c737617b95fb2a68235385ab43a19
                  cni.projectcalico.org/podIP: 10.42.65.210/32
                  cni.projectcalico.org/podIPs: 10.42.65.210/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: apiv2-proxy
                  kubectl.kubernetes.io/default-logs-container: apiv2-proxy
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.210
IPs:
  IP:           10.42.65.210
Controlled By:  ReplicaSet/apiv2-595d8cb5c
Init Containers:
  istio-init:
    Container ID:  containerd://4818ef08bde7c0a7b703dabc02655fb13b1cefb8da569bf26d37b0f66b1fb9d7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:23 +0000
      Finished:     Tue, 03 Feb 2026 17:48:24 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6pjz7 (ro)
  istio-proxy:
    Container ID:  containerd://8d2640419833f08c26113df5f2a80c26ca641a882a4632c4d79fe905307544a5
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      apiv2-595d8cb5c-lzwzx (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":8090,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     apiv2-proxy,apiv2-grpc
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      apiv2
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/apiv2
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/apiv2-grpc/livez":{"grpc":{"port":2380,"service":""},"timeoutSeconds":1},"/app-health/apiv2-grpc/readyz":{"grpc":{"port":2380,"service":""},"timeoutSeconds":1},"/app-health/apiv2-proxy/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/apiv2-proxy/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6pjz7 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  apiv2-proxy:
    Container ID:  containerd://aad9c6eaaa9853b04473ede0bbee44b62b8bdd85ddc08f86003708b7608323c1
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/apiv2:2.8.1
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/apiv2@sha256:991c54be3b676c15504c28bd25eb9695f4d8c6f99c173a555e35a995c998f29f
    Port:          8080/TCP
    Host Port:     0/TCP
    Command:
      /usr/local/bin/proxy
    Args:
      -allowedCorsOrigins=http://localhost:3000
      -baseRESTURL=/edge-infra.orchestrator.apis/v2
      -echoDebug=false
      -enableAuditing=true
      -enableAuth=true
      -enableTracing=false
      -globalLogLevel=debug
      -oamServerAddress=0.0.0.0:2379
      -serverAddress=0.0.0.0:8080
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:42 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/apiv2-proxy/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/apiv2-proxy/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                <set to the key 'oidc_server_url' of config map 'keycloak-apiv2'>                      Optional: false
      OIDC_TLS_INSECURE_SKIP_VERIFY:  <set to the key 'oidc_tls_insecure_skip_verify_value' of config map 'keycloak-apiv2'>  Optional: false
      ALLOW_MISSING_AUTH_CLIENTS:     
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6pjz7 (ro)
  apiv2-grpc:
    Container ID:  containerd://f1977c92befa4d3f7074e063898da7b0a91e91b3d070b1d114645c310a1331f4
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/apiv2:2.8.1
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/apiv2@sha256:991c54be3b676c15504c28bd25eb9695f4d8c6f99c173a555e35a995c998f29f
    Port:          8090/TCP
    Host Port:     0/TCP
    Command:
      /usr/local/bin/api
    Args:
      -enableAuditing=true
      -enableAuth=true
      -enableTracing=false
      -globalLogLevel=debug
      -grpcAddress=0.0.0.0:8090
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress=0.0.0.0:2380
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:59 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:48:47 +0000
      Finished:     Tue, 03 Feb 2026 17:48:49 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/apiv2-grpc/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/apiv2-grpc/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                <set to the key 'oidc_server_url' of config map 'keycloak-apiv2'>                      Optional: false
      OIDC_TLS_INSECURE_SKIP_VERIFY:  <set to the key 'oidc_tls_insecure_skip_verify_value' of config map 'keycloak-apiv2'>  Optional: false
      ALLOW_MISSING_AUTH_CLIENTS:     
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6pjz7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-6pjz7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57m                default-scheduler  Successfully assigned orch-infra/apiv2-595d8cb5c-lzwzx to orch-tf
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-init
  Normal   Started    57m                kubelet            Started container istio-init
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-proxy
  Normal   Started    57m                kubelet            Started container istio-proxy
  Warning  Unhealthy  57m (x2 over 57m)  kubelet            Startup probe failed: Get "http://10.42.65.210:15021/healthz/ready": dial tcp 10.42.65.210:15021: connect: connection refused
  Normal   Pulling    57m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/apiv2:2.8.1"
  Normal   Pulled     57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/apiv2:2.8.1" in 7.711s (7.711s including waiting). Image size: 32994511 bytes.
  Normal   Created    57m                kubelet            Created container: apiv2-proxy
  Normal   Started    57m                kubelet            Started container apiv2-proxy
  Warning  BackOff    57m (x3 over 57m)  kubelet            Back-off restarting failed container apiv2-grpc in pod apiv2-595d8cb5c-lzwzx_orch-infra(9bea5233-1869-447f-aea1-8b7f049127c9)
  Normal   Pulled     57m (x3 over 57m)  kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/apiv2:2.8.1" already present on machine
  Normal   Created    57m (x3 over 57m)  kubelet            Created container: apiv2-grpc
  Normal   Started    57m (x3 over 57m)  kubelet            Started container apiv2-grpc


Name:             attestationstatusmgr-6c7f99c4bf-84jmm
Namespace:        orch-infra
Priority:         0
Service Account:  attestationstatusmgr
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:04 +0000
Labels:           app.kubernetes.io/instance=infra-managers
                  app.kubernetes.io/name=attestationstatus-manager
                  pod-template-hash=6c7f99c4bf
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=attestationstatus-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 9b9d1f4f64aa18e350bcea2f189c978d97717713af6bb93e39100784ffbb4397
                  cni.projectcalico.org/podIP: 10.42.65.224/32
                  cni.projectcalico.org/podIPs: 10.42.65.224/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: attestationstatus-manager
                  kubectl.kubernetes.io/default-logs-container: attestationstatus-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.224
IPs:
  IP:           10.42.65.224
Controlled By:  ReplicaSet/attestationstatusmgr-6c7f99c4bf
Init Containers:
  istio-init:
    Container ID:  containerd://ae7d4644d198d62c678bdaa9b7783fca47e2aa5deae015d37e5f2abb7aede2bc
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:10 +0000
      Finished:     Tue, 03 Feb 2026 17:53:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hx55j (ro)
  istio-proxy:
    Container ID:  containerd://fc14459dcf3e2639279c8950e37d90f8f55d910ae813dee19e2d9c1caf3fcc0b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      attestationstatusmgr-6c7f99c4bf-84jmm (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"attest-api","containerPort":50007,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     attestationstatus-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      attestationstatusmgr
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/attestationstatusmgr
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/attestationstatus-manager/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/attestationstatus-manager/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hx55j (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  attestationstatus-manager:
    Container ID:  containerd://c2bbcac014e8110e9a2427cc8980d0095722ce76b4976ce9e072a77674e43439
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/attestationstatusmgr:0.8.1
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/attestationstatusmgr@sha256:bea197e3e6a2d3b5e503c7882eacce0f92662f4467d27f48e532d1c683bee605
    Port:          50007/TCP
    Host Port:     0/TCP
    Args:
      -serverAddress=0.0.0.0:50007
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress=0.0.0.0:2379
      -globalLogLevel=info
      -enableTracing=false
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:      50m
      memory:   64Mi
    Liveness:   http-get http://:15020/app-health/attestationstatus-manager/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/attestationstatus-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
      ALLOW_MISSING_AUTH_CLIENTS:     
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hx55j (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-hx55j:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  53m   default-scheduler  Successfully assigned orch-infra/attestationstatusmgr-6c7f99c4bf-84jmm to orch-tf
  Normal   Pulled     53m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m   kubelet            Created container: istio-init
  Normal   Started    53m   kubelet            Started container istio-init
  Normal   Pulled     53m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m   kubelet            Created container: istio-proxy
  Normal   Started    52m   kubelet            Started container istio-proxy
  Warning  Unhealthy  52m   kubelet            Startup probe failed: Get "http://10.42.65.224:15021/healthz/ready": dial tcp 10.42.65.224:15021: connect: connection refused
  Normal   Pulling    52m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/attestationstatusmgr:0.8.1"
  Normal   Pulled     52m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/attestationstatusmgr:0.8.1" in 6.172s (6.172s including waiting). Image size: 15799930 bytes.
  Normal   Created    52m   kubelet            Created container: attestationstatus-manager
  Normal   Started    52m   kubelet            Started container attestationstatus-manager
  Warning  Unhealthy  52m   kubelet            Readiness probe failed: Get "http://10.42.65.224:15020/app-health/attestationstatus-manager/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             dkam-7d8b696776-qgzcq
Namespace:        orch-infra
Priority:         0
Service Account:  dkam
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:54:02 +0000
Labels:           app.kubernetes.io/instance=infra-onboarding
                  app.kubernetes.io/name=dkam
                  pod-template-hash=7d8b696776
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=dkam
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 59e64ed958271d7ea8f56aa909b47e3005fe014cea09ffb63297db856dfe661f
                  cni.projectcalico.org/podIP: 10.42.65.245/32
                  cni.projectcalico.org/podIPs: 10.42.65.245/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: dkam
                  kubectl.kubernetes.io/default-logs-container: dkam
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.245
IPs:
  IP:           10.42.65.245
Controlled By:  ReplicaSet/dkam-7d8b696776
Init Containers:
  istio-init:
    Container ID:  containerd://aa36e2d81a3f508da24cacb1138c6a256b8ed0e0540b93451424adf48dc989c4
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:11 +0000
      Finished:     Tue, 03 Feb 2026 17:54:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6qhpv (ro)
  istio-proxy:
    Container ID:  containerd://cc03801a8bd108c8503ec0c4b03845e52bb8d9d94a9162f05d2980bf05314b88
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      dkam-7d8b696776-qgzcq (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"grpc","containerPort":5581,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     dkam
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      dkam
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/dkam
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6qhpv (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  volume-permissions:
    Container ID:    containerd://f2bc814333fed0bf796572b268433c3f14802b9faf60816befc09b470fa6c56b
    Image:           busybox:1.36.1
    Image ID:        docker.io/library/busybox@sha256:b9598f8c98e24d0ad42c1742c32516772c3aa2151011ebaf639089bd18c605b8
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
    Args:
      -c
      chmod -R 755 /data && chown 1000:1000 /data
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:28 +0000
      Finished:     Tue, 03 Feb 2026 17:54:28 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /data from disk-pvc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6qhpv (ro)
Containers:
  dkam:
    Container ID:  containerd://d724a4480a9e3b2b21f7231496398b71f679c2d2050725a65e43f5955c5f4dc7
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/dkammgr:1.32.9
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/dkammgr@sha256:a88e63138a294a2c1dee3f785b8255384f6834c55d59c1064f359419e5288d57
    Port:          5581/TCP
    Host Port:     0/TCP
    Args:
      -enableTracing=false
      -globalLogLevel=info
      -oamServerAddress=0.0.0.0:2379
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
      -configFile=/etc/infra-config/config.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:59 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     10m
      memory:  16Mi
    Environment:
      MODE:                           prod
      RS_PROFILE_SCRIPTS_REPO:        edge-orch/en/files/profile-scripts/
      http_proxy:                     
      https_proxy:                    
      no_proxy:                       
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
      ALLOW_MISSING_AUTH_CLIENTS:     
      RSPROXY_ADDRESS:                rs-proxy.orch-platform.svc.cluster.local:8081/
    Mounts:
      /data from disk-pvc (rw)
      /etc/infra-config from infra-config-volume (ro)
      /etc/ssl/boots-ca-cert from boots-ca-cert (ro)
      /etc/ssl/orch-ca-cert from orch-ca-cert (ro)
      /tmp from download-path (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6qhpv (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  infra-config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      infra-config
    Optional:  false
  disk-pvc:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  dkam-tink-shared-pvc
    ReadOnly:   false
  boots-ca-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  boots-ca-cert
    Optional:    true
  orch-ca-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gateway-ca-cert
    Optional:    true
  download-path:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-6qhpv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  52m                default-scheduler  Successfully assigned orch-infra/dkam-7d8b696776-qgzcq to orch-tf
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-init
  Normal   Started    52m                kubelet            Started container istio-init
  Normal   Pulled     51m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    51m                kubelet            Created container: istio-proxy
  Normal   Started    51m                kubelet            Started container istio-proxy
  Warning  Unhealthy  51m                kubelet            Startup probe failed: Get "http://10.42.65.245:15021/healthz/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  51m (x4 over 51m)  kubelet            Startup probe failed: Get "http://10.42.65.245:15021/healthz/ready": dial tcp 10.42.65.245:15021: connect: connection refused
  Normal   Pulling    51m                kubelet            Pulling image "busybox:1.36.1"
  Normal   Pulled     51m                kubelet            Successfully pulled image "busybox:1.36.1" in 807ms (807ms including waiting). Image size: 2217150 bytes.
  Normal   Created    51m                kubelet            Created container: volume-permissions
  Normal   Started    51m                kubelet            Started container volume-permissions
  Normal   Pulling    51m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/dkammgr:1.32.9"
  Normal   Pulled     51m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/dkammgr:1.32.9" in 27.657s (27.657s including waiting). Image size: 280534725 bytes.
  Normal   Created    51m                kubelet            Created container: dkam
  Normal   Started    51m                kubelet            Started container dkam


Name:             dm-manager-9b45bbb85-rctrc
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:56 +0000
Labels:           app.kubernetes.io/instance=infra-external
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=dm-manager
                  app.kubernetes.io/version=0.7.5
                  helm.sh/chart=dm-manager-0.7.2
                  pod-template-hash=9b45bbb85
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=dm-manager
                  service.istio.io/canonical-revision=0.7.5
Annotations:      cni.projectcalico.org/containerID: 6f36a047c4fcd754dee2cb511221af7866ff533c71e73fc141da820735a15cab
                  cni.projectcalico.org/podIP: 10.42.65.237/32
                  cni.projectcalico.org/podIPs: 10.42.65.237/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: dm-manager
                  kubectl.kubernetes.io/default-logs-container: dm-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.237
IPs:
  IP:           10.42.65.237
Controlled By:  ReplicaSet/dm-manager-9b45bbb85
Init Containers:
  istio-init:
    Container ID:  containerd://d0be5758fce762ee2081ae6de2a4ef28e4ee7df2b1a7aad8dafe79b327be4007
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:02 +0000
      Finished:     Tue, 03 Feb 2026 17:54:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rkvpc (ro)
  istio-proxy:
    Container ID:  containerd://b8902c217e741f0b23711b310837cc1cf8b0d3beab7324d3329b84d514e23466
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      dm-manager-9b45bbb85-rctrc (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     dm-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      dm-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/dm-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rkvpc (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  dm-manager:
    Container ID:  containerd://9a4c4d47515d361c160164f1ead1fbf604966b6db269769eeeb70a012fd114d8
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/dm-manager:0.7.5
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/dm-manager@sha256:87d183eacec53351bd4366a857e3f03cc13d9db61102fbab639cb68676179d97
    Port:          <none>
    Host Port:     <none>
    Args:
      -passwordPolicy=static
    State:          Running
      Started:      Tue, 03 Feb 2026 17:56:01 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:55:18 +0000
      Finished:     Tue, 03 Feb 2026 17:55:18 +0000
    Ready:          True
    Restart Count:  4
    Limits:
      cpu:     400m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
    Mounts:
      /etc/infra-config from infra-config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rkvpc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  infra-config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      infra-config
    Optional:  false
  kube-api-access-rkvpc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  52m                default-scheduler  Successfully assigned orch-infra/dm-manager-9b45bbb85-rctrc to orch-tf
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-init
  Normal   Started    52m                kubelet            Started container istio-init
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-proxy
  Normal   Started    52m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x3 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.237:15021/healthz/ready": dial tcp 10.42.65.237:15021: connect: connection refused
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/dm-manager:0.7.5"
  Normal   Pulled     51m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/dm-manager:0.7.5" in 11.657s (11.657s including waiting). Image size: 16355204 bytes.
  Warning  BackOff    50m (x7 over 51m)  kubelet            Back-off restarting failed container dm-manager in pod dm-manager-9b45bbb85-rctrc_orch-infra(30c23191-b506-4704-93ca-996f47b875ca)
  Normal   Pulled     50m (x4 over 51m)  kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/dm-manager:0.7.5" already present on machine
  Normal   Created    50m (x5 over 51m)  kubelet            Created container: dm-manager
  Normal   Started    50m (x5 over 51m)  kubelet            Started container dm-manager


Name:             edgenode-observability-grafana-5b64fcd855-nk8q8
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-grafana
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=grafana
                  app.kubernetes.io/version=12.2.1
                  helm.sh/chart=grafana-10.1.4
                  orchestrator/service=logging
                  pod-template-hash=5b64fcd855
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=grafana
                  service.istio.io/canonical-revision=12.2.1
Annotations:      checksum/config: 398fe015ce5b509779f747dda08b01dab496c1d109f58fc2c670ce0a35dfcff0
                  checksum/sc-dashboard-provider-config: c942752180ddff51a3ab63b7d256cf3d856d90757b6f804cbc420562989d5a84
                  cni.projectcalico.org/containerID: a69affd8bc4f7fa17d87386de8994afc96c0d9a2ce3f53a98b1fc03559d58bdd
                  cni.projectcalico.org/podIP: 10.42.65.67/32
                  cni.projectcalico.org/podIPs: 10.42.65.67/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: grafana
                  kubectl.kubernetes.io/default-logs-container: grafana-sc-dashboard
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 1000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 1024Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.67
IPs:
  IP:           10.42.65.67
Controlled By:  ReplicaSet/edgenode-observability-grafana-5b64fcd855
Init Containers:
  istio-init:
    Container ID:  containerd://161977f5f0f92ce8c15e33a4d2af430cc534d95d2b16c46224a79fc45ada16e1
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:44 +0000
      Finished:     Tue, 03 Feb 2026 17:35:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzn7d (ro)
  istio-proxy:
    Container ID:  containerd://6b3587b918ed3250cdd25c715480021f49d25fd072c99b2961ffe168cdb44896
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:47 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-grafana-5b64fcd855-nk8q8 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"grafana","containerPort":3000,"protocol":"TCP"}
                                         ,{"name":"gossip-tcp","containerPort":9094,"protocol":"TCP"}
                                         ,{"name":"gossip-udp","containerPort":9094,"protocol":"UDP"}
                                         ,{"name":"profiling","containerPort":6060,"protocol":"TCP"}
                                         ,{"containerPort":9190,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     grafana-sc-dashboard,grafana,grafana-proxy
      GOMEMLIMIT:                    1073741824 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-grafana
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-grafana
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/grafana/livez":{"httpGet":{"path":"/api/health","port":3000,"scheme":"HTTP"},"timeoutSeconds":30},"/app-health/grafana/readyz":{"httpGet":{"path":"/api/health","port":3000,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzn7d (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  grafana-sc-dashboard:
    Container ID:    containerd://ec342f4eaf9780892fb599677e6ad0f6019e032caac1ab01cc0b96eefb46cf1e
    Image:           quay.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:        docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:12 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:             WATCH
      LABEL:              grafana_dashboard
      LABEL_VALUE:        edgenode
      FOLDER:             /tmp/dashboards
      RESOURCE:           both
      NAMESPACE:          ALL
      FOLDER_ANNOTATION:  grafana_folder
      REQ_URL:            http://localhost:3000/api/admin/provisioning/dashboards/reload
      REQ_METHOD:         POST
    Mounts:
      /tmp/dashboards from sc-dashboard-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzn7d (ro)
  grafana:
    Container ID:    containerd://5d983a337cc153ee7510f4c518a5a7bc10d1a9552eabf2fb53d8a3f1c051927f
    Image:           docker.io/grafana/grafana:12.2.1
    Image ID:        docker.io/grafana/grafana@sha256:35c41e0fd0295f5d0ee5db7e780cf33506abfaf47686196f825364889dee878b
    Ports:           3000/TCP, 9094/TCP, 9094/UDP, 6060/TCP
    Host Ports:      0/TCP, 0/TCP, 0/UDP, 0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:43 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/grafana/livez delay=60s timeout=30s period=10s #success=1 #failure=10
    Readiness:  http-get http://:15020/app-health/grafana/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_IP:                                       (v1:status.podIP)
      GF_PATHS_DATA:                               /var/lib/grafana/
      GF_PATHS_LOGS:                               /var/log/grafana
      GF_PATHS_PLUGINS:                            /var/lib/grafana/plugins
      GF_PATHS_PROVISIONING:                       /etc/grafana/provisioning
      GF_SECURITY_DISABLE_INITIAL_ADMIN_CREATION:  true
    Mounts:
      /etc/grafana/grafana.ini from config (rw,path="grafana.ini")
      /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml from sc-dashboard-provider (rw,path="provider.yaml")
      /etc/grafana/provisioning/datasources/datsources.yaml from config (rw,path="datsources.yaml")
      /tmp/dashboards from sc-dashboard-volume (rw)
      /var/lib/grafana from storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzn7d (ro)
  grafana-proxy:
    Container ID:    containerd://9fecca3b6ec5d916ea2bd015bfbae6077f6b9fc614325306fcdb606a808b7030
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/o11y/grafana-proxy:0.5.1
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/o11y/grafana-proxy@sha256:d26129f13655c12d44492387c1c8757ce2b407860ba3e231b0862a47d1cf1a55
    Port:            9190/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --log-level=info
      --include-deleted
      --mimir-url=http://edgenode-observability-mimir-gateway.orch-infra.svc.cluster.local:8181/prometheus
      --loki-url=http://edgenode-observability-loki-gateway.orch-infra.svc.cluster.local:80
      --otc-url=observability-tenant-controller.orch-platform.svc.cluster.local:50051
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:46 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzn7d (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-grafana
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  sc-dashboard-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  sc-dashboard-provider:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-grafana-config-dashboards
    Optional:  false
  kube-api-access-fzn7d:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             edgenode-observability-loki-chunks-cache-0
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/component=memcached-chunks-cache
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=edgenode-observability-loki-chunks-cache-967dccc7
                  name=memcached-chunks-cache
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=edgenode-observability-loki-chunks-cache-0
Annotations:      cni.projectcalico.org/containerID: 0bf8a5ddc4e1b7ada569080d28138052380f2f64b3a6d8581f8d97eb5889e69e
                  cni.projectcalico.org/podIP: 10.42.65.105/32
                  cni.projectcalico.org/podIPs: 10.42.65.105/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: memcached
                  kubectl.kubernetes.io/default-logs-container: memcached
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 4000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 512Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.105
IPs:
  IP:           10.42.65.105
Controlled By:  StatefulSet/edgenode-observability-loki-chunks-cache
Init Containers:
  istio-init:
    Container ID:  containerd://af39d37d1f99a1d5fb70a46cc2bf85def5442d0e8fd97fc46c404c8f4ab1d3b2
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:45 +0000
      Finished:     Tue, 03 Feb 2026 17:35:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  512Mi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mk46j (ro)
  istio-proxy:
    Container ID:  containerd://1ff2d83e66f5d65c0b1d8ee87d10ad16d2b9e8a5fb2e784953b4ff6b7e5411a2
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  512Mi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-loki-chunks-cache-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               4 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"client","containerPort":11211,"protocol":"TCP"}
                                         ,{"name":"http-metrics","containerPort":9150,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     memcached,exporter
      GOMEMLIMIT:                    536870912 (limits.memory)
      GOMAXPROCS:                    4 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-loki-chunks-cache
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/edgenode-observability-loki-chunks-cache
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/exporter/livez":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":5},"/app-health/exporter/readyz":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":3},"/app-health/memcached/livez":{"tcpSocket":{"port":11211},"timeoutSeconds":5},"/app-health/memcached/readyz":{"tcpSocket":{"port":11211},"timeoutSeconds":3}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mk46j (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  memcached:
    Container ID:  containerd://7d57654884801086b8972fbb27e8a46a39e2ad466b3fb3cc271b39b2ef400a19
    Image:         memcached:1.6.39-alpine
    Image ID:      docker.io/library/memcached@sha256:c8503d4491edd3110cc07d0465089d3a41cf1daf8645e71149e39a51835e92cd
    Port:          11211/TCP
    Host Port:     0/TCP
    Args:
      -m 8192
      --extended=modern,track_sizes
      -I 5m
      -c 16384
      -v
      -u 11211
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/memcached/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/memcached/readyz delay=5s timeout=3s period=5s #success=1 #failure=6
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mk46j (ro)
  exporter:
    Container ID:  containerd://d1d0a3fd09c01b8635c7e13a5ae12967e514bd578f33699a32c792e926d6e43c
    Image:         prom/memcached-exporter:v0.15.3
    Image ID:      docker.io/prom/memcached-exporter@sha256:4356d5f0d2f0ba1525ad995d72fb83531ccaf85ee5468c02ab1049d4ccabd308
    Port:          9150/TCP
    Host Port:     0/TCP
    Args:
      --memcached.address=localhost:11211
      --web.listen-address=0.0.0.0:9150
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/exporter/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/exporter/readyz delay=5s timeout=3s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mk46j (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-mk46j:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             edgenode-observability-loki-gateway-59f4f949b8-8n8xm
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:37 +0000
Labels:           app.kubernetes.io/component=gateway
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  orchestrator/service=observability
                  pod-template-hash=59f4f949b8
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: eff97e29cec4a69afd9a6fd6e2547c578eacba0e09339fe09200d2b67c3f40e8
                  cni.projectcalico.org/containerID: bc92844c535fb3b29fe778cc5bc68e1a2f48f40b8ba1f14e426ce71bd86db5fa
                  cni.projectcalico.org/podIP: 10.42.65.119/32
                  cni.projectcalico.org/podIPs: 10.42.65.119/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: nginx
                  kubectl.kubernetes.io/default-logs-container: nginx
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.119
IPs:
  IP:           10.42.65.119
Controlled By:  ReplicaSet/edgenode-observability-loki-gateway-59f4f949b8
Init Containers:
  istio-init:
    Container ID:  containerd://1f6492d2e2911ab0522063ebc093c3615dc9db3d817816bf9559b355e20e7c8a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:43 +0000
      Finished:     Tue, 03 Feb 2026 17:35:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cjtlc (ro)
  istio-proxy:
    Container ID:  containerd://9234730a1576cf7cd9f70bd7ffb9f7b152710b8a917effd936267394cd206cd3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-loki-gateway-59f4f949b8-8n8xm (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     nginx
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-loki-gateway
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-loki-gateway
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/nginx/readyz":{"httpGet":{"path":"/","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cjtlc (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  nginx:
    Container ID:   containerd://2cc04798ab716f14560f4a944f011dcf9d63e9875d71573f42e4cea118858c57
    Image:          docker.io/nginxinc/nginx-unprivileged:1.29-alpine
    Image ID:       docker.io/nginxinc/nginx-unprivileged@sha256:5aea7cc516b419e3526f47dd1531be31a56a046cfe44754d94f9383e13e2ee99
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/nginx/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /docker-entrypoint.d from docker-entrypoint-d-override (rw)
      /etc/nginx from config (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cjtlc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-loki-gateway
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  docker-entrypoint-d-override:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-cjtlc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             edgenode-observability-loki-results-cache-0
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=memcached-results-cache
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=edgenode-observability-loki-results-cache-6c5dff74b9
                  name=memcached-results-cache
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=edgenode-observability-loki-results-cache-0
Annotations:      cni.projectcalico.org/containerID: 3309c454fac3073db39004e41d02573997728d6985056ed0fb554780940915ff
                  cni.projectcalico.org/podIP: 10.42.65.136/32
                  cni.projectcalico.org/podIPs: 10.42.65.136/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: memcached
                  kubectl.kubernetes.io/default-logs-container: memcached
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 500m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 512Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.136
IPs:
  IP:           10.42.65.136
Controlled By:  StatefulSet/edgenode-observability-loki-results-cache
Init Containers:
  istio-init:
    Container ID:  containerd://678dafef82b461fe983f2bf6b908102a430fea7a2ff353e6eea866fd3278e909
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:47 +0000
      Finished:     Tue, 03 Feb 2026 17:35:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mh2lk (ro)
  istio-proxy:
    Container ID:  containerd://3edbf51f66f934182dd1fb60140bb10dc5f81604610e69b022b78d5eaa3c0474
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:52 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-loki-results-cache-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"client","containerPort":11211,"protocol":"TCP"}
                                         ,{"name":"http-metrics","containerPort":9150,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     memcached,exporter
      GOMEMLIMIT:                    536870912 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-loki-results-cache
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/edgenode-observability-loki-results-cache
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/exporter/livez":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":5},"/app-health/exporter/readyz":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":3},"/app-health/memcached/livez":{"tcpSocket":{"port":11211},"timeoutSeconds":5},"/app-health/memcached/readyz":{"tcpSocket":{"port":11211},"timeoutSeconds":3}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mh2lk (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  memcached:
    Container ID:  containerd://8d7cbee225be948e7d3e6c1b8020541d6d24c4549254482b99d1a552e9dea780
    Image:         memcached:1.6.39-alpine
    Image ID:      docker.io/library/memcached@sha256:c8503d4491edd3110cc07d0465089d3a41cf1daf8645e71149e39a51835e92cd
    Port:          11211/TCP
    Host Port:     0/TCP
    Args:
      -m 1024
      --extended=modern,track_sizes
      -I 5m
      -c 16384
      -v
      -u 11211
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/memcached/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/memcached/readyz delay=5s timeout=3s period=5s #success=1 #failure=6
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mh2lk (ro)
  exporter:
    Container ID:  containerd://0aceceb4826b8f2769251d9098578dab0d060a1f904d52d857c21a8e48c531ea
    Image:         prom/memcached-exporter:v0.15.3
    Image ID:      docker.io/prom/memcached-exporter@sha256:4356d5f0d2f0ba1525ad995d72fb83531ccaf85ee5468c02ab1049d4ccabd308
    Port:          9150/TCP
    Host Port:     0/TCP
    Args:
      --memcached.address=localhost:11211
      --web.listen-address=0.0.0.0:9150
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/exporter/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/exporter/readyz delay=5s timeout=3s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mh2lk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-mh2lk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             edgenode-observability-mimir-compactor-0
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:14 +0000
Labels:           app.kubernetes.io/component=compactor
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=edgenode-observability-mimir-compactor-575dfb78d4
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=edgenode-observability-mimir-compactor-0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: b8333ad73df47bd79001513983c09ba9a9b5b97f6c39235ad33f54dc909b0a77
                  cni.projectcalico.org/podIP: 10.42.65.180/32
                  cni.projectcalico.org/podIPs: 10.42.65.180/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: compactor
                  kubectl.kubernetes.io/default-logs-container: compactor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 500m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 512Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.180
IPs:
  IP:           10.42.65.180
Controlled By:  StatefulSet/edgenode-observability-mimir-compactor
Init Containers:
  istio-init:
    Container ID:  containerd://6d4479862ce7daffbd348a363276a0163683bf978cf6a3b49f138696fdbe3fca
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:20 +0000
      Finished:     Tue, 03 Feb 2026 17:36:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2mq7s (ro)
  istio-proxy:
    Container ID:  containerd://dc7b426f188dec473872d9d57dc1e4fd175be9ab2c050f1f6f76f183b527c017
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-compactor-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     compactor
      GOMEMLIMIT:                    536870912 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-compactor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/edgenode-observability-mimir-compactor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/compactor/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2mq7s (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  compactor:
    Container ID:  containerd://943cb024c6ee623b278fa49b23160b1c318fdd2e22ad052519690f5cce535a2f
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=compactor
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/compactor/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2mq7s (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-edgenode-observability-mimir-compactor-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-2mq7s:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=compactor,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-distributor-56cdc7f846-dcq2p
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/component=distributor
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=56cdc7f846
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: ac81d37dff54d1b254aa3fb1acf6667b298d13f94aad6a161219f535604d3bb9
                  cni.projectcalico.org/podIP: 10.42.65.73/32
                  cni.projectcalico.org/podIPs: 10.42.65.73/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: distributor
                  kubectl.kubernetes.io/default-logs-container: distributor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 1000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 1Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.73
IPs:
  IP:           10.42.65.73
Controlled By:  ReplicaSet/edgenode-observability-mimir-distributor-56cdc7f846
Init Containers:
  istio-init:
    Container ID:  containerd://0ebf410f04ad88c55ab7147f2cf07765970d27de5b19b3a8f03310d9372d7125
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:44 +0000
      Finished:     Tue, 03 Feb 2026 17:35:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d4m25 (ro)
  istio-proxy:
    Container ID:  containerd://2998b8477f5aac41d2d83f87f72c115e6aca01fa3d02db60111b98cb00a17aee
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:49 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-distributor-56cdc7f846-dcq2p (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     distributor
      GOMEMLIMIT:                    1073741824 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-distributor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-distributor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/distributor/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d4m25 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  distributor:
    Container ID:  containerd://e263691928fe547b26653e5227068b73e0c34380e72e64921efc635f28b1c2f4
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=distributor
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -server.grpc.keepalive.max-connection-age=60s
      -server.grpc.keepalive.max-connection-age-grace=5m
      -server.grpc.keepalive.max-connection-idle=1m
      -shutdown-delay=90s
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:10 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/distributor/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  8
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d4m25 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-d4m25:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=distributor,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-gateway-75d747b9ff-kg64l
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=gateway
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=75d747b9ff
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: 5599cefbd04be5e4f64e3060aa788dcbba825d4f7118f139c6a9155778bd9a4c
                  cni.projectcalico.org/containerID: 3453ec843747cc46fa6ec4ec4ca7eab35883219a9303356c453d439eafc3e9fa
                  cni.projectcalico.org/podIP: 10.42.65.137/32
                  cni.projectcalico.org/podIPs: 10.42.65.137/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: gateway
                  kubectl.kubernetes.io/default-logs-container: gateway
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 1000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 1024Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.137
IPs:
  IP:           10.42.65.137
Controlled By:  ReplicaSet/edgenode-observability-mimir-gateway-75d747b9ff
Init Containers:
  istio-init:
    Container ID:  containerd://15598e92e54c6891223b7c4ab0e3a978ac700494004f46ee92cc9ada6b5ba725
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:48 +0000
      Finished:     Tue, 03 Feb 2026 17:35:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-brvk8 (ro)
  istio-proxy:
    Container ID:  containerd://b707d7dd1243deb0b4253fe67b95e92eb96fa8bbd2297b80e1bb3aa1bdcaece8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-gateway-75d747b9ff-kg64l (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     gateway
      GOMEMLIMIT:                    1073741824 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-gateway
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-gateway
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/gateway/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-brvk8 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  gateway:
    Container ID:   containerd://284abf8c049aa5b7104741c344661f538c7418729c137ae592f42dc4ab946fc8
    Image:          docker.io/nginxinc/nginx-unprivileged:1.29-alpine
    Image ID:       docker.io/nginxinc/nginx-unprivileged@sha256:5aea7cc516b419e3526f47dd1531be31a56a046cfe44754d94f9383e13e2ee99
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/gateway/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /docker-entrypoint.d from docker-entrypoint-d-override (rw)
      /etc/nginx/nginx.conf from nginx-config (rw,path="nginx.conf")
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-brvk8 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  nginx-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-gateway-nginx
    Optional:  false
  docker-entrypoint-d-override:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-brvk8:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=gateway,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-ingester-0
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:56 +0000
Labels:           app.kubernetes.io/component=ingester
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=edgenode-observability-mimir-ingester-7dfcf45548
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=edgenode-observability-mimir-ingester-0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 51cc576e9089f344d33090349023469418b8049f701103739ddff1e78ae88a2d
                  cni.projectcalico.org/podIP: 10.42.65.169/32
                  cni.projectcalico.org/podIPs: 10.42.65.169/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: ingester
                  kubectl.kubernetes.io/default-logs-container: ingester
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 1000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 1Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.169
IPs:
  IP:           10.42.65.169
Controlled By:  StatefulSet/edgenode-observability-mimir-ingester
Init Containers:
  istio-init:
    Container ID:  containerd://9757bdbfe918f0e92dc27a408f2e6152ce1ab74ff944b453c3978ce2b634936f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:06 +0000
      Finished:     Tue, 03 Feb 2026 17:36:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-px9ct (ro)
  istio-proxy:
    Container ID:  containerd://e005595b26d3d2fbf04f28fb23a086f7cc91bc8631e7108264c8c2e2ad4de780
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-ingester-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     ingester
      GOMEMLIMIT:                    1073741824 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-ingester
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/edgenode-observability-mimir-ingester
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/ingester/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-px9ct (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  ingester:
    Container ID:  containerd://476dce27b36af47a116d933c6ab57277fa9ba946f42ae966c9d86fc984464d8a
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=ingester
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -ingester.ring.instance-availability-zone=zone-default
      -server.grpc-max-concurrent-streams=500
      -memberlist.abort-if-fast-join-fails=true
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/ingester/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  4
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-px9ct (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-edgenode-observability-mimir-ingester-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-px9ct:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=ingester,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-ingester-1
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:12 +0000
Labels:           app.kubernetes.io/component=ingester
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=1
                  controller-revision-hash=edgenode-observability-mimir-ingester-7dfcf45548
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=edgenode-observability-mimir-ingester-1
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 4ba756eac864307407803802bfd718900ee87696834777015cd76d5a0ca83319
                  cni.projectcalico.org/podIP: 10.42.65.176/32
                  cni.projectcalico.org/podIPs: 10.42.65.176/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: ingester
                  kubectl.kubernetes.io/default-logs-container: ingester
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 1000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 1Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.176
IPs:
  IP:           10.42.65.176
Controlled By:  StatefulSet/edgenode-observability-mimir-ingester
Init Containers:
  istio-init:
    Container ID:  containerd://1a3c22e71c4b41ffc7347ac1eb2d2359ddb35a2cbf221b2a1a237c7481497530
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:16 +0000
      Finished:     Tue, 03 Feb 2026 17:36:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-drbhw (ro)
  istio-proxy:
    Container ID:  containerd://89d2a59b7c3febc350491a7edbb5f2fec92194884e5f1754c7f244058d66b52f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-ingester-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     ingester
      GOMEMLIMIT:                    1073741824 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-ingester
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/edgenode-observability-mimir-ingester
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/ingester/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-drbhw (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  ingester:
    Container ID:  containerd://b187b29113743d9aa4a39011b1762f4fc1eb7c984069595c7003fb144693fe47
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=ingester
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -ingester.ring.instance-availability-zone=zone-default
      -server.grpc-max-concurrent-streams=500
      -memberlist.abort-if-fast-join-fails=true
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/ingester/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  4
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-drbhw (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-edgenode-observability-mimir-ingester-1
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-drbhw:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=ingester,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-make-minio-buckets-5.4.0-8x9rc
Namespace:        orch-infra
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app=mimir-distributed-job
                  batch.kubernetes.io/controller-uid=017a2040-0a1c-496b-ae9e-2f7f3363818d
                  batch.kubernetes.io/job-name=edgenode-observability-mimir-make-minio-buckets-5.4.0
                  controller-uid=017a2040-0a1c-496b-ae9e-2f7f3363818d
                  job-name=edgenode-observability-mimir-make-minio-buckets-5.4.0
                  orchestrator/service=observability
                  release=edgenode-observability
Annotations:      cni.projectcalico.org/containerID: 56950a256e3e56873874217ce8b84a1061bdde7fda00ea555d2f3c190337a163
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
IP:               10.42.65.146
IPs:
  IP:           10.42.65.146
Controlled By:  Job/edgenode-observability-mimir-make-minio-buckets-5.4.0
Containers:
  minio-mc:
    Container ID:  containerd://4b503ae7637c3ff362d01fd71336c50f2381186aec2603eee221879883263e76
    Image:         quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
    Image ID:      quay.io/minio/mc@sha256:a7fe349ef4bd8521fb8497f55c6042871b2ae640607cf99d9bede5e9bdf11727
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      /config/initialize
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:11 +0000
      Finished:     Tue, 03 Feb 2026 17:36:29 +0000
    Ready:          False
    Restart Count:  0
    Requests:
      memory:  128Mi
    Environment:
      MINIO_ENDPOINT:  edgenode-observability-minio
      MINIO_PORT:      9000
    Mounts:
      /config from minio-configuration (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zhwjx (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  minio-configuration:
    Type:                Projected (a volume that contains injected data from multiple sources)
    ConfigMapName:       edgenode-observability-minio
    ConfigMapOptional:   <nil>
    SecretName:          edgenode-observability-minio
    SecretOptionalName:  <nil>
  kube-api-access-zhwjx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             edgenode-observability-mimir-querier-6949957c4-9wkpb
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=querier
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=6949957c4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 288fc9f45207e97a0ee88776f5056812f1e42100716e8d8010a61371971b9301
                  cni.projectcalico.org/podIP: 10.42.65.132/32
                  cni.projectcalico.org/podIPs: 10.42.65.132/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: querier
                  kubectl.kubernetes.io/default-logs-container: querier
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 4000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 6Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.132
IPs:
  IP:           10.42.65.132
Controlled By:  ReplicaSet/edgenode-observability-mimir-querier-6949957c4
Init Containers:
  istio-init:
    Container ID:  containerd://d71261fee55757caa6c944810d3609e5b909a37210156349e9c4902f2f298756
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:45 +0000
      Finished:     Tue, 03 Feb 2026 17:35:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  6Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24jcg (ro)
  istio-proxy:
    Container ID:  containerd://dce6eddfe42b86ed291b2c0f2350bad2b79034f2bb3c5e7f884486c3e467dc9c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  6Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-querier-6949957c4-9wkpb (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               4 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     querier
      GOMEMLIMIT:                    6442450944 (limits.memory)
      GOMAXPROCS:                    4 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-querier
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-querier
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/querier/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24jcg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  querier:
    Container ID:  containerd://be6b2330f7c081098ac1bfa54bdc141265258fc4259b6d541903654117db6222
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=querier
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -querier.store-gateway-client.grpc-max-recv-msg-size=209715200
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:10 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/querier/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  5
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24jcg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-24jcg:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=querier,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-querier-6949957c4-pr79r
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/component=querier
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=6949957c4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 09e97947e59517e8537ec05b1c6cd37e66b79053571e10a1c6c580080d98d0e8
                  cni.projectcalico.org/podIP: 10.42.65.128/32
                  cni.projectcalico.org/podIPs: 10.42.65.128/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: querier
                  kubectl.kubernetes.io/default-logs-container: querier
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 4000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 6Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.128
IPs:
  IP:           10.42.65.128
Controlled By:  ReplicaSet/edgenode-observability-mimir-querier-6949957c4
Init Containers:
  istio-init:
    Container ID:  containerd://013b34e7567ecad22759ea15a0ceda3a5e3407dfd9342e785d22b9b1072c311e
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:45 +0000
      Finished:     Tue, 03 Feb 2026 17:35:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  6Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2tzj7 (ro)
  istio-proxy:
    Container ID:  containerd://cce6a5284d2cdef5efa85ed928649b6384e10e1698a2ea5a10bf2528ed9a7756
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:49 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  6Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-querier-6949957c4-pr79r (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               4 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     querier
      GOMEMLIMIT:                    6442450944 (limits.memory)
      GOMAXPROCS:                    4 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-querier
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-querier
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/querier/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2tzj7 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  querier:
    Container ID:  containerd://297addfe9aa43d30cb1f56bbc94c219c6ca46d71033a49408a22b9155f999414
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=querier
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -querier.store-gateway-client.grpc-max-recv-msg-size=209715200
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:10 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/querier/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  5
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2tzj7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-2tzj7:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=querier,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-querier-6949957c4-tx66h
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=querier
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=6949957c4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 4782c22e6b0a9d64cdcc8c5a4859a939930fb3f7c87cd08a2ea9dd737b2d3a5d
                  cni.projectcalico.org/podIP: 10.42.65.148/32
                  cni.projectcalico.org/podIPs: 10.42.65.148/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: querier
                  kubectl.kubernetes.io/default-logs-container: querier
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 4000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 6Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.148
IPs:
  IP:           10.42.65.148
Controlled By:  ReplicaSet/edgenode-observability-mimir-querier-6949957c4
Init Containers:
  istio-init:
    Container ID:  containerd://73dfba1efba38f957f22ae28c023509d89c21df0db9e10b99929293991d472bb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:52 +0000
      Finished:     Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  6Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b4qxv (ro)
  istio-proxy:
    Container ID:  containerd://46fe2484581e76575e2508f8c125d9e30a41a2a7ab21ba3fbea84d1311020a3b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:57 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  6Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-querier-6949957c4-tx66h (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               4 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     querier
      GOMEMLIMIT:                    6442450944 (limits.memory)
      GOMAXPROCS:                    4 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-querier
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-querier
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/querier/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b4qxv (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  querier:
    Container ID:  containerd://bed27a115680a300be1e9870c513f9e81278b0ac047eeedc827eedbdb24b3d0b
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=querier
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -querier.store-gateway-client.grpc-max-recv-msg-size=209715200
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/querier/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  5
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b4qxv (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-b4qxv:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=querier,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-query-frontend-85f5d7574f-8glvl
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/component=query-frontend
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=85f5d7574f
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 85e20d2335bc570e388e938caac1715779b7f6a35162b98f8e81474d15526391
                  cni.projectcalico.org/podIP: 10.42.65.129/32
                  cni.projectcalico.org/podIPs: 10.42.65.129/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: query-frontend
                  kubectl.kubernetes.io/default-logs-container: query-frontend
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 3000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 4Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.129
IPs:
  IP:           10.42.65.129
Controlled By:  ReplicaSet/edgenode-observability-mimir-query-frontend-85f5d7574f
Init Containers:
  istio-init:
    Container ID:  containerd://c965fd80db770635b2bbdf08d37af2e0882ff0ec82eb8d01260efc5e8989c039
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:46 +0000
      Finished:     Tue, 03 Feb 2026 17:35:46 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     3
      memory:  4Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4nsf5 (ro)
  istio-proxy:
    Container ID:  containerd://ed592706966c49a535f0bd3a8d7753de543356ae9edc547a87afa44c428f596c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:51 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     3
      memory:  4Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-query-frontend-85f5d7574f-8glvl (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               3 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     query-frontend
      GOMEMLIMIT:                    4294967296 (limits.memory)
      GOMAXPROCS:                    3 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-query-frontend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-query-frontend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/query-frontend/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4nsf5 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  query-frontend:
    Container ID:  containerd://bb478b2445e25effc54f7552677126c935ed178541df35a7f21d0b58dd0eecfc
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      -target=query-frontend
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -server.grpc.keepalive.max-connection-age=30s
      -shutdown-delay=90s
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:10 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/query-frontend/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4nsf5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-4nsf5:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=query-frontend,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-query-scheduler-6f96467d4b-sbmrl
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/component=query-scheduler
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  pod-template-hash=6f96467d4b
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 5c98079ceae1b2336634e89b3981491635abc21d48477a6f8cda55464b62ae4f
                  cni.projectcalico.org/podIP: 10.42.65.110/32
                  cni.projectcalico.org/podIPs: 10.42.65.110/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: query-scheduler
                  kubectl.kubernetes.io/default-logs-container: query-scheduler
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.110
IPs:
  IP:           10.42.65.110
Controlled By:  ReplicaSet/edgenode-observability-mimir-query-scheduler-6f96467d4b
Init Containers:
  istio-init:
    Container ID:  containerd://199701c7d8732266f4723e70b988f0faa16e81c7a210ef4399fb272e43062649
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:42 +0000
      Finished:     Tue, 03 Feb 2026 17:35:42 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pqtq4 (ro)
  istio-proxy:
    Container ID:  containerd://d16a183cbf110ff85cd08efc7c0fc48b8be87ede94480b3d08ce7bc482a7b76f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-query-scheduler-6f96467d4b-sbmrl (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     query-scheduler
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-query-scheduler
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-query-scheduler
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/query-scheduler/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pqtq4 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  query-scheduler:
    Container ID:  containerd://64df6e3d14e292dd8bed5c61ede92f2ab9b528a9fec15797e7fa5649db5e0901
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      -target=query-scheduler
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:10 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     128Mi
    Readiness:    http-get http://:15020/app-health/query-scheduler/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pqtq4 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-pqtq4:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=query-scheduler,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-query-scheduler-6f96467d4b-zvkdm
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/component=query-scheduler
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  pod-template-hash=6f96467d4b
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 3f871babbabf804273bad2a63a9687d03e4878921f6e0b37efddc3aa72afe5fe
                  cni.projectcalico.org/podIP: 10.42.65.69/32
                  cni.projectcalico.org/podIPs: 10.42.65.69/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: query-scheduler
                  kubectl.kubernetes.io/default-logs-container: query-scheduler
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.69
IPs:
  IP:           10.42.65.69
Controlled By:  ReplicaSet/edgenode-observability-mimir-query-scheduler-6f96467d4b
Init Containers:
  istio-init:
    Container ID:  containerd://6a1bcdf7ee3969072bde98093f78e62bed3d69bab2c20693ac9617534aa1ae53
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:44 +0000
      Finished:     Tue, 03 Feb 2026 17:35:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wq9cw (ro)
  istio-proxy:
    Container ID:  containerd://fa43bae00586207f38bb83bb97ef356aa7a8799ed93d3761a7ce351d973dd0ff
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:46 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-query-scheduler-6f96467d4b-zvkdm (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     query-scheduler
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-query-scheduler
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-query-scheduler
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/query-scheduler/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wq9cw (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  query-scheduler:
    Container ID:  containerd://502014037bc3a75607c1db20afdf50815a33b5691e43640eccd2d36463872cbf
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      -target=query-scheduler
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:10 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     128Mi
    Readiness:    http-get http://:15020/app-health/query-scheduler/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wq9cw (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-wq9cw:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=query-scheduler,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-ruler-766977f695-96fzs
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=ruler
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=766977f695
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 758c44e7785381e45aa7a7dc8c638c7acc74aa6758ba468af5e15007a5eecc4a
                  cni.projectcalico.org/podIP: 10.42.65.140/32
                  cni.projectcalico.org/podIPs: 10.42.65.140/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: ruler
                  kubectl.kubernetes.io/default-logs-container: ruler
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 2000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 2Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.140
IPs:
  IP:           10.42.65.140
Controlled By:  ReplicaSet/edgenode-observability-mimir-ruler-766977f695
Init Containers:
  istio-init:
    Container ID:  containerd://2934d1a2dfb2b25f541d3850b7d3244dc30ef6a9e3f7705e6468a75b6c838194
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:49 +0000
      Finished:     Tue, 03 Feb 2026 17:35:50 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  2Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p9w8v (ro)
  istio-proxy:
    Container ID:  containerd://deea622e85bd5f0e3d500c24d6345edbdf441434defc141fa9d90fd52455b812
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:54 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  2Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-ruler-766977f695-96fzs (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               2 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     ruler
      GOMEMLIMIT:                    2147483648 (limits.memory)
      GOMAXPROCS:                    2 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-ruler
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-ruler
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/ruler/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p9w8v (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  ruler:
    Container ID:  containerd://405de9374886180800ba78753606f7c3c4b10743fe71ae65fcc73c0f3c4f03e9
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=ruler
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -distributor.remote-timeout=10s
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/ruler/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p9w8v (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-p9w8v:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=ruler,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-ruler-766977f695-bhm4g
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=ruler
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=766977f695
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: f5c7bf506bc331d7c78e423a2e7cee42cebacb520a84783973bac6d05c5f1bd4
                  cni.projectcalico.org/podIP: 10.42.65.144/32
                  cni.projectcalico.org/podIPs: 10.42.65.144/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: ruler
                  kubectl.kubernetes.io/default-logs-container: ruler
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 2000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 2Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.144
IPs:
  IP:           10.42.65.144
Controlled By:  ReplicaSet/edgenode-observability-mimir-ruler-766977f695
Init Containers:
  istio-init:
    Container ID:  containerd://c37bffc22fca395f7219dea898e70b89ebdc8e4d5077d2b80b148a1dfc1c6579
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:51 +0000
      Finished:     Tue, 03 Feb 2026 17:35:51 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  2Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7rk6p (ro)
  istio-proxy:
    Container ID:  containerd://6c029a317f5198f8645684304b63f4762124fb8aa0f1d66768b013ef940488c2
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  2Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-ruler-766977f695-bhm4g (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               2 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     ruler
      GOMEMLIMIT:                    2147483648 (limits.memory)
      GOMAXPROCS:                    2 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-ruler
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-mimir-ruler
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/ruler/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7rk6p (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  ruler:
    Container ID:  containerd://79b126b767df9c579790aade23c798d0b14e2dd4011676faca5f935289c0641b
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=ruler
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -distributor.remote-timeout=10s
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/ruler/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7rk6p (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-7rk6p:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=ruler,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-mimir-store-gateway-0
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:59 +0000
Labels:           app.kubernetes.io/component=store-gateway
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=edgenode-observability-mimir-store-gateway-84975c76d8
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=edgenode-observability-mimir-store-gateway-0
Annotations:      checksum/config: faadf4f33dc52cd0202d4c064fba6be4b6a300c6a8bd9685180b8b6481c3d205
                  cni.projectcalico.org/containerID: 4f550f5dde9db5dc48aad0e0be13dd71605ea0f3e83ec4eb47460be9567e73e2
                  cni.projectcalico.org/podIP: 10.42.65.172/32
                  cni.projectcalico.org/podIPs: 10.42.65.172/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: store-gateway
                  kubectl.kubernetes.io/default-logs-container: store-gateway
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 1000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 1Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.172
IPs:
  IP:           10.42.65.172
Controlled By:  StatefulSet/edgenode-observability-mimir-store-gateway
Init Containers:
  istio-init:
    Container ID:  containerd://aefaa378ceb84f2b6583f5eb8d39c9d8553fb462d5d968f70eb760ebf3f68a17
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:06 +0000
      Finished:     Tue, 03 Feb 2026 17:36:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wbptg (ro)
  istio-proxy:
    Container ID:  containerd://9c705c08f15c777143f6796c8a230372a7ce9498d7ae27548ba6ab339fae2c62
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-mimir-store-gateway-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     store-gateway
      GOMEMLIMIT:                    1073741824 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-mimir-store-gateway
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/edgenode-observability-mimir-store-gateway
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/store-gateway/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wbptg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  store-gateway:
    Container ID:  containerd://5aafc483a7b6f5c7130c59937edf98b86f400a34d4b39c477bbcf62f45c145b5
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=store-gateway
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -server.grpc-max-send-msg-size-bytes=209715200
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/store-gateway/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  5
      GOMEMLIMIT:  1048576
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wbptg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-edgenode-observability-mimir-store-gateway-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-wbptg:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=store-gateway,app.kubernetes.io/instance=edgenode-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             edgenode-observability-minio-748dc47495-7zz2l
Namespace:        orch-infra
Priority:         0
Service Account:  minio-sa
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:52 +0000
Labels:           app=minio
                  orchestrator/service=observability
                  pod-template-hash=748dc47495
                  release=edgenode-observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=minio
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 9be2530257d28457634291b0c18150a977e5e6105adc27ecdcb919e2677acf8a
                  checksum/secrets: 843271090ec1790d233925e53225ff4f91eb3bd43f5f1e17f820d4641414f20e
                  cni.projectcalico.org/containerID: 333b6469e5e4c9fe10c587621ca53fe8976fb5b89efa9e6449b3fa710193ca67
                  cni.projectcalico.org/podIP: 10.42.65.166/32
                  cni.projectcalico.org/podIPs: 10.42.65.166/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: minio
                  kubectl.kubernetes.io/default-logs-container: minio
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.166
IPs:
  IP:           10.42.65.166
Controlled By:  ReplicaSet/edgenode-observability-minio-748dc47495
Init Containers:
  istio-init:
    Container ID:  containerd://33c36c7b55970489177e38bfce2859c2b2945ae9c622f0fa657a4a94184a4a71
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:03 +0000
      Finished:     Tue, 03 Feb 2026 17:36:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-s4dqj (ro)
  istio-proxy:
    Container ID:  containerd://eaade97c7aa98de9f7c4d39d6b0e3f752a1499beba327f904deb4c20009b1078
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-minio-748dc47495-7zz2l (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":9000,"protocol":"TCP"}
                                         ,{"name":"http-console","containerPort":9001,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     minio
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-minio
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-minio
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-s4dqj (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  minio:
    Container ID:  containerd://4b7b2337297990e449de46c208eaca9924f53aaca50dccd394a92a082b8b081d
    Image:         quay.io/minio/minio:RELEASE.2025-09-07T16-13-09Z
    Image ID:      quay.io/minio/minio@sha256:14cea493d9a34af32f524e538b8346cf79f3321eff8e708c1e2960462bd8936e
    Ports:         9000/TCP, 9001/TCP
    Host Ports:    0/TCP, 0/TCP
    Command:
      /bin/sh
      -ce
      /usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/ --address :9000 --console-address :9001
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      MINIO_ROOT_USER:             <set to the key 'rootUser' in secret 'edgenode-observability-minio'>      Optional: false
      MINIO_ROOT_PASSWORD:         <set to the key 'rootPassword' in secret 'edgenode-observability-minio'>  Optional: false
      MINIO_PROMETHEUS_AUTH_TYPE:  public
    Mounts:
      /export from export (rw)
      /tmp/credentials from minio-user (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-s4dqj (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  export:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  edgenode-observability-minio
    ReadOnly:   false
  minio-user:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  edgenode-observability-minio
    Optional:    false
  kube-api-access-s4dqj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             edgenode-observability-opentelemetry-collector-56ffc7db8d-fqsdt
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-opentelemetry-collector
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=opentelemetry-collector
                  component=standalone-collector
                  pod-template-hash=56ffc7db8d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=opentelemetry-collector
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 86b4b21c877f4ad0c0ee3d8e42034658bbb40fdf031721bc7e398c3ad62d70cb
                  cni.projectcalico.org/containerID: 168c325e9c592f886ec2161835ae9f6b098371d97f68f51544783d9efd1fa938
                  cni.projectcalico.org/podIP: 10.42.65.123/32
                  cni.projectcalico.org/podIPs: 10.42.65.123/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: opentelemetry-collector
                  kubectl.kubernetes.io/default-logs-container: opentelemetry-collector
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 2000m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 512Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.123
IPs:
  IP:           10.42.65.123
Controlled By:  ReplicaSet/edgenode-observability-opentelemetry-collector-56ffc7db8d
Init Containers:
  istio-init:
    Container ID:  containerd://518fb6cdbc9f1992ef35e1e596c7af5f98367254d645c0f00be3465910b8be00
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:44 +0000
      Finished:     Tue, 03 Feb 2026 17:35:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  512Mi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-568jl (ro)
  istio-proxy:
    Container ID:  containerd://15aa8cb520c65b57124c71d431ddfb7982a865547d123519c9932a95fd49cf6d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:47 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  512Mi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      edgenode-observability-opentelemetry-collector-56ffc7db8d-fqsdt (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               2 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"jaeger-compact","containerPort":6831,"protocol":"UDP"}
                                         ,{"name":"jaeger-grpc","containerPort":14250,"protocol":"TCP"}
                                         ,{"name":"jaeger-thrift","containerPort":14268,"protocol":"TCP"}
                                         ,{"name":"metrics","containerPort":8888,"protocol":"TCP"}
                                         ,{"name":"otlp","containerPort":4317,"protocol":"TCP"}
                                         ,{"name":"otlp-http","containerPort":4318,"protocol":"TCP"}
                                         ,{"name":"zipkin","containerPort":9411,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     opentelemetry-collector
      GOMEMLIMIT:                    536870912 (limits.memory)
      GOMAXPROCS:                    2 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      edgenode-observability-opentelemetry-collector
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/edgenode-observability-opentelemetry-collector
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/opentelemetry-collector/livez":{"httpGet":{"path":"/","port":13133,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/opentelemetry-collector/readyz":{"httpGet":{"path":"/","port":13133,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-568jl (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  opentelemetry-collector:
    Container ID:  containerd://18dac320a1dbaf026a4ec94307e2569e2358c39270dd02ffc1345cf75f9a7945
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/orch-otelcol:0.2.2
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/orch-otelcol@sha256:65ceb4066705977aff3f0e42e577d0c8087335e2d5fb62bb6dcdd0c2e3e906c8
    Ports:         6831/UDP, 14250/TCP, 14268/TCP, 8888/TCP, 4317/TCP, 4318/TCP, 9411/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Command:
      /otelcontribcol
    Args:
      --config=/conf/relay.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:09 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/opentelemetry-collector/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/opentelemetry-collector/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MY_POD_IP:    (v1:status.podIP)
      GOMEMLIMIT:  52428MiB
    Mounts:
      /conf from opentelemetry-collector-configmap (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-568jl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  opentelemetry-collector-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      edgenode-observability-opentelemetry-collector
    Optional:  false
  kube-api-access-568jl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             exporter-785b7cb76d-d82pq
Namespace:        orch-infra
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:18 +0000
Labels:           app.kubernetes.io/instance=infra-core
                  app.kubernetes.io/name=exporter
                  pod-template-hash=785b7cb76d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=exporter
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 0f2a14b5946a8c6e88d193ac4948bd51d9c9150b2dc9828e461195723f749871
                  cni.projectcalico.org/podIP: 10.42.65.209/32
                  cni.projectcalico.org/podIPs: 10.42.65.209/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: exporter
                  kubectl.kubernetes.io/default-logs-container: exporter
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.209
IPs:
  IP:           10.42.65.209
Controlled By:  ReplicaSet/exporter-785b7cb76d
Init Containers:
  istio-init:
    Container ID:  containerd://d0896f33138b0d7da61d02b57747ef8000bf8c25cb613ec37c71db3238b9d3d6
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:22 +0000
      Finished:     Tue, 03 Feb 2026 17:48:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nzcqk (ro)
  istio-proxy:
    Container ID:  containerd://591377000c7c3d43f007a66eb3e7ab302eeadc562bd7dc6424d1aa04de41a568
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      exporter-785b7cb76d-d82pq (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":9101,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     exporter
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      exporter
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/exporter
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/exporter/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/exporter/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nzcqk (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  exporter:
    Container ID:  containerd://42c9a6d11b045a76462c1e60f85e7c2b3a8080740dd63169ae703d4d787597ff
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/exporter:1.23.0
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/exporter@sha256:eade74addf682a0de5565e58c72b8a7b6475d6969074e56c6a348aae869c19ae
    Port:          9101/TCP
    Host Port:     0/TCP
    Args:
      -enableTracing=false
      -exporterAddress=0.0.0.0:9101
      -exporterPath=/metrics
      -globalLogLevel=info
      -inventoryAddress=inventory:50051
      -oamservaddr=0.0.0.0:2379
      -traceURL=fluent-bit:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:49:03 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:48:41 +0000
      Finished:     Tue, 03 Feb 2026 17:48:41 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/exporter/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/exporter/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      EN_LOKI_URL:  edgenode-observability-loki-gateway.orch-infra.svc.cluster.local:80
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nzcqk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-nzcqk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57m                default-scheduler  Successfully assigned orch-infra/exporter-785b7cb76d-d82pq to orch-tf
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-init
  Normal   Started    57m                kubelet            Started container istio-init
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-proxy
  Normal   Started    57m                kubelet            Started container istio-proxy
  Warning  Unhealthy  57m                kubelet            Startup probe failed: Get "http://10.42.65.209:15021/healthz/ready": dial tcp 10.42.65.209:15021: connect: connection refused
  Normal   Pulling    57m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/exporter:1.23.0"
  Normal   Pulled     57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/exporter:1.23.0" in 5.826s (5.826s including waiting). Image size: 13651502 bytes.
  Warning  BackOff    57m (x3 over 57m)  kubelet            Back-off restarting failed container exporter in pod exporter-785b7cb76d-d82pq_orch-infra(f8090d1f-7d8b-4e4f-b03e-a93be946982f)
  Normal   Pulled     57m (x2 over 57m)  kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/exporter:1.23.0" already present on machine
  Normal   Created    57m (x3 over 57m)  kubelet            Created container: exporter
  Normal   Started    57m (x3 over 57m)  kubelet            Started container exporter


Name:             host-manager-66544cc56d-q65fb
Namespace:        orch-infra
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:05 +0000
Labels:           app.kubernetes.io/instance=infra-managers
                  app.kubernetes.io/name=host-manager
                  pod-template-hash=66544cc56d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=host-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 98c09703eb96d8686dbb8b616ddaef0e96e50cedd32eeee7ac1aa77aeccd72f2
                  cni.projectcalico.org/podIP: 10.42.65.225/32
                  cni.projectcalico.org/podIPs: 10.42.65.225/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: host-manager
                  kubectl.kubernetes.io/default-logs-container: host-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.225
IPs:
  IP:           10.42.65.225
Controlled By:  ReplicaSet/host-manager-66544cc56d
Init Containers:
  istio-init:
    Container ID:  containerd://85a21fbe5bc2accf315abf77d2a6da7c6f02425fe6e8cf335e0c74c26d90628c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:10 +0000
      Finished:     Tue, 03 Feb 2026 17:53:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7qfks (ro)
  istio-proxy:
    Container ID:  containerd://6c0f0b130feec2d7bfb37be330b7427cbfe6fc1068b678c7b6f17121c75306b5
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      host-manager-66544cc56d-q65fb (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"hostmgr-api","containerPort":50001,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     host-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      host-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/host-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/host-manager/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/host-manager/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7qfks (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  host-manager:
    Container ID:  containerd://810211de823f8fa0969be4393c0ac7544b178eb7872b1b58a83ec01aae549631
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/hostmgr:1.24.1
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/hostmgr@sha256:a11bd867a323e31541ffeff297384f890966897d413cbafd2d53b36f547825bc
    Port:          50001/TCP
    Host Port:     0/TCP
    Args:
      -inventoryAddress
      inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress
      0.0.0.0:2379
      -globalLogLevel
      info
      -baseTimeDuration
      30
      -timeoutTimes
      3
      -enableAuth=true
      -enableTracing=false
      -invCacheStaleTimeout=5m
      -invCacheUuidEnable=true
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/host-manager/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/host-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
      ALLOW_MISSING_AUTH_CLIENTS:     
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7qfks (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-7qfks:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  53m   default-scheduler  Successfully assigned orch-infra/host-manager-66544cc56d-q65fb to orch-tf
  Normal   Pulled     53m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m   kubelet            Created container: istio-init
  Normal   Started    53m   kubelet            Started container istio-init
  Normal   Pulled     53m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m   kubelet            Created container: istio-proxy
  Normal   Started    52m   kubelet            Started container istio-proxy
  Warning  Unhealthy  52m   kubelet            Startup probe failed: Get "http://10.42.65.225:15021/healthz/ready": dial tcp 10.42.65.225:15021: connect: connection refused
  Normal   Pulling    52m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/hostmgr:1.24.1"
  Normal   Pulled     52m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/hostmgr:1.24.1" in 6.413s (6.413s including waiting). Image size: 15945849 bytes.
  Normal   Created    52m   kubelet            Created container: host-manager
  Normal   Started    52m   kubelet            Started container host-manager
  Warning  Unhealthy  52m   kubelet            Readiness probe failed: Get "http://10.42.65.225:15020/app-health/host-manager/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             init-amt-vault-job-j9mts
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:58 +0000
Labels:           app=mps
                  app.kubernetes.io/instance=infra-external
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mps
                  app.kubernetes.io/version=0.0.17
                  batch.kubernetes.io/controller-uid=6d034877-cb6b-4056-80db-e588e7347176
                  batch.kubernetes.io/job-name=init-amt-vault-job
                  controller-uid=6d034877-cb6b-4056-80db-e588e7347176
                  helm.sh/chart=mps-0.0.29
                  job-name=init-amt-vault-job
Annotations:      cni.projectcalico.org/containerID: e96576d34757d06c93dae6b6d8de12857a3472e92650cd02fc6fadd0fbe0e0f0
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  traffic.sidecar.istio.io/excludeInboundPorts: 4433
                  traffic.sidecar.istio.io/excludeOutboundPorts: 4433
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.240
IPs:
  IP:           10.42.65.240
Controlled By:  Job/init-amt-vault-job
Containers:
  amt:
    Container ID:  containerd://dce2bec9b5163438a1f528e2ca70c0890403c137979134cfd37158df4a81328b
    Image:         badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Image ID:      docker.io/badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      /amt.sh
      http://platform-keycloak.orch-platform:8080
      http://vault.orch-platform.svc:8200
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:11 +0000
      Finished:     Tue, 03 Feb 2026 17:54:12 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      ADMIN_USER:    admin
      ADMIN_PASS:    <set to the key 'admin-password' in secret 'platform-keycloak'>  Optional: false
      ADMIN_CLIENT:  system-client
    Mounts:
      /amt.sh from amt-vol (rw,path="amt.sh")
      /tmp from tmp-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4lqpb (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  amt-vol:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      amt-vault-script
    Optional:  false
  tmp-vol:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-4lqpb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  52m   default-scheduler  Successfully assigned orch-infra/init-amt-vault-job-j9mts to orch-tf
  Normal  Pulled     52m   kubelet            Container image "badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b" already present on machine
  Normal  Created    52m   kubelet            Created container: amt
  Normal  Started    52m   kubelet            Started container amt


Name:             inventory-64df587969-l5vlr
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:17 +0000
Labels:           app.kubernetes.io/instance=infra-core
                  app.kubernetes.io/name=inventory
                  pod-template-hash=64df587969
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=inventory
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 1c13e26388f7fa60a65015e794f05b6c2f91d5cf3285ca42287758e76ce0f4d7
                  cni.projectcalico.org/podIP: 10.42.65.208/32
                  cni.projectcalico.org/podIPs: 10.42.65.208/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: inventory
                  kubectl.kubernetes.io/default-logs-container: inventory
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.208
IPs:
  IP:           10.42.65.208
Controlled By:  ReplicaSet/inventory-64df587969
Init Containers:
  istio-init:
    Container ID:  containerd://1ac7258617400bc7c81a9b3bba7fd0609d9bc409b8fb65037403a5780232c0cf
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:22 +0000
      Finished:     Tue, 03 Feb 2026 17:48:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dg6t7 (ro)
  istio-proxy:
    Container ID:  containerd://cdd86f7513d85c32e21035a85b842bfc30753fd7b27ede61f5f81d8b4a898c1a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      inventory-64df587969-l5vlr (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"grpc","containerPort":50051,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     inventory
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      inventory
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/inventory
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/inventory/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/inventory/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dg6t7 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  inventory:
    Container ID:  containerd://48da1f288f59aee77cf39de1aba14aed8c95472297b6964feb072cb20adad17b
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/inventory:2.33.3
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/inventory@sha256:505bf91b396839f2e742364f879adc55217fc7ccbe9a8d234279b67f954aa61e
    Port:          50051/TCP
    Host Port:     0/TCP
    Args:
      -enableAuditing=false
      -enableAuth=false
      -enableTracing=false
      -globalLogLevel=info
      -oamServerAddress=0.0.0.0:2379
      -policyBundlePath=/rego/policy_bundle.tar.gz
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:42 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/inventory/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/inventory/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                <set to the key 'oidc_server_url' of config map 'keycloak-inventory'>                      Optional: false
      OIDC_TLS_INSECURE_SKIP_VERIFY:  <set to the key 'oidc_tls_insecure_skip_verify_value' of config map 'keycloak-inventory'>  Optional: false
      ALLOW_MISSING_AUTH_CLIENTS:     
      PGUSER:                         <set to the key 'PGUSER' in secret 'inventory-local-postgresql'>      Optional: false
      PGPASSWORD:                     <set to the key 'PGPASSWORD' in secret 'inventory-local-postgresql'>  Optional: false
      PGDATABASE:                     <set to the key 'PGDATABASE' in secret 'inventory-local-postgresql'>  Optional: false
      PGHOST:                         <set to the key 'PGHOST' in secret 'inventory-local-postgresql'>      Optional: false
      PGPORT:                         <set to the key 'PGPORT' in secret 'inventory-local-postgresql'>      Optional: false
      PGSSLMODE:                      disable
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dg6t7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-dg6t7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57m                default-scheduler  Successfully assigned orch-infra/inventory-64df587969-l5vlr to orch-tf
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-init
  Normal   Started    57m                kubelet            Started container istio-init
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-proxy
  Normal   Started    57m                kubelet            Started container istio-proxy
  Warning  Unhealthy  57m (x2 over 57m)  kubelet            Startup probe failed: Get "http://10.42.65.208:15021/healthz/ready": dial tcp 10.42.65.208:15021: connect: connection refused
  Normal   Pulling    57m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/inventory:2.33.3"
  Normal   Pulled     57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/inventory:2.33.3" in 9.086s (9.086s including waiting). Image size: 33088711 bytes.
  Normal   Created    57m                kubelet            Created container: inventory
  Normal   Started    57m                kubelet            Started container inventory


Name:             loki-backend-0
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=backend
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=loki-backend-7979db55c8
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-backend-0
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 4fc753d67f55aa1b93381fdb631b92b1db5c9a2a7a137b946890fdfc7a53db7a
                  cni.projectcalico.org/podIP: 10.42.65.134/32
                  cni.projectcalico.org/podIPs: 10.42.65.134/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 500m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 512Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.134
IPs:
  IP:           10.42.65.134
Controlled By:  StatefulSet/loki-backend
Init Containers:
  istio-init:
    Container ID:  containerd://f91d220c7709e8b2418c1cdd2a217e7c875ff1250d674d4082b14c4e56064764
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:48 +0000
      Finished:     Tue, 03 Feb 2026 17:35:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-87w5p (ro)
  istio-proxy:
    Container ID:  containerd://92723204f92656d3ff9f7aa45028d3537aa1499aae284c2a702f8ef4d0b6c75a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-backend-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki,loki-sc-rules
      GOMEMLIMIT:                    536870912 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-backend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/loki-backend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-87w5p (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://4d8731d7228d69ba687c21f894a7aa5e1caf661fef9c892f5f3d720d51a26d09
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=backend
      -legacy-read-mode=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:42 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:36:21 +0000
      Finished:     Tue, 03 Feb 2026 17:36:25 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /rules from sc-rules-volume (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-87w5p (ro)
  loki-sc-rules:
    Container ID:   containerd://abb3f9a4131ec2c9453448c53bc432141372332c10b6ad0c0de31caad3271380
    Image:          docker.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:       docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:                WATCH
      LABEL:                 loki_rule
      FOLDER:                /rules
      RESOURCE:              both
      WATCH_SERVER_TIMEOUT:  60
      WATCH_CLIENT_TIMEOUT:  60
      LOG_LEVEL:             INFO
    Mounts:
      /rules from sc-rules-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-87w5p (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  sc-rules-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-87w5p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-backend-1
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=backend
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=1
                  controller-revision-hash=loki-backend-7979db55c8
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-backend-1
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 4a68414fbb3b0b3addac6e1f0e71a27a20d4c8992fff62cef2bd1b5e6341cedf
                  cni.projectcalico.org/podIP: 10.42.65.135/32
                  cni.projectcalico.org/podIPs: 10.42.65.135/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 500m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 512Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.135
IPs:
  IP:           10.42.65.135
Controlled By:  StatefulSet/loki-backend
Init Containers:
  istio-init:
    Container ID:  containerd://a0b1c752159ac2dd62b35d45291cea778a69498d04de1918f18ed610c4b22a1e
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:47 +0000
      Finished:     Tue, 03 Feb 2026 17:35:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-htntn (ro)
  istio-proxy:
    Container ID:  containerd://8f7f59233a5936c75f09566f7daa85176be8f915fa415724fd274687048f88d0
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-backend-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki,loki-sc-rules
      GOMEMLIMIT:                    536870912 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-backend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/loki-backend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-htntn (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://fddc17186f78c8cdc64a4d15162a6c527aa5a4546c27fd9e181f7d659b711f8d
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=backend
      -legacy-read-mode=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:41 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
      Finished:     Tue, 03 Feb 2026 17:36:26 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /rules from sc-rules-volume (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-htntn (ro)
  loki-sc-rules:
    Container ID:   containerd://21410540d3643732cd9cc7ec35f3c7cc6f96e3ac7d3502c402b0b2760190992a
    Image:          docker.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:       docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:                WATCH
      LABEL:                 loki_rule
      FOLDER:                /rules
      RESOURCE:              both
      WATCH_SERVER_TIMEOUT:  60
      WATCH_CLIENT_TIMEOUT:  60
      LOG_LEVEL:             INFO
    Mounts:
      /rules from sc-rules-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-htntn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  sc-rules-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-htntn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-backend-2
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=backend
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=2
                  controller-revision-hash=loki-backend-7979db55c8
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-backend-2
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 4f718df9fb1a126e77025ee3264cacd11d8e4f0f14557e12391d2387757136c8
                  cni.projectcalico.org/podIP: 10.42.65.131/32
                  cni.projectcalico.org/podIPs: 10.42.65.131/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 10m
                  sidecar.istio.io/proxyCPULimit: 500m
                  sidecar.istio.io/proxyMemory: 32Mi
                  sidecar.istio.io/proxyMemoryLimit: 512Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.131
IPs:
  IP:           10.42.65.131
Controlled By:  StatefulSet/loki-backend
Init Containers:
  istio-init:
    Container ID:  containerd://410affddb89a140a68b324559713d80087d7c6f34b8d90cf15a65b3db540f18f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:46 +0000
      Finished:     Tue, 03 Feb 2026 17:35:46 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     10m
      memory:  32Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ktmj6 (ro)
  istio-proxy:
    Container ID:  containerd://8921ae417443f9d583217a66ea123de03622818dc9babf9a1a94346885edcca8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:51 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:      10m
      memory:   32Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-backend-2 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki,loki-sc-rules
      GOMEMLIMIT:                    536870912 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-backend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/loki-backend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ktmj6 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://d2927e6db3dfa0e8e0ef1975eebdae35b67881e30a58065253493502c6c017f7
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=backend
      -legacy-read-mode=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:41 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
      Finished:     Tue, 03 Feb 2026 17:36:26 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /rules from sc-rules-volume (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ktmj6 (ro)
  loki-sc-rules:
    Container ID:   containerd://69f34144058ddd2d5c3979dc3150e4bc1e17b4e8d426900d42446d40b16d4ba9
    Image:          docker.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:       docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:                WATCH
      LABEL:                 loki_rule
      FOLDER:                /rules
      RESOURCE:              both
      WATCH_SERVER_TIMEOUT:  60
      WATCH_CLIENT_TIMEOUT:  60
      LOG_LEVEL:             INFO
    Mounts:
      /rules from sc-rules-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ktmj6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  sc-rules-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-ktmj6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-read-66b8f4476d-56rzw
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=read
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  orchestrator/service=observability
                  pod-template-hash=66b8f4476d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 8613c77ab4be6bf4491610894c7a670c3eeed4be456814135c1490f140d9883b
                  cni.projectcalico.org/podIP: 10.42.65.130/32
                  cni.projectcalico.org/podIPs: 10.42.65.130/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 100m
                  sidecar.istio.io/proxyCPULimit: 750m
                  sidecar.istio.io/proxyMemory: 128Mi
                  sidecar.istio.io/proxyMemoryLimit: 256Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.130
IPs:
  IP:           10.42.65.130
Controlled By:  ReplicaSet/loki-read-66b8f4476d
Init Containers:
  istio-init:
    Container ID:  containerd://35e25b98cc3e4acfdfd82af5fdfa9aa1dde134e4feb4e24aae92548b091fd9e8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:45 +0000
      Finished:     Tue, 03 Feb 2026 17:35:46 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zqnck (ro)
  istio-proxy:
    Container ID:  containerd://07e71f9f043a3b03f9bbd30cebf8435a8054419e60f8a4a4a460711f68139b30
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:49 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-read-66b8f4476d-56rzw (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    268435456 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-read
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/loki-read
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zqnck (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://72b57b5d54bfbd5f2bddd779439cb712be98ef55681a3f4751b512c3398e120f
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=read
      -legacy-read-mode=false
      -common.compactor-grpc-address=loki-backend.orch-infra.svc.cluster.local:9095
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zqnck (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-zqnck:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-read-66b8f4476d-mkkmg
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=read
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  orchestrator/service=observability
                  pod-template-hash=66b8f4476d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 482790cd6067f10f49cc882a50e91b53cc19dab84688948c451b5fdc88ef47ff
                  cni.projectcalico.org/podIP: 10.42.65.143/32
                  cni.projectcalico.org/podIPs: 10.42.65.143/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 100m
                  sidecar.istio.io/proxyCPULimit: 750m
                  sidecar.istio.io/proxyMemory: 128Mi
                  sidecar.istio.io/proxyMemoryLimit: 256Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.143
IPs:
  IP:           10.42.65.143
Controlled By:  ReplicaSet/loki-read-66b8f4476d
Init Containers:
  istio-init:
    Container ID:  containerd://fe34de037c91edd21bb9f8cf3d08e1b7ff2b26b87fdbf2bc26a478b301eadd2f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:50 +0000
      Finished:     Tue, 03 Feb 2026 17:35:51 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hj557 (ro)
  istio-proxy:
    Container ID:  containerd://731a0ea2364fdea42596641512010cdf1d4cc511b2aec795b7f638680c129a8f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-read-66b8f4476d-mkkmg (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    268435456 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-read
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/loki-read
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hj557 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://d3c2077db974be324836fdde83672bf154ae29a9828e1b914c8684efcede7110
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=read
      -legacy-read-mode=false
      -common.compactor-grpc-address=loki-backend.orch-infra.svc.cluster.local:9095
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hj557 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-hj557:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-read-66b8f4476d-p77qc
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:39 +0000
Labels:           app.kubernetes.io/component=read
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  orchestrator/service=observability
                  pod-template-hash=66b8f4476d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 542278b258abc8b1cc32b5ba8b14d6c4f92d96e2f7ca7b1869a46ed1ffb37f42
                  cni.projectcalico.org/podIP: 10.42.65.139/32
                  cni.projectcalico.org/podIPs: 10.42.65.139/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 100m
                  sidecar.istio.io/proxyCPULimit: 750m
                  sidecar.istio.io/proxyMemory: 128Mi
                  sidecar.istio.io/proxyMemoryLimit: 256Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.139
IPs:
  IP:           10.42.65.139
Controlled By:  ReplicaSet/loki-read-66b8f4476d
Init Containers:
  istio-init:
    Container ID:  containerd://e5f34a8e29b95a43f6bee200c08da1b0542a746fee4022d225f639fc7a476239
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:49 +0000
      Finished:     Tue, 03 Feb 2026 17:35:49 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjgkr (ro)
  istio-proxy:
    Container ID:  containerd://a2e206a7364d2b87223e9b7d679fbf2d70826ed79e1dc6f544dc6f74a6c5338e
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:54 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-read-66b8f4476d-p77qc (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    268435456 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-read
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/loki-read
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjgkr (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://184fbeb191da3f37c8b68b21f510f7da8f7d78926e5b4d8c13f8945c5afeabd2
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=read
      -legacy-read-mode=false
      -common.compactor-grpc-address=loki-backend.orch-infra.svc.cluster.local:9095
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjgkr (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-vjgkr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-write-0
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:57 +0000
Labels:           app.kubernetes.io/component=write
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=loki-write-7dcd95d859
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-write-0
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 9e24ee7bf1360d617bb71fe5f0c6fb6a3840c40252d3b04a80fefb11c14bef18
                  cni.projectcalico.org/podIP: 10.42.65.170/32
                  cni.projectcalico.org/podIPs: 10.42.65.170/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 100m
                  sidecar.istio.io/proxyCPULimit: 1500m
                  sidecar.istio.io/proxyMemory: 128Mi
                  sidecar.istio.io/proxyMemoryLimit: 256Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.170
IPs:
  IP:           10.42.65.170
Controlled By:  StatefulSet/loki-write
Init Containers:
  istio-init:
    Container ID:  containerd://9d9ae630bf351c7af659ce7734c7cc447fd1f3d24b13d7379ba2aaf91c3528e4
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:06 +0000
      Finished:     Tue, 03 Feb 2026 17:36:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1500m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b9vjl (ro)
  istio-proxy:
    Container ID:  containerd://0abe9ccfe04deb88dc36fe329b7b24fce1d5f00b84f9a76f9c641968b9e9f24f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1500m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-write-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               2 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    268435456 (limits.memory)
      GOMAXPROCS:                    2 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-write
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/loki-write
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b9vjl (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://efdf0d8b34c2beaa5c56723aa8b4c9443c7cb216fa70f422fab5fc9a8b64f574
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=write
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b9vjl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-loki-write-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-b9vjl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-write-1
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:09 +0000
Labels:           app.kubernetes.io/component=write
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=1
                  controller-revision-hash=loki-write-7dcd95d859
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-write-1
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: e6b6062b335af90179f7df95698c76e2e10d2eb7ca1fc8ca7f64cb04530dd12b
                  cni.projectcalico.org/podIP: 10.42.65.175/32
                  cni.projectcalico.org/podIPs: 10.42.65.175/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 100m
                  sidecar.istio.io/proxyCPULimit: 1500m
                  sidecar.istio.io/proxyMemory: 128Mi
                  sidecar.istio.io/proxyMemoryLimit: 256Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.175
IPs:
  IP:           10.42.65.175
Controlled By:  StatefulSet/loki-write
Init Containers:
  istio-init:
    Container ID:  containerd://6fe325b6d96023115c711c936e5c60c533931fe63e15d6f22e881c3611af41f5
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:13 +0000
      Finished:     Tue, 03 Feb 2026 17:36:13 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1500m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t8hcn (ro)
  istio-proxy:
    Container ID:  containerd://c53518bd97635aa03ea3ce09848f7c03b3b8d9cc7cc7d888efd6a87ef2bbdf25
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1500m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-write-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               2 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    268435456 (limits.memory)
      GOMAXPROCS:                    2 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-write
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/loki-write
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t8hcn (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://a7bc6e932a2fefe8f67ad3161a93b842b9db8d1dfeb3d2be7a52eb20ec44fc48
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=write
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t8hcn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-loki-write-1
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-t8hcn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-write-2
Namespace:        orch-infra
Priority:         0
Service Account:  edgenode-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:12 +0000
Labels:           app.kubernetes.io/component=write
                  app.kubernetes.io/instance=edgenode-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=2
                  controller-revision-hash=loki-write-7dcd95d859
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-write-2
Annotations:      checksum/config: e4a84a90e775b37ee3b3cbf455d737646e823ae0f286d993b9b1d88f20220f87
                  cni.projectcalico.org/containerID: 49dd8e1f35ede1105d456f854b9f61bd0344dcb285dad072ab499445ba1ed418
                  cni.projectcalico.org/podIP: 10.42.65.177/32
                  cni.projectcalico.org/podIPs: 10.42.65.177/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 100m
                  sidecar.istio.io/proxyCPULimit: 1500m
                  sidecar.istio.io/proxyMemory: 128Mi
                  sidecar.istio.io/proxyMemoryLimit: 256Mi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.177
IPs:
  IP:           10.42.65.177
Controlled By:  StatefulSet/loki-write
Init Containers:
  istio-init:
    Container ID:  containerd://b1d3a94e083b74a5a84701b36ee5d4b30eaf5dc8194ba5a7db27e579c025947b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:16 +0000
      Finished:     Tue, 03 Feb 2026 17:36:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1500m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txfm6 (ro)
  istio-proxy:
    Container ID:  containerd://91cc2097105c9bbac031650f2b333049d54d7875201447b6bf7eb3c3df1db53f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1500m
      memory:  256Mi
    Requests:
      cpu:      100m
      memory:   128Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-write-2 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               2 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    268435456 (limits.memory)
      GOMAXPROCS:                    2 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-write
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/statefulsets/loki-write
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txfm6 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://966530a4dfcee92fed873e8bde63505544b60e5d6e41db9d62138bc8db85d8d4
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=write
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txfm6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-loki-write-2
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-txfm6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             maintenance-manager-6d5f4d99cb-gfvfq
Namespace:        orch-infra
Priority:         0
Service Account:  maintenance-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:02 +0000
Labels:           app.kubernetes.io/instance=infra-managers
                  app.kubernetes.io/name=maintenance-manager
                  pod-template-hash=6d5f4d99cb
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=maintenance-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: f578cd9228f63926b04070bcb0ec1014f1eac6265f9e084b86b6615b9c57c578
                  cni.projectcalico.org/podIP: 10.42.65.221/32
                  cni.projectcalico.org/podIPs: 10.42.65.221/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: maintenance-manager
                  kubectl.kubernetes.io/default-logs-container: maintenance-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.221
IPs:
  IP:           10.42.65.221
Controlled By:  ReplicaSet/maintenance-manager-6d5f4d99cb
Init Containers:
  istio-init:
    Container ID:  containerd://87a744b447ba7dcc23167ee3448275bf4839b139b196c6f61284baf21dc8224f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:07 +0000
      Finished:     Tue, 03 Feb 2026 17:53:07 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mpgx5 (ro)
  istio-proxy:
    Container ID:  containerd://6306aaacaa7afbb621e382e4fb5fbd03ee0563bfe260ba1af8ad4a3e0eeeb320
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:09 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      maintenance-manager-6d5f4d99cb-gfvfq (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"maintmgr-api","containerPort":50002,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     maintenance-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      maintenance-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/maintenance-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/maintenance-manager/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/maintenance-manager/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mpgx5 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  maintenance-manager:
    Container ID:  containerd://86d28f5504de3b6a914d364c64b395dcffd5be9ee748e18e0fda3988e1e60457
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/maintmgr:1.24.3
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/maintmgr@sha256:128a1c22a1ccf8d13beb779364f996a84de4ecd5acd89ad6dfb1750d55fefb71
    Port:          50002/TCP
    Host Port:     0/TCP
    Args:
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress=0.0.0.0:2379
      -globalLogLevel=info
      -enableAuth=true
      -invCacheStaleTimeout=10m
      -invCacheUuidEnable=true
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/maintenance-manager/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/maintenance-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mpgx5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-mpgx5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-infra/maintenance-manager-6d5f4d99cb-gfvfq to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-proxy
  Normal   Started    53m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x5 over 53m)  kubelet            Startup probe failed: Get "http://10.42.65.221:15021/healthz/ready": dial tcp 10.42.65.221:15021: connect: connection refused
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/maintmgr:1.24.3"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/maintmgr:1.24.3" in 6.521s (6.523s including waiting). Image size: 15911410 bytes.
  Normal   Created    52m                kubelet            Created container: maintenance-manager
  Normal   Started    52m                kubelet            Started container maintenance-manager
  Warning  Unhealthy  52m                kubelet            Readiness probe failed: HTTP probe failed with statuscode: 500


Name:             mps-758b476f77-5gcs4
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 18:00:02 +0000
Labels:           app=mps
                  app.kubernetes.io/instance=infra-external
                  app.kubernetes.io/name=mps
                  pod-template-hash=758b476f77
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mps
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 46deb46af9262c10ad5ee005bd30a3fde69c19552a5427383c78f3058d7ac10e
                  cni.projectcalico.org/podIP: 10.42.65.197/32
                  cni.projectcalico.org/podIPs: 10.42.65.197/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: mps
                  kubectl.kubernetes.io/default-logs-container: mps
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
                  traffic.sidecar.istio.io/excludeInboundPorts: 4433
                  traffic.sidecar.istio.io/excludeOutboundPorts: 4433
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.197
IPs:
  IP:           10.42.65.197
Controlled By:  ReplicaSet/mps-758b476f77
Init Containers:
  istio-init:
    Container ID:  containerd://2c5b7d7338e9dc3edb420cf012526302532de635ef48d9e757438a70fa391f9a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,4433,15020
      -o
      4433
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:00:05 +0000
      Finished:     Tue, 03 Feb 2026 18:00:05 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jx2z9 (ro)
  istio-proxy:
    Container ID:  containerd://9e24c0fc0f0c58732fdf15b9bd0d4f109c199fcd8f7f352e93fa6614f6e10ce9
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 18:00:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      mps-758b476f77-5gcs4 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"mps","containerPort":3000,"protocol":"TCP"}
                                         ,{"name":"mpsws","containerPort":4433,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     mps
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      mps
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/mps
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/mps/readyz":{"httpGet":{"path":"/api/v1/health","port":3000,"scheme":"HTTP"},"timeoutSeconds":12}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jx2z9 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  mps:
    Container ID:   containerd://b12ea46453d419362c691142c738d1ef0653e0c26d2b7e6732ceb75e8080c5ac
    Image:          docker.io/intel/oact-mps:v2.14.2
    Image ID:       docker.io/intel/oact-mps@sha256:577f837e29b251dbab2b51215b25db8eec91dbddb072b3fa2120fbcb15a58fe8
    Ports:          3000/TCP, 4433/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Tue, 03 Feb 2026 18:00:09 +0000
    Ready:          True
    Restart Count:  0
    Readiness:      http-get http://:15020/app-health/mps/readyz delay=0s timeout=12s period=12s #success=1 #failure=3
    Environment:
      STAKATER_VAULT_TOKEN_SECRET:  0fb09245e1e7007091aacd60c664b7f5f66f59f9
      MPS_LOG_LEVEL:                silly
      MPS_COMMON_NAME:              mps.cluster.onprem
      MPS_CONNECTION_STRING:        <set to the key 'connectionString' in secret 'mps'>  Optional: false
      MPS_VAULT_ADDRESS:            http://vault.orch-platform.svc:8200
      MPS_SECRETS_PATH:             secret/data
      MPS_PORT:                     4433
      MPS_WEB_AUTH_ENABLED:         false
      MPS_INSTANCE_NAME:             (v1:status.podIP)
      MPS_VAULT_TOKEN:              <set to the key 'vault-token' in secret 'vault-token'>  Optional: false
      MPS_JWT_SECRET:               notUsed
    Mounts:
      /mps/dist/middleware/custom from middleware-function (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jx2z9 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  middleware-function:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      mps-middleware-configmap
    Optional:  false
  kube-api-access-jx2z9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  46m   default-scheduler  Successfully assigned orch-infra/mps-758b476f77-5gcs4 to orch-tf
  Normal   Pulled     46m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    46m   kubelet            Created container: istio-init
  Normal   Started    46m   kubelet            Started container istio-init
  Normal   Pulled     46m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    46m   kubelet            Created container: istio-proxy
  Normal   Started    46m   kubelet            Started container istio-proxy
  Warning  Unhealthy  46m   kubelet            Startup probe failed: Get "http://10.42.65.197:15021/healthz/ready": dial tcp 10.42.65.197:15021: connect: connection refused
  Normal   Pulled     46m   kubelet            Container image "docker.io/intel/oact-mps:v2.14.2" already present on machine
  Normal   Created    46m   kubelet            Created container: mps
  Normal   Started    46m   kubelet            Started container mps
  Warning  Unhealthy  46m   kubelet            Readiness probe failed: HTTP probe failed with statuscode: 500


Name:             networking-manager-858b45b448-zgqgl
Namespace:        orch-infra
Priority:         0
Service Account:  networking-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:05 +0000
Labels:           app.kubernetes.io/instance=infra-managers
                  app.kubernetes.io/name=networking-manager
                  pod-template-hash=858b45b448
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=networking-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 11db63bca94ddfdb293a7a01998a896286c0ca121c39a707b6310c9d5e27500c
                  cni.projectcalico.org/podIP: 10.42.65.223/32
                  cni.projectcalico.org/podIPs: 10.42.65.223/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: networking-manager
                  kubectl.kubernetes.io/default-logs-container: networking-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.223
IPs:
  IP:           10.42.65.223
Controlled By:  ReplicaSet/networking-manager-858b45b448
Init Containers:
  istio-init:
    Container ID:  containerd://517d287a126561d84015630c86ea55a17a2c2bfd257fd5376bdee93b0528ac6f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:09 +0000
      Finished:     Tue, 03 Feb 2026 17:53:10 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-glb6f (ro)
  istio-proxy:
    Container ID:  containerd://c0f1a478723740afb4bb83553530c17d6e333bc29a5e080736e4e57688c287b3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      networking-manager-858b45b448-zgqgl (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":50003,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     networking-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      networking-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/networking-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/networking-manager/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/networking-manager/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-glb6f (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  networking-manager:
    Container ID:  containerd://2696c2417a82d6f183f62a773c6cb4d41cbd9a29f72ccca69ef88afb11232e61
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/netmgr:1.20.0
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/netmgr@sha256:cbc1f69e0afdcaa62d36cb696ce5633d81b4d665e3df26ac571a13c3cd297b25
    Port:          50003/TCP
    Host Port:     0/TCP
    Args:
      -enableTracing=false
      -globalLogLevel=info
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress=0.0.0.0:2379
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        10m
      memory:     16Mi
    Liveness:     http-get http://:15020/app-health/networking-manager/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/networking-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-glb6f (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-glb6f:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-infra/networking-manager-858b45b448-zgqgl to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-proxy
  Normal   Started    53m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x3 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.223:15021/healthz/ready": dial tcp 10.42.65.223:15021: connect: connection refused
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/netmgr:1.20.0"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/netmgr:1.20.0" in 6.365s (6.365s including waiting). Image size: 14781045 bytes.
  Normal   Created    52m                kubelet            Created container: networking-manager
  Normal   Started    52m                kubelet            Started container networking-manager


Name:             onboarding-manager-669695b5c4-6gsf9
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:44 +0000
Labels:           app.kubernetes.io/instance=infra-onboarding
                  app.kubernetes.io/name=onboarding-manager
                  pod-template-hash=669695b5c4
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=onboarding-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 2a7454eb03fd79f79158c7d576a0ac400e995338eb0c0cc7b1d88468ef3e4b6d
                  cni.projectcalico.org/podIP: 10.42.65.235/32
                  cni.projectcalico.org/podIPs: 10.42.65.235/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: onboarding-manager
                  kubectl.kubernetes.io/default-logs-container: onboarding-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.235
IPs:
  IP:           10.42.65.235
Controlled By:  ReplicaSet/onboarding-manager-669695b5c4
Init Containers:
  istio-init:
    Container ID:  containerd://ebf6eef9fab337349116accbc6c237f17e090b134bec4b6a68d0e91a406477cf
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:50 +0000
      Finished:     Tue, 03 Feb 2026 17:53:51 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pmxkj (ro)
  istio-proxy:
    Container ID:  containerd://22de120f37ca71c7439be729fe34e5e185eded33f692d0057b12f6e9fff6b47a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      onboarding-manager-669695b5c4-6gsf9 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"grpc-port","containerPort":50054,"protocol":"TCP"}
                                         ,{"name":"grpc-port-nio","containerPort":50055,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     onboarding-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      onboarding-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/onboarding-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/onboarding-manager/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/onboarding-manager/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pmxkj (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  onboarding-manager:
    Container ID:  containerd://61c1c5a0e6f0f1ff41b31260b885578997f5c61b8394fbde62c76c4da9d8516f
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/onboardingmgr:1.38.15
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/onboardingmgr@sha256:0b8ec24ea5ad17bfab49f9e390c7833f88b7272fee334d2b6128ae9c0b405533
    Ports:         50054/TCP, 50055/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      -disableCredentialsManagement=false
      -enableAuth=true
      -enableTracing=false
      -globalLogLevel=info
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress=0.0.0.0:2379
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
      -configFile=/etc/infra-config/config.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:18 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/onboarding-manager/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/onboarding-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MGR_HOST:                               (v1:status.podIP)
      TINKER_VERSION:                        1.20.0
      TINKER_ARTIFACT_NAME:                  edge-orch/infra/tinker-actions
      ONBMGR_PORT:                           50054
      VAULT_URL:                             http://vault.orch-platform.svc.cluster.local:8200
      VAULT_PKI_ROLE:                        orch-svc
      KEYCLOAK_URL:                          http://platform-keycloak.orch-platform.svc.cluster.local:8080
      KEYCLOAK_REALM:                        master
      OIDC_SERVER_URL:                       http://platform-keycloak.orch-platform.svc/realms/master
      EN_DKAMMODE:                           prod
      EN_USERNAME:                           user
      EN_PASSWORD:                           user
      OIDC_TLS_INSECURE_SKIP_VERIFY:         true
      ALLOW_MISSING_AUTH_CLIENTS:            cdn-boots
      DEFAULT_K8S_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      ONBOARDING_MANAGER_CLIENT_NAME:        host-manager-m2m-client
      TEMPLATE_CLIENT_SERVICE_ACCOUNT_USER:  service-account-en-m2m-template-client
      ONBOARDING_CREDENTIALS_SECRET_NAME:    host-manager-m2m-client-secret
      ONBOARDING_CREDENTIALS_SECRET_KEY:     client_secret
      EN_CREDENTIALS_PREFIX:                 edgenode-
      RSPROXY_ADDRESS:                       rs-proxy.orch-platform.svc.cluster.local:8081/
    Mounts:
      /etc/infra-config from infra-config-volume (ro)
      /etc/ssl/orch-ca-cert from orch-ca-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pmxkj (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  infra-config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      infra-config
    Optional:  false
  orch-ca-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  gateway-ca-cert
    Optional:    true
  kube-api-access-pmxkj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  52m   default-scheduler  Successfully assigned orch-infra/onboarding-manager-669695b5c4-6gsf9 to orch-tf
  Normal   Pulled     52m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m   kubelet            Created container: istio-init
  Normal   Started    52m   kubelet            Started container istio-init
  Normal   Pulled     52m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m   kubelet            Created container: istio-proxy
  Normal   Started    52m   kubelet            Started container istio-proxy
  Warning  Unhealthy  52m   kubelet            Startup probe failed: Get "http://10.42.65.235:15021/healthz/ready": dial tcp 10.42.65.235:15021: connect: connection refused
  Normal   Pulling    52m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/onboardingmgr:1.38.15"
  Normal   Pulled     52m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/onboardingmgr:1.38.15" in 12.456s (12.456s including waiting). Image size: 25531694 bytes.
  Normal   Created    51m   kubelet            Created container: onboarding-manager
  Normal   Started    51m   kubelet            Started container onboarding-manager


Name:             os-resource-manager-f8597bdff-s96d5
Namespace:        orch-infra
Priority:         0
Service Account:  os-resource-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:03 +0000
Labels:           app.kubernetes.io/instance=infra-managers
                  app.kubernetes.io/name=os-resource-manager
                  pod-template-hash=f8597bdff
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=os-resource-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 5c1bf2170ac9f3e4dbe72a36202540915f0ea4e3788e5d9ad0ba771e8f913129
                  cni.projectcalico.org/podIP: 10.42.65.222/32
                  cni.projectcalico.org/podIPs: 10.42.65.222/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: os-resource-manager
                  kubectl.kubernetes.io/default-logs-container: os-resource-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.222
IPs:
  IP:           10.42.65.222
Controlled By:  ReplicaSet/os-resource-manager-f8597bdff
Init Containers:
  istio-init:
    Container ID:  containerd://eea9200252a2a8e579322e2330331658745ab04229a5e346e6ef01343fefe175
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:07 +0000
      Finished:     Tue, 03 Feb 2026 17:53:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6f9c (ro)
  istio-proxy:
    Container ID:  containerd://dd00cd2c04cd8884cf2f581cd7b408cea5888f749f8e7b618bd5014b4a53b743
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      os-resource-manager-f8597bdff-s96d5 (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     os-resource-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      os-resource-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/os-resource-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/os-resource-manager/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/os-resource-manager/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6f9c (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  os-resource-manager:
    Container ID:  containerd://380eb1d9be80ea4a8d0ca40c48a2cabb6ba38d46a2befde1f987cf3232c822b7
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/osresourcemgr:0.20.5
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/osresourcemgr@sha256:0d10bc66e0aaaa731f64fa540d485f72e2ecee12f9a0ba7de7caf54fe5e2733b
    Port:          <none>
    Host Port:     <none>
    Args:
      --inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      --oamServerAddress=0.0.0.0:2379
      --osProfileRevision=0.10.2
      --autoProvisionEnabled=false
      --osSecurityFeatureEnable=false
      --enabledProfiles=ubuntu-22.04-lts-generic,microvisor-rt,microvisor-nonrt,ubuntu-22.04-lts-generic-ext,ubuntu-24.04-lts
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/os-resource-manager/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/os-resource-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      http_proxy:                     
      https_proxy:                    
      no_proxy:                       
      HTTP_PROXY:                     
      HTTPS_PROXY:                    
      NO_PROXY:                       
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc.cluster.local/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
      ALLOW_MISSING_AUTH_CLIENTS:     
      RSPROXY_ADDRESS:                rs-proxy.orch-platform.svc.cluster.local:8081/
      RSPROXY_FILES_ADDRESS:          rs-proxy-files.orch-platform.svc.cluster.local:8081/
      RS_EN_PROFILE_REPO:             edge-orch/en/files/os-profile/
      INVENTORY_TICKER_PERIOD:        12h
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6f9c (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-p6f9c:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-infra/os-resource-manager-f8597bdff-s96d5 to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-proxy
  Normal   Started    53m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x5 over 53m)  kubelet            Startup probe failed: Get "http://10.42.65.222:15021/healthz/ready": dial tcp 10.42.65.222:15021: connect: connection refused
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/osresourcemgr:0.20.5"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/osresourcemgr:0.20.5" in 6.286s (6.286s including waiting). Image size: 15009249 bytes.
  Normal   Created    52m                kubelet            Created container: os-resource-manager
  Normal   Started    52m                kubelet            Started container os-resource-manager


Name:             rps-5fbbcb4dbf-jrnnz
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 18:00:03 +0000
Labels:           app=rps
                  app.kubernetes.io/instance=infra-external
                  app.kubernetes.io/name=rps
                  pod-template-hash=5fbbcb4dbf
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=rps
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 57c56e9ec01fd049788c2dc0cc0994e2e1825327baf5abbe89da437576a73cf0
                  cni.projectcalico.org/podIP: 10.42.65.195/32
                  cni.projectcalico.org/podIPs: 10.42.65.195/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: rps
                  kubectl.kubernetes.io/default-logs-container: rps
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.195
IPs:
  IP:           10.42.65.195
Controlled By:  ReplicaSet/rps-5fbbcb4dbf
Init Containers:
  istio-init:
    Container ID:  containerd://896db9490053d0eedd1f405315b09e862ddb6cb0f68d315f0b0647fcea18cd85
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:00:04 +0000
      Finished:     Tue, 03 Feb 2026 18:00:05 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lg6hd (ro)
  istio-proxy:
    Container ID:  containerd://ca9ddb47091119012b450b3e294689a57cdfacbb90aadc51422419929f99c0fb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 18:00:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      rps-5fbbcb4dbf-jrnnz (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"rps","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"rpsweb","containerPort":8081,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     rps
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      rps
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/rps
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/rps/readyz":{"httpGet":{"path":"/api/v1/admin/health","port":8081,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lg6hd (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  rps:
    Container ID:   containerd://a1e5fbe485d35ce04ad81759115998f61093b70fb5b387204634caac345c9a77
    Image:          docker.io/intel/oact-rps:v2.26.0
    Image ID:       docker.io/intel/oact-rps@sha256:5d9cfcadefaf84fd78db3573a334a998ca4330802d59227c3499ec898c54726c
    Ports:          8080/TCP, 8081/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Tue, 03 Feb 2026 18:00:10 +0000
    Ready:          True
    Restart Count:  0
    Readiness:      http-get http://:15020/app-health/rps/readyz delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      STAKATER_VAULT_TOKEN_SECRET:  0fb09245e1e7007091aacd60c664b7f5f66f59f9
      RPS_LOG_LEVEL:                silly
      RPS_VAULT_ADDRESS:            http://vault.orch-platform.svc:8200
      RPS_SECRETS_PATH:             secret/data/
      RPS_MPS_SERVER:               http://mps:3000
      RPS_CONNECTION_STRING:        <set to the key 'connectionString' in secret 'rps'>     Optional: false
      RPS_VAULT_TOKEN:              <set to the key 'vault-token' in secret 'vault-token'>  Optional: false
      RPS_VAULT_ROLE:               orch-svc
    Mounts:
      /rps/dist/middleware/custom from middleware-function (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lg6hd (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  middleware-function:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rps-middleware-configmap
    Optional:  false
  kube-api-access-lg6hd:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  46m                default-scheduler  Successfully assigned orch-infra/rps-5fbbcb4dbf-jrnnz to orch-tf
  Normal   Pulled     46m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    46m                kubelet            Created container: istio-init
  Normal   Started    46m                kubelet            Started container istio-init
  Normal   Pulled     46m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    46m                kubelet            Created container: istio-proxy
  Normal   Started    46m                kubelet            Started container istio-proxy
  Warning  Unhealthy  46m (x2 over 46m)  kubelet            Startup probe failed: Get "http://10.42.65.195:15021/healthz/ready": dial tcp 10.42.65.195:15021: connect: connection refused
  Normal   Pulled     46m                kubelet            Container image "docker.io/intel/oact-rps:v2.26.0" already present on machine
  Normal   Created    46m                kubelet            Created container: rps
  Normal   Started    46m                kubelet            Started container rps


Name:             setup-databases-mps-wvq6q
Namespace:        orch-infra
Priority:         0
Service Account:  db-role-amt-sa
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:57 +0000
Labels:           batch.kubernetes.io/controller-uid=a938d90d-8be4-4c8b-81f9-27d36b9cbd01
                  batch.kubernetes.io/job-name=setup-databases-mps
                  controller-uid=a938d90d-8be4-4c8b-81f9-27d36b9cbd01
                  job-name=setup-databases-mps
Annotations:      cni.projectcalico.org/containerID: 0255474062c87fb23b42e6ec9d9ce7ed4d59f223ec787dc754bdbe4cdcdc9ac5
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.239
IPs:
  IP:           10.42.65.239
Controlled By:  Job/setup-databases-mps
Containers:
  db-setup:
    Container ID:  containerd://59a042a7b6dd34efd6816a41a474031089b4c5a7bdfa8a922e224e8aef351e04
    Image:         postgres:16.10-bookworm
    Image ID:      docker.io/library/postgres@sha256:38471f330eb885e04de130b768d6db4e10469e2311879c7e5c699f6d2d8a1c74
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      until psql -W -c "SELECT 1"; do
        echo "Waiting for PostgreSQL to be ready for mps user..."
        sleep 1
      done
      psql -W -f /mps/initMPS.sql
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:11 +0000
      Finished:     Tue, 03 Feb 2026 17:54:13 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      PGPASSWORD:  <set to the key 'PGPASSWORD' in secret 'mps-local-postgresql'>  Optional: false
      PGPORT:      <set to the key 'PGPORT' in secret 'mps-local-postgresql'>      Optional: false
      PGUSER:      <set to the key 'PGUSER' in secret 'mps-local-postgresql'>      Optional: false
      PGDATABASE:  <set to the key 'PGDATABASE' in secret 'mps-local-postgresql'>  Optional: false
      PGHOST:      <set to the key 'PGHOST' in secret 'mps-local-postgresql'>      Optional: false
    Mounts:
      /mps from mps-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-htvds (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  mps-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      mps-configmap
    Optional:  false
  kube-api-access-htvds:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  52m   default-scheduler  Successfully assigned orch-infra/setup-databases-mps-wvq6q to orch-tf
  Normal  Pulled     52m   kubelet            Container image "postgres:16.10-bookworm" already present on machine
  Normal  Created    52m   kubelet            Created container: db-setup
  Normal  Started    52m   kubelet            Started container db-setup


Name:             setup-databases-rps-8sx69
Namespace:        orch-infra
Priority:         0
Service Account:  db-role-amt-sa
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:58 +0000
Labels:           batch.kubernetes.io/controller-uid=610befc8-911d-4c13-9332-0f2c9b1e9327
                  batch.kubernetes.io/job-name=setup-databases-rps
                  controller-uid=610befc8-911d-4c13-9332-0f2c9b1e9327
                  job-name=setup-databases-rps
Annotations:      cni.projectcalico.org/containerID: 87cb356cb34d1a0a620f09b199483a384058439156a5fcd145f4dc96f7d317ba
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.241
IPs:
  IP:           10.42.65.241
Controlled By:  Job/setup-databases-rps
Containers:
  db-setup:
    Container ID:  containerd://0d6dd2f1832a5ac8f2db1ec3800023e410a77f4ae68b1755dff03a20d0ddff0f
    Image:         postgres:16.10-bookworm
    Image ID:      docker.io/library/postgres@sha256:38471f330eb885e04de130b768d6db4e10469e2311879c7e5c699f6d2d8a1c74
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      until psql -W -c "SELECT 1"; do
        echo "Waiting for PostgreSQL to be ready for RPS user..."
        sleep 1
      done
      psql -W -f /rps/init.sql
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:11 +0000
      Finished:     Tue, 03 Feb 2026 17:54:15 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      PGPASSWORD:  <set to the key 'PGPASSWORD' in secret 'rps-local-postgresql'>  Optional: false
      PGPORT:      <set to the key 'PGPORT' in secret 'rps-local-postgresql'>      Optional: false
      PGUSER:      <set to the key 'PGUSER' in secret 'rps-local-postgresql'>      Optional: false
      PGDATABASE:  <set to the key 'PGDATABASE' in secret 'rps-local-postgresql'>  Optional: false
      PGHOST:      <set to the key 'PGHOST' in secret 'rps-local-postgresql'>      Optional: false
    Mounts:
      /rps from rps-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n2hb6 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  rps-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rps-configmap
    Optional:  false
  kube-api-access-n2hb6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  52m   default-scheduler  Successfully assigned orch-infra/setup-databases-rps-8sx69 to orch-tf
  Normal  Pulled     52m   kubelet            Container image "postgres:16.10-bookworm" already present on machine
  Normal  Created    52m   kubelet            Created container: db-setup
  Normal  Started    52m   kubelet            Started container db-setup


Name:             telemetry-manager-645d95768f-kc9pr
Namespace:        orch-infra
Priority:         0
Service Account:  telemetry-manager
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:05 +0000
Labels:           app.kubernetes.io/instance=infra-managers
                  app.kubernetes.io/name=telemetry-manager
                  pod-template-hash=645d95768f
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=telemetry-manager
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 18c42b31cf47e5de371fc7d02f02ece6e17e796a072aa21ebb640c78893e577a
                  cni.projectcalico.org/podIP: 10.42.65.227/32
                  cni.projectcalico.org/podIPs: 10.42.65.227/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: telemetry-manager
                  kubectl.kubernetes.io/default-logs-container: telemetry-manager
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.227
IPs:
  IP:           10.42.65.227
Controlled By:  ReplicaSet/telemetry-manager-645d95768f
Init Containers:
  istio-init:
    Container ID:  containerd://af8f17bfd19d555971cfb745f6f59d3b652baff641511377fa7f6a92289a2a73
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:10 +0000
      Finished:     Tue, 03 Feb 2026 17:53:11 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lb9fb (ro)
  istio-proxy:
    Container ID:  containerd://184dca615490331c9735381575d472567f593b23f6bffd981b56ea244c4875d8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      telemetry-manager-645d95768f-kc9pr (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"telmgr-api","containerPort":50004,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     telemetry-manager
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      telemetry-manager
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/telemetry-manager
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/telemetry-manager/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/telemetry-manager/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lb9fb (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  telemetry-manager:
    Container ID:  containerd://57e92683707090526436c7d11f281ae518daeddbde612b306fc4aa09433a2732
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/telemetrymgr:1.24.1
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/telemetrymgr@sha256:915edeca596a54a88c5ef9d47909193cb951735c0fdf1002f8b6b98c88093e8c
    Port:          50004/TCP
    Host Port:     0/TCP
    Args:
      -ServerAddress=0.0.0.0:50004
      -enableAuth=true
      -enableTracing=false
      -enableVal=true
      -globalLogLevel=info
      -invCacheStaleTimeout=5m
      -invCacheUuidEnable=true
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -oamServerAddress=0.0.0.0:2379
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/telemetry-manager/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/telemetry-manager/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_TLS_INSECURE_SKIP_VERIFY:  true
      ALLOW_MISSING_AUTH_CLIENTS:     
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lb9fb (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-lb9fb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  53m                default-scheduler  Successfully assigned orch-infra/telemetry-manager-645d95768f-kc9pr to orch-tf
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-init
  Normal   Started    53m                kubelet            Started container istio-init
  Normal   Pulled     53m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m                kubelet            Created container: istio-proxy
  Normal   Started    53m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x4 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.227:15021/healthz/ready": dial tcp 10.42.65.227:15021: connect: connection refused
  Normal   Pulling    52m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/telemetrymgr:1.24.1"
  Normal   Pulled     52m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/telemetrymgr:1.24.1" in 6.885s (6.885s including waiting). Image size: 15828177 bytes.
  Normal   Created    52m                kubelet            Created container: telemetry-manager
  Normal   Started    52m                kubelet            Started container telemetry-manager
  Warning  Unhealthy  52m                kubelet            Readiness probe failed: Get "http://10.42.65.227:15020/app-health/telemetry-manager/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


Name:             tenant-controller-6bd549f749-68rtl
Namespace:        orch-infra
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:48:17 +0000
Labels:           app.kubernetes.io/instance=infra-core
                  app.kubernetes.io/name=tenant-controller
                  pod-template-hash=6bd549f749
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=tenant-controller
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: e292fd9314a1f0bf6e683e7cf4f7b27a3702415ec6deadc788717e3de6947db6
                  cni.projectcalico.org/podIP: 10.42.65.207/32
                  cni.projectcalico.org/podIPs: 10.42.65.207/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: tenant-controller
                  kubectl.kubernetes.io/default-logs-container: tenant-controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.207
IPs:
  IP:           10.42.65.207
Controlled By:  ReplicaSet/tenant-controller-6bd549f749
Init Containers:
  istio-init:
    Container ID:  containerd://d0da218f791be189af82c82f713555f7548767e6af29a96ca834cf83574696d9
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:48:20 +0000
      Finished:     Tue, 03 Feb 2026 17:48:21 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kcrsf (ro)
  istio-proxy:
    Container ID:  containerd://1ddc45efb366082b551aaf1f1ccf264717458a2206d76d1e08bdf505cd471f29
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      tenant-controller-6bd549f749-68rtl (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     tenant-controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      tenant-controller
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/tenant-controller
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/tenant-controller/livez":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1},"/app-health/tenant-controller/readyz":{"grpc":{"port":2379,"service":""},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kcrsf (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  tenant-controller:
    Container ID:  containerd://923a6ba02fa2b4de8f710c2baafb9e9ebda24968464207ed887e0f9990a725c2
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/infra/tenant-controller:0.24.2
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/infra/tenant-controller@sha256:2f2291810cf9397a143dbc062f880b7792c7d564ee16e782156808c99992ca25
    Port:          <none>
    Host Port:     <none>
    Args:
      -disableCredentialsManagement=false
      -enableTracing=false
      -globalLogLevel=info
      -initResourcesDefinitionPath=/configuration/default/resources.json
      -inventoryAddress=inventory.orch-infra.svc.cluster.local:50051
      -lenovoResourcesDefinitionPath=/configuration/default/resources-lenovo.json
      -oamServerAddress=0.0.0.0:2379
      -traceURL=orchestrator-observability-opentelemetry-collector.orch-platform.svc:4318
    State:          Running
      Started:      Tue, 03 Feb 2026 17:49:01 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:48:44 +0000
      Finished:     Tue, 03 Feb 2026 17:48:46 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      10m
      memory:   16Mi
    Liveness:   http-get http://:15020/app-health/tenant-controller/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/tenant-controller/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      VAULT_URL:       http://vault.orch-platform.svc.cluster.local:8200
      VAULT_PKI_ROLE:  orch-svc
    Mounts:
      /configuration/default/ from init-resources (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kcrsf (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  init-resources:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tc-init-resources
    Optional:  false
  kube-api-access-kcrsf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57m                default-scheduler  Successfully assigned orch-infra/tenant-controller-6bd549f749-68rtl to orch-tf
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-init
  Normal   Started    57m                kubelet            Started container istio-init
  Normal   Pulled     57m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    57m                kubelet            Created container: istio-proxy
  Normal   Started    57m                kubelet            Started container istio-proxy
  Warning  Unhealthy  57m (x3 over 57m)  kubelet            Startup probe failed: Get "http://10.42.65.207:15021/healthz/ready": dial tcp 10.42.65.207:15021: connect: connection refused
  Normal   Pulling    57m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/tenant-controller:0.24.2"
  Normal   Pulled     57m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/tenant-controller:0.24.2" in 6.396s (6.396s including waiting). Image size: 23821362 bytes.
  Warning  BackOff    57m (x2 over 57m)  kubelet            Back-off restarting failed container tenant-controller in pod tenant-controller-6bd549f749-68rtl_orch-infra(c40b35e9-03a5-41df-8ab6-b13c89d91817)
  Normal   Pulled     57m (x2 over 57m)  kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/infra/tenant-controller:0.24.2" already present on machine
  Normal   Created    57m (x3 over 57m)  kubelet            Created container: tenant-controller
  Normal   Started    57m (x3 over 57m)  kubelet            Started container tenant-controller


Name:             tinkerbell-f8678998d-xm5dr
Namespace:        orch-infra
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:54:01 +0000
Labels:           app=tink-stack
                  pod-template-hash=f8678998d
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=tink-stack
                  service.istio.io/canonical-revision=latest
Annotations:      chart-version: 2.13.0
                  checksum/config: cb0f047b326e6b9d8e2e4c0728f94d2a027f32ba9a732f5ee284f382b71e946a
                  cni.projectcalico.org/containerID: 0f99749b1ca723d9554d7637f79cac931c05ea998af69c562cc414cbc093c546
                  cni.projectcalico.org/podIP: 10.42.65.244/32
                  cni.projectcalico.org/podIPs: 10.42.65.244/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: tinkerbell
                  kubectl.kubernetes.io/default-logs-container: tinkerbell
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.244
IPs:
  IP:           10.42.65.244
Controlled By:  ReplicaSet/tinkerbell-f8678998d
Init Containers:
  istio-init:
    Container ID:  containerd://fc0bd330de23b74f045201c431b8b5917a88a234a1452a15d5ae4fc583a2497d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:54:08 +0000
      Finished:     Tue, 03 Feb 2026 17:54:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6b5hx (ro)
  istio-proxy:
    Container ID:  containerd://bbaf5437700158427799275c186f755db50e0445287cbc4224b0c10fea682368
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:13 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      tinkerbell-f8678998d-xm5dr (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"tink-grpc","containerPort":42113,"protocol":"TCP"}
                                         ,{"name":"hook-http","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     tinkerbell
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      tinkerbell
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/tinkerbell
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/tinkerbell/livez":{"httpGet":{"path":"/boot.ipxe","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/tinkerbell/readyz":{"httpGet":{"path":"/boot.ipxe","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6b5hx (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  tinkerbell:
    Container ID:  containerd://915acee3ae7dda6c476f291e52a2a1748dde5c7d4e05754a3b36b096b555a1c7
    Image:         nginxinc/nginx-unprivileged:alpine3.22
    Image ID:      docker.io/nginxinc/nginx-unprivileged@sha256:5aea7cc516b419e3526f47dd1531be31a56a046cfe44754d94f9383e13e2ee99
    Ports:         42113/TCP, 8080/TCP
    Host Ports:    0/TCP, 0/TCP
    Command:
      /bin/sh
      -xeuc
    Args:
      POD_NAMESERVER=$(awk '/nameserver/ {print $2}' /etc/resolv.conf) \
        envsubst '$POD_NAMESERVER' \
        </tmp-nginx/nginx.conf.template \
        >/etc/nginx/nginx.conf
      
      envsubst '$DOWNLOAD_URL $GRPC_AUTHORITY $TINK_WORKER_IMAGE $ADDITIONAL_KERNEL_ARGS' \
        </tmp-boot-ipxe/boot.ipxe.template \
        >/usr/share/nginx/html/boot.ipxe
      
      exec nginx -g 'daemon off;'
      
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:21 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      200m
      memory:   256Mi
    Liveness:   http-get http://:15020/app-health/tinkerbell/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/tinkerbell/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DOWNLOAD_URL:            https://tinkerbell-nginx.cluster.onprem/tink-stack
      GRPC_AUTHORITY:          127.0.0.1:42113
      TINK_WORKER_IMAGE:       quay.io/tinkerbell/tink-worker:v0.10.0
      ADDITIONAL_KERNEL_ARGS:  http_proxy= https_proxy= no_proxy= HTTP_PROXY= HTTPS_PROXY= NO_PROXY= DEBUG=false TIMEOUT=120s syslog_host=127.0.0.1
    Mounts:
      /tmp-boot-ipxe from boot-ipxe (ro)
      /tmp-nginx from nginx-conf (ro)
      /usr/share/nginx/html from disk-pvc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6b5hx (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  disk-pvc:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  dkam-tink-shared-pvc
    ReadOnly:   false
  nginx-conf:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nginx-conf
    Optional:  false
  boot-ipxe:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      boot-ipxe
    Optional:  false
  kube-api-access-6b5hx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  52m                default-scheduler  Successfully assigned orch-infra/tinkerbell-f8678998d-xm5dr to orch-tf
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-init
  Normal   Started    52m                kubelet            Started container istio-init
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-proxy
  Normal   Started    52m                kubelet            Started container istio-proxy
  Warning  Unhealthy  51m (x3 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.244:15021/healthz/ready": dial tcp 10.42.65.244:15021: connect: connection refused
  Normal   Pulling    51m                kubelet            Pulling image "nginxinc/nginx-unprivileged:alpine3.22"
  Normal   Pulled     51m                kubelet            Successfully pulled image "nginxinc/nginx-unprivileged:alpine3.22" in 1.542s (1.542s including waiting). Image size: 22634779 bytes.
  Normal   Created    51m                kubelet            Created container: tinkerbell
  Normal   Started    51m                kubelet            Started container tinkerbell


Name:             tinkerbell-tink-controller-6bc9d4859b-lz2vj
Namespace:        orch-infra
Priority:         0
Service Account:  tinkerbell-tink-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:46 +0000
Labels:           app=tinkerbell-tink-controller
                  pod-template-hash=6bc9d4859b
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=tinkerbell-tink-controller
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: b2999c1c98d02ae6e3a7bbe381eba615ec5184cff521b57f41abffaeac2a12d3
                  cni.projectcalico.org/podIP: 10.42.65.236/32
                  cni.projectcalico.org/podIPs: 10.42.65.236/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: tinkerbell-tink-controller
                  kubectl.kubernetes.io/default-logs-container: tinkerbell-tink-controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.236
IPs:
  IP:           10.42.65.236
Controlled By:  ReplicaSet/tinkerbell-tink-controller-6bc9d4859b
Init Containers:
  istio-init:
    Container ID:  containerd://a819422f40e4000c0a37fab5fb8ba2853d5e76163266e08f0be83c433876dd36
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:56 +0000
      Finished:     Tue, 03 Feb 2026 17:53:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6x9nx (ro)
  istio-proxy:
    Container ID:  containerd://936b5deffcd93518376d1f52f0aaa292cfa5443395ca037309d6ff2e86f82fbe
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      tinkerbell-tink-controller-6bc9d4859b-lz2vj (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     tinkerbell-tink-controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      tinkerbell-tink-controller
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/tinkerbell-tink-controller
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6x9nx (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  tinkerbell-tink-controller:
    Container ID:   containerd://8b534f8a7888383f890b1f9628d6e0a7bb07ffe03a12ad24495bb4a59d71b1b6
    Image:          quay.io/tinkerbell/tink-controller:v0.10.0
    Image ID:       quay.io/tinkerbell/tink-controller@sha256:36b5f00eabeeac416fa0467554f46a2b2f01593387113d845b15718625b344c8
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        10m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6x9nx (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-6x9nx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  52m                default-scheduler  Successfully assigned orch-infra/tinkerbell-tink-controller-6bc9d4859b-lz2vj to orch-tf
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-init
  Normal   Started    52m                kubelet            Started container istio-init
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-proxy
  Normal   Started    52m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x6 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.236:15021/healthz/ready": dial tcp 10.42.65.236:15021: connect: connection refused
  Normal   Pulling    52m                kubelet            Pulling image "quay.io/tinkerbell/tink-controller:v0.10.0"
  Normal   Pulled     51m                kubelet            Successfully pulled image "quay.io/tinkerbell/tink-controller:v0.10.0" in 8.687s (8.687s including waiting). Image size: 29231034 bytes.
  Normal   Created    51m                kubelet            Created container: tinkerbell-tink-controller
  Normal   Started    51m                kubelet            Started container tinkerbell-tink-controller


Name:             tinkerbell-tink-server-7fd4d778bb-cntzg
Namespace:        orch-infra
Priority:         0
Service Account:  tinkerbell-tink-server
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:53:44 +0000
Labels:           app=tinkerbell-tink-server
                  pod-template-hash=7fd4d778bb
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=tinkerbell-tink-server
                  service.istio.io/canonical-revision=latest
                  stack=tinkerbell
Annotations:      cni.projectcalico.org/containerID: 4f5924f48647ab974204e8f41ded2e637a084f0272e6e8215151ac0928035d71
                  cni.projectcalico.org/podIP: 10.42.65.234/32
                  cni.projectcalico.org/podIPs: 10.42.65.234/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: server
                  kubectl.kubernetes.io/default-logs-container: server
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.234
IPs:
  IP:           10.42.65.234
Controlled By:  ReplicaSet/tinkerbell-tink-server-7fd4d778bb
Init Containers:
  istio-init:
    Container ID:  containerd://7d4960f532a45955973ca14e00d57bd91aff8092b1c9e08287a3bffacc33b35c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:53:50 +0000
      Finished:     Tue, 03 Feb 2026 17:53:50 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ztx5d (ro)
  istio-proxy:
    Container ID:  containerd://738a481642d10ce35857d5d35e2da4e898005a1dc5fa01dc956ca51a15228c21
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:53:59 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      tinkerbell-tink-server-7fd4d778bb-cntzg (v1:metadata.name)
      POD_NAMESPACE:                 orch-infra (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"tink-grpc","containerPort":42113,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     server
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      tinkerbell-tink-server
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-infra/deployments/tinkerbell-tink-server
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/server/livez":{"httpGet":{"path":"/healthz","port":42114,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/server/readyz":{"httpGet":{"path":"/healthz","port":42114,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ztx5d (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  server:
    Container ID:  containerd://f4f46c735ec42ea80ba48460037234ae5f56e1f2822328c98be35aea5f498b13
    Image:         quay.io/tinkerbell/tink:v0.10.0
    Image ID:      quay.io/tinkerbell/tink@sha256:c8457c45007497f3dbe2d6828b488d904e5e7b0e2e9d4b23df968d9da70ac979
    Port:          42113/TCP
    Host Port:     0/TCP
    Args:
      --backend=kubernetes
    State:          Running
      Started:      Tue, 03 Feb 2026 17:54:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        10m
      memory:     64Mi
    Liveness:     http-get http://:15020/app-health/server/livez delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/server/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ztx5d (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-ztx5d:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  52m                default-scheduler  Successfully assigned orch-infra/tinkerbell-tink-server-7fd4d778bb-cntzg to orch-tf
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-init
  Normal   Started    52m                kubelet            Started container istio-init
  Normal   Pulled     52m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    52m                kubelet            Created container: istio-proxy
  Normal   Started    52m                kubelet            Started container istio-proxy
  Warning  Unhealthy  52m (x2 over 52m)  kubelet            Startup probe failed: Get "http://10.42.65.234:15021/healthz/ready": dial tcp 10.42.65.234:15021: connect: connection refused
  Warning  Unhealthy  52m                kubelet            Startup probe failed: HTTP probe failed with statuscode: 503
  Normal   Pulling    52m                kubelet            Pulling image "quay.io/tinkerbell/tink:v0.10.0"
  Normal   Pulled     52m                kubelet            Successfully pulled image "quay.io/tinkerbell/tink:v0.10.0" in 9.848s (9.848s including waiting). Image size: 29318258 bytes.
  Normal   Created    52m                kubelet            Created container: server
  Normal   Started    51m                kubelet            Started container server


Name:             adm-secret-c2deb4c4c6-7plnl
Namespace:        orch-platform
Priority:         0
Service Account:  adm-secret
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:52 +0000
Labels:           batch.kubernetes.io/controller-uid=44e0991d-3314-4e3f-96ad-4c37951ad37a
                  batch.kubernetes.io/job-name=adm-secret-c2deb4c4c6
                  controller-uid=44e0991d-3314-4e3f-96ad-4c37951ad37a
                  job-name=adm-secret-c2deb4c4c6
Annotations:      cni.projectcalico.org/containerID: f9c869f526c3d73a4a6beb534fa55680bb2df3a20cc17e1fbf1567c364ae754a
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
IP:               10.42.65.199
IPs:
  IP:           10.42.65.199
Controlled By:  Job/adm-secret-c2deb4c4c6
Containers:
  adm-secret:
    Container ID:    containerd://889fa89f78297e9052fac6b9e1719016cda74c3ec7ff7f89f8d1e32e38a78512
    Image:           alpine/kubectl:1.34.1
    Image ID:        docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
      /tmp/bin/run.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:55 +0000
      Finished:     Tue, 03 Feb 2026 17:47:58 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      VAULT_KEYS:          <set to the key 'vault-keys' in secret 'vault-keys'>              Optional: false
      APP_GITEA_USER:      <set to the key 'username' in secret 'app-gitea-credential'>      Optional: false
      APP_GITEA_PASS:      <set to the key 'password' in secret 'app-gitea-credential'>      Optional: false
      CLUSTER_GITEA_USER:  <set to the key 'username' in secret 'cluster-gitea-credential'>  Optional: false
      CLUSTER_GITEA_PASS:  <set to the key 'password' in secret 'cluster-gitea-credential'>  Optional: false
      SEC_NAMESPACE:       orch-app
    Mounts:
      /tmp/bin from script (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jrwll (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  script:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      adm-secret
    Optional:  false
  orch-svc-token:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  orch-svc-token
    Optional:    false
  kube-api-access-jrwll:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  58m   default-scheduler  Successfully assigned orch-platform/adm-secret-c2deb4c4c6-7plnl to orch-tf
  Normal  Pulled     58m   kubelet            Container image "alpine/kubectl:1.34.1" already present on machine
  Normal  Created    58m   kubelet            Created container: adm-secret
  Normal  Started    58m   kubelet            Started container adm-secret


Name:             component-status-8547678557-wgkfq
Namespace:        orch-platform
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:42 +0000
Labels:           app.kubernetes.io/instance=component-status
                  app.kubernetes.io/name=component-status
                  pod-template-hash=8547678557
Annotations:      checksum/config: f5437bbab637187c58910f6be3cc620a026d5d1dfcda033d86da266270f7d08b
                  cni.projectcalico.org/containerID: faf2755e351adc6960ade16a49c9813fd9e16fa632eaf91422362753eb622ad4
                  cni.projectcalico.org/podIP: 10.42.65.198/32
                  cni.projectcalico.org/podIPs: 10.42.65.198/32
                  sidecar.istio.io/inject: false
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.198
IPs:
  IP:           10.42.65.198
Controlled By:  ReplicaSet/component-status-8547678557
Containers:
  component-status:
    Container ID:    containerd://3ba85cc38ee626a1084fe7a52dd770a78e0c416924f1ecffadff3ac315d73428
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/common/component-status:26.0.3
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/common/component-status@sha256:b553c641a498685b17635151db272f015652f6210f071f5a2037745dd65a8a99
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:47:46 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     100m
      memory:  64Mi
    Requests:
      cpu:      50m
      memory:   32Mi
    Liveness:   http-get http://:http/healthz delay=10s timeout=5s period=30s #success=1 #failure=3
    Readiness:  http-get http://:http/readyz delay=5s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PORT:         8080
      CONFIG_PATH:  /etc/component-status/config.yaml
    Mounts:
      /etc/component-status from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zhx65 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      component-status-config
    Optional:  false
  kube-api-access-zhx65:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  58m   default-scheduler  Successfully assigned orch-platform/component-status-8547678557-wgkfq to orch-tf
  Normal  Pulling    58m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/common/component-status:26.0.3"
  Normal  Pulled     58m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/common/component-status:26.0.3" in 2.069s (2.069s including waiting). Image size: 3580443 bytes.
  Normal  Created    58m   kubelet            Created container: component-status
  Normal  Started    58m   kubelet            Started container component-status


Name:             keycloak-config-cli-tt6qn
Namespace:        orch-platform
Priority:         0
Service Account:  keycloak-config-cli
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 18:34:32 +0000
Labels:           batch.kubernetes.io/controller-uid=3b315d87-603f-477f-aed6-6074667147dc
                  batch.kubernetes.io/job-name=keycloak-config-cli
                  controller-uid=3b315d87-603f-477f-aed6-6074667147dc
                  job-name=keycloak-config-cli
                  sidecar.istio.io/inject=false
Annotations:      cni.projectcalico.org/containerID: aa81baf28bb13701ba7e19dbc0b3353af6179c2cbf9bf35df1415c4a42351924
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.240
IPs:
  IP:           10.42.65.240
Controlled By:  Job/keycloak-config-cli
Init Containers:
  wait-for-keycloak:
    Container ID:    containerd://bb7cd33114d99a55a95a37355372843d81ef87ee7be13fd1039379b8c9a34e46
    Image:           busybox:1.36
    Image ID:        docker.io/library/busybox@sha256:b9598f8c98e24d0ad42c1742c32516772c3aa2151011ebaf639089bd18c605b8
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      sh
      -c
      set -e
      echo "Waiting for Keycloak to be ready..."
      MAX_ATTEMPTS=60
      ATTEMPT=0
      while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
        if timeout 2 sh -c "echo '' | nc -w1 platform-keycloak.orch-platform.svc 8080" >/dev/null 2>&1; then
          echo "Keycloak port 8080 is open!"
          exit 0
        fi
        ATTEMPT=$((ATTEMPT + 1))
        echo "Attempt $ATTEMPT/$MAX_ATTEMPTS - Waiting for Keycloak..."
        sleep 5
      done
      echo "WARNING: Keycloak did not respond, but proceeding anyway..."
      exit 0
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:34:33 +0000
      Finished:     Tue, 03 Feb 2026 18:34:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:        50m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24n79 (ro)
Containers:
  keycloak-config-cli:
    Container ID:    containerd://12ffabbe573d05d2b35c75657d733a401a72633e5e529bacb71184beb362813c
    Image:           docker.io/adorsys/keycloak-config-cli:6.4.0-26
    Image ID:        docker.io/adorsys/keycloak-config-cli@sha256:44fcacaba522c159f4bb23a6bef0dd9e82291c5c84be3ed0ada77f6ff4663fd4
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Terminated
      Reason:        Completed
      Exit Code:     0
      Started:       Tue, 03 Feb 2026 18:34:35 +0000
      Finished:      Tue, 03 Feb 2026 18:34:50 +0000
    Ready:           False
    Restart Count:   0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     200m
      memory:  512Mi
    Environment:
      KEYCLOAK_URL:                        http://platform-keycloak.orch-platform.svc.cluster.local/
      KEYCLOAK_USER:                       <set to the key 'username' in secret 'platform-keycloak'>  Optional: false
      KEYCLOAK_PASSWORD:                   <set to the key 'password' in secret 'platform-keycloak'>  Optional: false
      KEYCLOAK_AVAILABILITYCHECK_ENABLED:  true
      KEYCLOAK_AVAILABILITYCHECK_TIMEOUT:  120s
      IMPORT_VARSUBSTITUTION_ENABLED:      true
      IMPORT_FILES_LOCATIONS:              /config/*
      IMPORT_MANAGED_GROUP:                no-delete
      IMPORT_MANAGED_REQUIRED_ACTION:      no-delete
      IMPORT_MANAGED_ROLE:                 no-delete
      IMPORT_MANAGED_CLIENT:               no-delete
      IMPORT_MANAGED_USER:                 no-delete
      IMPORT_REMOTE_STATE_ENABLED:         true
      LOGGING_LEVEL_ROOT:                  INFO
      LOGGING_LEVEL_KEYCLOAKCONFIGCLI:     DEBUG
    Mounts:
      /config from config (ro)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-24n79 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      keycloak-config-cli-realm-master
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-24n79:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned orch-platform/keycloak-config-cli-tt6qn to orch-tf
  Normal  Pulled     11m   kubelet            Container image "busybox:1.36" already present on machine
  Normal  Created    11m   kubelet            Created container: wait-for-keycloak
  Normal  Started    11m   kubelet            Started container wait-for-keycloak
  Normal  Pulled     11m   kubelet            Container image "docker.io/adorsys/keycloak-config-cli:6.4.0-26" already present on machine
  Normal  Created    11m   kubelet            Created container: keycloak-config-cli
  Normal  Started    11m   kubelet            Started container keycloak-config-cli


Name:             keycloak-m2m-secrets-extractor-bt5mp
Namespace:        orch-platform
Priority:         0
Service Account:  keycloak-m2m-secret-creator
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 18:34:41 +0000
Labels:           app.kubernetes.io/name=keycloak-m2m-secrets-extractor
                  app.kubernetes.io/part-of=keycloak-instance
                  batch.kubernetes.io/controller-uid=e6290e12-6703-48ce-939a-c239f5e1297d
                  batch.kubernetes.io/job-name=keycloak-m2m-secrets-extractor
                  controller-uid=e6290e12-6703-48ce-939a-c239f5e1297d
                  job-name=keycloak-m2m-secrets-extractor
Annotations:      cni.projectcalico.org/containerID: 00933aa5a4c28e4ff97bc01c47bf96472d91bfaf16f1aeadc041bbe30c7c4573
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  sidecar.istio.io/inject: false
Status:           Succeeded
SeccompProfile:   RuntimeDefault
IP:               10.42.65.241
IPs:
  IP:           10.42.65.241
Controlled By:  Job/keycloak-m2m-secrets-extractor
Containers:
  m2m-secret-extractor:
    Container ID:    containerd://c4a6ec0862ad229f0f690e888640f609c90ae472755a194cbf29d780b9b5296c
    Image:           badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Image ID:        docker.io/badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
    Args:
      /scripts/extract-secrets.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 18:34:43 +0000
      Finished:     Tue, 03 Feb 2026 18:34:46 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     250m
      memory:  256Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      KEYCLOAK_NAMESPACE:  orch-platform
      KEYCLOAK_USER:       admin
      KEYCLOAK_PASSWORD:   <set to the key 'password' in secret 'platform-keycloak'>  Optional: false
      M2M_CLIENTS:         alerts-m2m-client host-manager-m2m-client co-manager-m2m-client ktc-m2m-client 3rd-party-host-manager-m2m-client edge-manager-m2m-client
    Mounts:
      /scripts from script (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9p674 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  script:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      keycloak-m2m-extractor-script
    Optional:  false
  kube-api-access-9p674:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned orch-platform/keycloak-m2m-secrets-extractor-bt5mp to orch-tf
  Normal  Pulled     11m   kubelet            Container image "badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b" already present on machine
  Normal  Created    11m   kubelet            Created container: m2m-secret-extractor
  Normal  Started    11m   kubelet            Started container m2m-secret-extractor


Name:             keycloak-operator-67766684cc-sg7lj
Namespace:        orch-platform
Priority:         0
Service Account:  keycloak-operator
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:30:28 +0000
Labels:           app.kubernetes.io/managed-by=quarkus
                  app.kubernetes.io/name=keycloak-operator
                  app.kubernetes.io/version=26.1.0
                  pod-template-hash=67766684cc
Annotations:      app.quarkus.io/build-timestamp: 2026-01-06 - 07:49:45 +0000
                  app.quarkus.io/quarkus-version: 3.27.1
                  app.quarkus.io/vcs-uri: https://github.com/keycloak/keycloak.git
                  cni.projectcalico.org/containerID: 9e36f4fea5789d53e25e0a38f5992e1f0407061e26cc029d1e5e6cde19d38f92
                  cni.projectcalico.org/podIP: 10.42.65.85/32
                  cni.projectcalico.org/podIPs: 10.42.65.85/32
Status:           Running
IP:               10.42.65.85
IPs:
  IP:           10.42.65.85
Controlled By:  ReplicaSet/keycloak-operator-67766684cc
Containers:
  keycloak-operator:
    Container ID:    containerd://60289fa1188c490a5ec340b36e506b1f1854d100a37d126dce1d5df3106b5843
    Image:           quay.io/keycloak/keycloak-operator:26.5.0
    Image ID:        quay.io/keycloak/keycloak-operator@sha256:62ac64da8a46a51f7dc48e75b35046caabf7fdca48f945fb28c355cb1aa8eec7
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:30:35 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     700m
      memory:  450Mi
    Requests:
      cpu:      300m
      memory:   450Mi
    Liveness:   http-get http://:8080/q/health/live delay=5s timeout=10s period=10s #success=1 #failure=3
    Readiness:  http-get http://:8080/q/health/ready delay=5s timeout=10s period=10s #success=1 #failure=3
    Startup:    http-get http://:8080/q/health/started delay=5s timeout=10s period=10s #success=1 #failure=3
    Environment:
      KUBERNETES_NAMESPACE:    orch-platform (v1:metadata.namespace)
      RELATED_IMAGE_KEYCLOAK:  quay.io/keycloak/keycloak:26.5.0
    Mounts:
      /tmp from tmp (rw)
      /var/cache from var-cache (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rthmx (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  var-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-rthmx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             keycloak-tenant-controller-set-0
Namespace:        orch-platform
Priority:         0
Service Account:  orch-svc
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:46:12 +0000
Labels:           app=keycloak-tenant-controller-pod
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=keycloak-tenant-controller-set-b5fdc556f
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=keycloak-tenant-controller-pod
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=keycloak-tenant-controller-set-0
Annotations:      cni.projectcalico.org/containerID: e79239b8c9ea2bd98f3c8101def22238bda736eea1a38240ec61573a49482616
                  cni.projectcalico.org/podIP: 10.42.65.173/32
                  cni.projectcalico.org/podIPs: 10.42.65.173/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: keycloak-tenant-controller-pod
                  kubectl.kubernetes.io/default-logs-container: keycloak-tenant-controller-pod
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.173
IPs:
  IP:           10.42.65.173
Controlled By:  StatefulSet/keycloak-tenant-controller-set
Init Containers:
  istio-init:
    Container ID:  containerd://9f835f8a785744b3787e3df27e90482c97627f618435a7578eb1ce9860b6c177
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:46:14 +0000
      Finished:     Tue, 03 Feb 2026 17:46:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9kmqg (ro)
  istio-proxy:
    Container ID:  containerd://cc004b381fcb998d0993588f07f82f92887351167f30ea0a926caef88e92c353
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:46:19 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:46:15 +0000
      Finished:     Tue, 03 Feb 2026 17:46:18 +0000
    Ready:          True
    Restart Count:  1
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      keycloak-tenant-controller-set-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"default-port","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     keycloak-tenant-controller-pod
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      keycloak-tenant-controller-set
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/keycloak-tenant-controller-set
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9kmqg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  ktc-creds-cntr:
    Container ID:  containerd://a1f155985c2ddd249097f8c6df68fa6395c82f9682551351e164aa1dad3dcbb7
    Image:         badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Image ID:      docker.io/badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      /ktc-credentials.sh
      http://platform-keycloak.orch-platform.svc:8080
      http://vault.orch-platform.svc:8200
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:46:17 +0000
      Finished:     Tue, 03 Feb 2026 17:46:18 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      VAULT_URL:       http://vault.orch-platform.svc.cluster.local:8200
      VAULT_PKI_ROLE:  orch-svc
      ADMIN_USER:      admin
      ADMIN_PASS:      <set to the key 'admin-password' in secret 'platform-keycloak'>  Optional: false
      ADMIN_CLIENT:    system-client
    Mounts:
      /ktc-credentials.sh from ktc-credentials (rw,path="ktc-credentials.sh")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9kmqg (ro)
  wait-for-crd:
    Container ID:  containerd://c01bfd40bf412308842dceb4f2f152869356882f450e791b33d3b20758102b8e
    Image:         alpine/kubectl:1.34.1
    Image ID:      docker.io/alpine/kubectl@sha256:8413f8890d19aa03f63851654f642957e65ba59654b0c9357ddc6ec0b05b63a6
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
    Args:
      set -e
      echo "edge-orchestrator.intel.com, waiting for CR..."
      echo "Waiting for CR 'orgs.org.infra-host.com' to be available..."
      until kubectl get orgs.org.edge-orchestrator.intel.com; do
        echo "orgs.org.edge-orchestrator.intel.com CR not found, waiting..."
        sleep 10
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:46:18 +0000
      Finished:     Tue, 03 Feb 2026 17:46:28 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9kmqg (ro)
Containers:
  keycloak-tenant-controller-pod:
    Container ID:    containerd://7be9fda750a9e60548fb1420ce31bfe1b7a1eeab5ea5133e536ab9f50dc00190
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/common/keycloak-tenant-controller:26.0.1
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/common/keycloak-tenant-controller@sha256:b7570aab92a6745086ef7bc1f8105ab47064b6c2d09639fd47dd079647f24551
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:46:33 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      VAULT_URL:             http://vault.orch-platform.svc.cluster.local:8200
      VAULT_PKI_ROLE:        orch-svc
      KEYCLOAK_URL:          http://platform-keycloak.orch-platform.svc:8080
      KTC_SERVER_LOG_LEVEL:  info
      KEYCLOAK_REALM:        master
      KEYCLOAK_SI_GROUPS:    
      KEYCLOAK_ORG_GROUPS:   {
                               "<org-id>_Project-Manager-Group": [
                                 "<org-id>_project-read-role",
                                 "<org-id>_project-write-role",
                                 "<org-id>_project-update-role",
                                 "<org-id>_project-delete-role"
                               ]
                             }
      KEYCLOAK_PROJ_GROUPS:  {
                               "<project-id>_Edge-Node-M2M-Service-Account": [
                                 "rs-access-r",
                                 "rs-proxy-r",
                                 "<project-id>_cat-r",
                                 "<project-id>_reg-r",
                                 "<project-id>_en-agent-rw"
                               ],
                               "<project-id>_Edge-Manager-Group": [
                                 "account/manage-account",
                                 "account/view-profile",
                                 "<project-id>_tc-r",
                                 "<project-id>_ao-rw",
                                 "<project-id>_cat-rw",
                                 "<project-id>_cl-rw",
                                 "<project-id>_cl-tpl-rw",
                                 "<project-id>_reg-a",
                                 "<project-id>_reg-r",
                                 "<project-id>_im-r",
                                 "<project-id>_alrt-rw",
                                 "<org-id>_<project-id>_m"
                               ],
                               "<project-id>_Edge-Onboarding-Group": [
                                 "rs-access-r",
                                 "<project-id>_en-ob"
                               ],
                               "<project-id>_Edge-Operator-Group": [
                                 "account/manage-account",
                                 "account/view-profile",
                                 "<project-id>_tc-r",
                                 "<project-id>_ao-rw",
                                 "<project-id>_cat-r",
                                 "<project-id>_cl-r",
                                 "<project-id>_cl-tpl-r",
                                 "<project-id>_reg-r",
                                 "<project-id>_im-r",
                                 "<project-id>_alrt-r",
                                 "<org-id>_<project-id>_m"
                               ],
                               "<project-id>_Host-Manager-Group": [
                                 "account/manage-account",
                                 "account/view-profile",
                                 "<project-id>_tc-r",
                                 "<project-id>_im-rw",
                                 "<project-id>_en-ob",
                                 "<org-id>_<project-id>_m"
                               ]
                             }
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9kmqg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  ktc-credentials:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      ktc-credentials
    Optional:  false
  kube-api-access-9kmqg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age                From               Message
  ----    ------     ----               ----               -------
  Normal  Scheduled  60m                default-scheduler  Successfully assigned orch-platform/keycloak-tenant-controller-set-0 to orch-tf
  Normal  Pulled     60m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal  Created    60m                kubelet            Created container: istio-init
  Normal  Started    60m                kubelet            Started container istio-init
  Normal  Pulled     59m                kubelet            Container image "badouralix/curl-jq@sha256:8ee002ae4452b23a3c70750c5c081e95334cfe9f7968fb4d67a90d4001c29d0b" already present on machine
  Normal  Started    59m                kubelet            Started container ktc-creds-cntr
  Normal  Created    59m                kubelet            Created container: ktc-creds-cntr
  Normal  Started    59m                kubelet            Started container wait-for-crd
  Normal  Pulled     59m                kubelet            Container image "alpine/kubectl:1.34.1" already present on machine
  Normal  Created    59m                kubelet            Created container: wait-for-crd
  Normal  Started    59m (x2 over 60m)  kubelet            Started container istio-proxy
  Normal  Created    59m (x2 over 60m)  kubelet            Created container: istio-proxy
  Normal  Pulled     59m (x2 over 60m)  kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal  Pulling    59m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/common/keycloak-tenant-controller:26.0.1"
  Normal  Pulled     59m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/common/keycloak-tenant-controller:26.0.1" in 3.874s (3.874s including waiting). Image size: 22466646 bytes.
  Normal  Created    59m                kubelet            Created container: keycloak-tenant-controller-pod
  Normal  Started    59m                kubelet            Started container keycloak-tenant-controller-pod


Name:             loki-backend-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:41 +0000
Labels:           app.kubernetes.io/component=backend
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=loki-backend-7cc4dd9cc
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-backend-0
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: e87708993609cd0a98db3feb842d7e75de6adc5bc193a44694c096ae8e0bd9da
                  cni.projectcalico.org/podIP: 10.42.65.160/32
                  cni.projectcalico.org/podIPs: 10.42.65.160/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.160
IPs:
  IP:           10.42.65.160
Controlled By:  StatefulSet/loki-backend
Init Containers:
  istio-init:
    Container ID:  containerd://de9ef8368b9018e1674e517ef3bf13407df72c848741610e3107b9e05373d4cf
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:56 +0000
      Finished:     Tue, 03 Feb 2026 17:35:57 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-245qk (ro)
  istio-proxy:
    Container ID:  containerd://f724d1a1436a9cd8bd964a0d079cf0433b986060914d1aa3a005a4abfeefe357
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-backend-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki,loki-sc-rules
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-backend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/loki-backend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-245qk (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://76bed2a97cd3a93a9305528089a0d095b9f9f389460914b3d17c0226a8d0123b
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=backend
      -legacy-read-mode=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:40 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:36:22 +0000
      Finished:     Tue, 03 Feb 2026 17:36:26 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /rules from sc-rules-volume (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-245qk (ro)
  loki-sc-rules:
    Container ID:   containerd://3be648e24b3222fe8fb0c948ad4b410041e24691f945ff8711e2c923e3e25984
    Image:          docker.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:       docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:                WATCH
      LABEL:                 loki_rule
      FOLDER:                /rules
      RESOURCE:              both
      WATCH_SERVER_TIMEOUT:  60
      WATCH_CLIENT_TIMEOUT:  60
      LOG_LEVEL:             INFO
    Mounts:
      /rules from sc-rules-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-245qk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  sc-rules-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-245qk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-backend-1
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:41 +0000
Labels:           app.kubernetes.io/component=backend
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=1
                  controller-revision-hash=loki-backend-7cc4dd9cc
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-backend-1
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: ca1c1c7116c87f048bcc56099801ad600a3d7cc8981502a260b7da37d1ff6895
                  cni.projectcalico.org/podIP: 10.42.65.162/32
                  cni.projectcalico.org/podIPs: 10.42.65.162/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.162
IPs:
  IP:           10.42.65.162
Controlled By:  StatefulSet/loki-backend
Init Containers:
  istio-init:
    Container ID:  containerd://c1ec5a24d601225a9967236e44a18f2598d729fee607e935fb8aa714b192fcc8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:58 +0000
      Finished:     Tue, 03 Feb 2026 17:35:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mb9mm (ro)
  istio-proxy:
    Container ID:  containerd://8c777f4859335810620dc8a841cb6f081bb052db92c498778b0180d13a311dc6
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:03 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-backend-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki,loki-sc-rules
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-backend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/loki-backend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mb9mm (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://7d306ecc352a8ac8265c4a48bade965cbd789acb6ee18b1ba3f3386ebfafe31a
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=backend
      -legacy-read-mode=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:42 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
      Finished:     Tue, 03 Feb 2026 17:36:26 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /rules from sc-rules-volume (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mb9mm (ro)
  loki-sc-rules:
    Container ID:   containerd://8704282a4e335093f1032cff909d06d14c4370d64d152aa6f141b235e04adfe6
    Image:          docker.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:       docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:                WATCH
      LABEL:                 loki_rule
      FOLDER:                /rules
      RESOURCE:              both
      WATCH_SERVER_TIMEOUT:  60
      WATCH_CLIENT_TIMEOUT:  60
      LOG_LEVEL:             INFO
    Mounts:
      /rules from sc-rules-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mb9mm (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  sc-rules-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-mb9mm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-backend-2
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:41 +0000
Labels:           app.kubernetes.io/component=backend
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=2
                  controller-revision-hash=loki-backend-7cc4dd9cc
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-backend-2
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: 0e089f6bc418148a8c50b4c057f945a76009350618bc081acef01651e5e989e2
                  cni.projectcalico.org/podIP: 10.42.65.161/32
                  cni.projectcalico.org/podIPs: 10.42.65.161/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.161
IPs:
  IP:           10.42.65.161
Controlled By:  StatefulSet/loki-backend
Init Containers:
  istio-init:
    Container ID:  containerd://a51ba38c3c19ce4e7dec93c5e9bdc51c2f0b4acef991c9d1a89777264849beae
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:56 +0000
      Finished:     Tue, 03 Feb 2026 17:35:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rwss5 (ro)
  istio-proxy:
    Container ID:  containerd://4fe9e03eb76673bad469493b7db5f9ea4776f973eabc97ca5207e2c9783fadb3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-backend-2 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki,loki-sc-rules
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-backend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/loki-backend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rwss5 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://ae59033518f830e586d01eb6132e15121a89c4aadf140ef8f087ff8f9512f67c
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=backend
      -legacy-read-mode=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:43 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 03 Feb 2026 17:36:23 +0000
      Finished:     Tue, 03 Feb 2026 17:36:27 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /rules from sc-rules-volume (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rwss5 (ro)
  loki-sc-rules:
    Container ID:   containerd://794900cf5b32e15366e63d446cd2dbc08d947ec271ab084d1071fb4b6701530c
    Image:          docker.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:       docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:21 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:                WATCH
      LABEL:                 loki_rule
      FOLDER:                /rules
      RESOURCE:              both
      WATCH_SERVER_TIMEOUT:  60
      WATCH_CLIENT_TIMEOUT:  60
      LOG_LEVEL:             INFO
    Mounts:
      /rules from sc-rules-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rwss5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  sc-rules-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-rwss5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-read-6bd96d7584-d4dv6
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=read
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  orchestrator/service=observability
                  pod-template-hash=6bd96d7584
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: 1855b08811d78798485aca882572db299d1bff8ae9f7f6783c07a6a12568dde6
                  cni.projectcalico.org/podIP: 10.42.65.152/32
                  cni.projectcalico.org/podIPs: 10.42.65.152/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.152
IPs:
  IP:           10.42.65.152
Controlled By:  ReplicaSet/loki-read-6bd96d7584
Init Containers:
  istio-init:
    Container ID:  containerd://2da88fccad505c65a5f5565f53934dc818bfb6ae56e4fba1a3790426b57f89e2
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:53 +0000
      Finished:     Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-69wjx (ro)
  istio-proxy:
    Container ID:  containerd://0b75ae847d337f4c1c417e9a41ede060c9a32506ec469bc25bc7f8b4ae5dea9e
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-read-6bd96d7584-d4dv6 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-read
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/loki-read
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-69wjx (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://312bf0950a439355f1442a071cb88498712d4b301d273009d709930943772e02
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=read
      -legacy-read-mode=false
      -common.compactor-grpc-address=loki-backend.orch-platform.svc.cluster.local:9095
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:16 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-69wjx (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-69wjx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-read-6bd96d7584-m94vw
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=read
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  orchestrator/service=observability
                  pod-template-hash=6bd96d7584
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: 3c7e3762cd5248ef6de258b98129094ac6017db2cda42298ff23d114bfe43f44
                  cni.projectcalico.org/podIP: 10.42.65.149/32
                  cni.projectcalico.org/podIPs: 10.42.65.149/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.149
IPs:
  IP:           10.42.65.149
Controlled By:  ReplicaSet/loki-read-6bd96d7584
Init Containers:
  istio-init:
    Container ID:  containerd://d727ac6198a4673ceae711182a7e534fe0c7d6a3d18fea5a2d2cf6741962168a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:53 +0000
      Finished:     Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6gkzk (ro)
  istio-proxy:
    Container ID:  containerd://86f715f9b8bd24d837f56721390568ece349ada5b0aaeb0a0e31598de38a210a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-read-6bd96d7584-m94vw (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-read
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/loki-read
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6gkzk (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://933269f3ffe9c27a9d72696648370be882abb05833880127190017c3dc22772f
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=read
      -legacy-read-mode=false
      -common.compactor-grpc-address=loki-backend.orch-platform.svc.cluster.local:9095
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6gkzk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-6gkzk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-read-6bd96d7584-vjh8w
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=read
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  orchestrator/service=observability
                  pod-template-hash=6bd96d7584
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: 4f939218f7baff61d669d78ba9fce10f9fd6c823d9a8bd6d080ef467e2d25ed4
                  cni.projectcalico.org/podIP: 10.42.65.145/32
                  cni.projectcalico.org/podIPs: 10.42.65.145/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.145
IPs:
  IP:           10.42.65.145
Controlled By:  ReplicaSet/loki-read-6bd96d7584
Init Containers:
  istio-init:
    Container ID:  containerd://35bf7c77055085615b70e3bbe82d00ec2f90b79f8b25e5926aee6e0c03e985d0
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:51 +0000
      Finished:     Tue, 03 Feb 2026 17:35:51 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gs29m (ro)
  istio-proxy:
    Container ID:  containerd://870d1433ecfd8ffbc4514504be10eb61296438b3e56ba8e9e887a50f361762de
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-read-6bd96d7584-vjh8w (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-read
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/loki-read
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gs29m (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://bc8657ac302ac694b7a09a5b34ef8a592e94ec1afe3067119d414496e9dacf03
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=read
      -legacy-read-mode=false
      -common.compactor-grpc-address=loki-backend.orch-platform.svc.cluster.local:9095
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /tmp from tmp (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gs29m (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-gs29m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-write-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:32 +0000
Labels:           app.kubernetes.io/component=write
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=loki-write-6db87b6645
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-write-0
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: 29c4c6540fddee13c25a0b2d9f47965b707f299f78e852953b0d342f542dd3a1
                  cni.projectcalico.org/podIP: 10.42.65.189/32
                  cni.projectcalico.org/podIPs: 10.42.65.189/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.189
IPs:
  IP:           10.42.65.189
Controlled By:  StatefulSet/loki-write
Init Containers:
  istio-init:
    Container ID:  containerd://92b2507f42c6916f680d0c270517d9889846fffe34ef495d6a66c54c072d6114
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:34 +0000
      Finished:     Tue, 03 Feb 2026 17:36:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bvfq7 (ro)
  istio-proxy:
    Container ID:  containerd://88dc490eccbada39eec4526a64959f3b2d98aa2bb24839a0a9390a46bf97addf
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-write-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-write
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/loki-write
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bvfq7 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://c0f69aa491b6e0f6d5d7f5905ee9c05a6c62a8d0e185d169d9c1d43c962b571d
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=write
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bvfq7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-loki-write-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-bvfq7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             loki-write-1
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:35 +0000
Labels:           app.kubernetes.io/component=write
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.5.7
                  apps.kubernetes.io/pod-index=1
                  controller-revision-hash=loki-write-6db87b6645
                  helm.sh/chart=loki-6.46.0
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=3.5.7
                  statefulset.kubernetes.io/pod-name=loki-write-1
Annotations:      checksum/config: 78ded405105ffeccf388d896648ffec2e95903219d22bedea241ca823d0808cf
                  cni.projectcalico.org/containerID: c05c58cacfff125979a1cd5526816a969ee918f2f1f27f1c5ef944eed2b6ed9e
                  cni.projectcalico.org/podIP: 10.42.65.191/32
                  cni.projectcalico.org/podIPs: 10.42.65.191/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: loki
                  kubectl.kubernetes.io/default-logs-container: loki
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.191
IPs:
  IP:           10.42.65.191
Controlled By:  StatefulSet/loki-write
Init Containers:
  istio-init:
    Container ID:  containerd://9aa1eb9e40b3c477a7f8e851871a22e7e045417f69af7b2b4f6a2527d4e1cbd3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:37 +0000
      Finished:     Tue, 03 Feb 2026 17:36:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zmst2 (ro)
  istio-proxy:
    Container ID:  containerd://62a15e5a19d50a3267c2874ffc7e2bc8b45c0002d54d54edc8ea9147b8a15c9d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      loki-write-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":3100,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"http-memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     loki
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      loki-write
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/loki-write
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/loki/readyz":{"httpGet":{"path":"/ready","port":3100,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zmst2 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  loki:
    Container ID:    containerd://4a6695b722c93ee90d2ca2ea0956da75b448e89e89fccdd38156103f863be210
    Image:           docker.io/grafana/loki:3.5.7
    Image ID:        docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
    Ports:           3100/TCP, 9095/TCP, 7946/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -config.file=/etc/loki/config/config.yaml
      -target=write
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:40 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/loki/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/loki/config from config (rw)
      /etc/loki/runtime-config from runtime-config (rw)
      /var/loki from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zmst2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-loki-write-1
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      loki-runtime
    Optional:  false
  kube-api-access-zmst2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             observability-tenant-controller-6f8c8d55fb-dfjbz
Namespace:        orch-platform
Priority:         0
Service Account:  observability-tenant-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:52:36 +0000
Labels:           app.kubernetes.io/instance=observability-tenant-controller
                  app.kubernetes.io/name=observability-tenant-controller
                  pod-template-hash=6f8c8d55fb
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=observability-tenant-controller
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 7ce02b29ef3d602923fec48ab5244e9247466ebf9018a3e8b55e027075e5db1f
                  cni.projectcalico.org/podIP: 10.42.65.219/32
                  cni.projectcalico.org/podIPs: 10.42.65.219/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: observability-tenant-controller
                  kubectl.kubernetes.io/default-logs-container: observability-tenant-controller
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.219
IPs:
  IP:           10.42.65.219
Controlled By:  ReplicaSet/observability-tenant-controller-6f8c8d55fb
Init Containers:
  istio-init:
    Container ID:  containerd://b03c39f6e218db6aee27867021cc9438d79dcdc09bf07113a5e1b765f401d52a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:52:38 +0000
      Finished:     Tue, 03 Feb 2026 17:52:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9m2c2 (ro)
  istio-proxy:
    Container ID:  containerd://0879c0d2be5fb5ffe7a4b58c0f788c033df4d35f7fc0dbf8fdfdfc0888202050
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:52:40 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      observability-tenant-controller-6f8c8d55fb-dfjbz (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":9273,"protocol":"TCP"}
                                         ,{"containerPort":50051,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     observability-tenant-controller
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      observability-tenant-controller
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/observability-tenant-controller
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9m2c2 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  observability-tenant-controller:
    Container ID:  containerd://30e1b751a54658076cfb54ee0f8c4fe0d0f443456c358f1b0b1b239bf719b4b2
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/observability-tenant-controller:0.7.0
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/observability-tenant-controller@sha256:61e5e82149ee85d6d8851d13086bcf7d467ab8df6832818e0c47b05dd824e071
    Ports:         9273/TCP, 50051/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      --config=/etc/config/config.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:52:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  256Mi
    Requests:
      cpu:        50m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /etc/config from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9m2c2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      observability-tenant-controller-config
    Optional:  false
  kube-api-access-9m2c2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  53m   default-scheduler  Successfully assigned orch-platform/observability-tenant-controller-6f8c8d55fb-dfjbz to orch-tf
  Normal   Pulled     53m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m   kubelet            Created container: istio-init
  Normal   Started    53m   kubelet            Started container istio-init
  Normal   Pulled     53m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    53m   kubelet            Created container: istio-proxy
  Normal   Started    53m   kubelet            Started container istio-proxy
  Warning  Unhealthy  53m   kubelet            Startup probe failed: Get "http://10.42.65.219:15021/healthz/ready": dial tcp 10.42.65.219:15021: connect: connection refused
  Normal   Pulling    53m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/observability-tenant-controller:0.7.0"
  Normal   Pulled     53m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/observability-tenant-controller:0.7.0" in 2.774s (2.774s including waiting). Image size: 18003931 bytes.
  Normal   Created    53m   kubelet            Created container: observability-tenant-controller
  Normal   Started    53m   kubelet            Started container observability-tenant-controller


Name:             orchestrator-observability-grafana-6878f4b56c-scmnv
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-grafana
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=grafana
                  app.kubernetes.io/version=12.2.1
                  helm.sh/chart=grafana-10.1.4
                  orchestrator/service=logging
                  pod-template-hash=6878f4b56c
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=grafana
                  service.istio.io/canonical-revision=12.2.1
Annotations:      checksum/config: b191238ba95d1244d2de0ee5dcab558ad1426a72e52d668eef1954730380cd2d
                  checksum/sc-dashboard-provider-config: 81f722b8194d01e4a84ce12a0cbbe9b705b2d9145719c803ac264b4786c64a09
                  cni.projectcalico.org/containerID: 905deb687a67fad9fcbd30b6d737d181db6a5a2382b0a5f429fff2178755252c
                  cni.projectcalico.org/podIP: 10.42.65.154/32
                  cni.projectcalico.org/podIPs: 10.42.65.154/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: grafana
                  kubectl.kubernetes.io/default-logs-container: grafana-sc-dashboard
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.154
IPs:
  IP:           10.42.65.154
Controlled By:  ReplicaSet/orchestrator-observability-grafana-6878f4b56c
Init Containers:
  istio-init:
    Container ID:  containerd://2acfaf212a93bc25dfc09b2ef6f8476daa898b2dc208ab08ac1c1f2ecc467c4d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:54 +0000
      Finished:     Tue, 03 Feb 2026 17:35:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ptcsc (ro)
  istio-proxy:
    Container ID:  containerd://258d139ba872ac9b2571e02c4007e24e7fd61dbeccb0b852fbfd990991eff46a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:01 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-grafana-6878f4b56c-scmnv (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"grafana","containerPort":3000,"protocol":"TCP"}
                                         ,{"name":"gossip-tcp","containerPort":9094,"protocol":"TCP"}
                                         ,{"name":"gossip-udp","containerPort":9094,"protocol":"UDP"}
                                         ,{"containerPort":9190,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     grafana-sc-dashboard,grafana,grafana-proxy
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-grafana
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-grafana
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/grafana/livez":{"httpGet":{"path":"/api/health","port":3000,"scheme":"HTTP"},"timeoutSeconds":30},"/app-health/grafana/readyz":{"httpGet":{"path":"/api/health","port":3000,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ptcsc (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  grafana-sc-dashboard:
    Container ID:    containerd://93c522fa7d8c7cb6c55151d7054fbe4925e955c09c1f7fd97cfaadcba4904bff
    Image:           quay.io/kiwigrid/k8s-sidecar:1.30.10
    Image ID:        docker.io/kiwigrid/k8s-sidecar@sha256:835d79d8fbae62e42d8a86929d4e3c5eec2e869255dd37756b5a3166c2f22309
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:12 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      METHOD:             WATCH
      LABEL:              grafana_dashboard
      LABEL_VALUE:        orchestrator
      FOLDER:             /tmp/dashboards
      RESOURCE:           both
      NAMESPACE:          ALL
      FOLDER_ANNOTATION:  grafana_folder
      REQ_URL:            http://localhost:3000/api/admin/provisioning/dashboards/reload
      REQ_METHOD:         POST
    Mounts:
      /tmp/dashboards from sc-dashboard-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ptcsc (ro)
  grafana:
    Container ID:    containerd://b61f0da460dd0da6e381ecf268b520c5b887a99e29d36103714b3d4fbb7b855e
    Image:           docker.io/grafana/grafana:12.2.1
    Image ID:        docker.io/grafana/grafana@sha256:35c41e0fd0295f5d0ee5db7e780cf33506abfaf47686196f825364889dee878b
    Ports:           3000/TCP, 9094/TCP, 9094/UDP
    Host Ports:      0/TCP, 0/TCP, 0/UDP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:36:43 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/grafana/livez delay=60s timeout=30s period=10s #success=1 #failure=10
    Readiness:  http-get http://:15020/app-health/grafana/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_IP:                                       (v1:status.podIP)
      GF_PATHS_DATA:                               /var/lib/grafana/
      GF_PATHS_LOGS:                               /var/log/grafana
      GF_PATHS_PLUGINS:                            /var/lib/grafana/plugins
      GF_PATHS_PROVISIONING:                       /etc/grafana/provisioning
      GF_SECURITY_DISABLE_INITIAL_ADMIN_CREATION:  true
    Mounts:
      /etc/grafana/grafana.ini from config (rw,path="grafana.ini")
      /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml from sc-dashboard-provider (rw,path="provider.yaml")
      /etc/grafana/provisioning/datasources/datsources.yaml from config (rw,path="datsources.yaml")
      /tmp/dashboards from sc-dashboard-volume (rw)
      /var/lib/grafana from storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ptcsc (ro)
  grafana-proxy:
    Container ID:    containerd://c467a1e9de50a7aaa67fbe6f892ad887e895ebf4aa5668356f7cc077f6c64f82
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/o11y/grafana-proxy:0.5.1
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/o11y/grafana-proxy@sha256:d26129f13655c12d44492387c1c8757ce2b407860ba3e231b0862a47d1cf1a55
    Port:            9190/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --log-level=info
      --include-deleted
      --include-edgenode-system
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:46 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  256Mi
    Requests:
      cpu:        10m
      memory:     32Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ptcsc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-grafana
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  sc-dashboard-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  sc-dashboard-provider:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-grafana-config-dashboards
    Optional:  false
  kube-api-access-ptcsc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-observability-loki-chunks-cache-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=memcached-chunks-cache
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=orchestrator-observability-loki-chunks-cache-5995dbccd
                  name=memcached-chunks-cache
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=orchestrator-observability-loki-chunks-cache-0
Annotations:      cni.projectcalico.org/containerID: 2dbf8fcab510afa961f54395a8bfdf9bcb81d75851453f4f38c8a473f0d8bdf8
                  cni.projectcalico.org/podIP: 10.42.65.155/32
                  cni.projectcalico.org/podIPs: 10.42.65.155/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: memcached
                  kubectl.kubernetes.io/default-logs-container: memcached
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.155
IPs:
  IP:           10.42.65.155
Controlled By:  StatefulSet/orchestrator-observability-loki-chunks-cache
Init Containers:
  istio-init:
    Container ID:  containerd://ebc75b39e30ad92cbaa089dd56079185a22e42bff4ef0bb632c6191daf00b90a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:54 +0000
      Finished:     Tue, 03 Feb 2026 17:35:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zj2bp (ro)
  istio-proxy:
    Container ID:  containerd://a8321151ab009a6402fc1774c37bd5527e8e575c719829adabee468079b57f8c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-loki-chunks-cache-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"client","containerPort":11211,"protocol":"TCP"}
                                         ,{"name":"http-metrics","containerPort":9150,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     memcached,exporter
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-loki-chunks-cache
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/orchestrator-observability-loki-chunks-cache
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/exporter/livez":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":5},"/app-health/exporter/readyz":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":3},"/app-health/memcached/livez":{"tcpSocket":{"port":11211},"timeoutSeconds":5},"/app-health/memcached/readyz":{"tcpSocket":{"port":11211},"timeoutSeconds":3}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zj2bp (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  memcached:
    Container ID:  containerd://736c3c8760f6e6be9f6fc41c35983e139a2971e5977cdb8b07663319b5f59aac
    Image:         memcached:1.6.39-alpine
    Image ID:      docker.io/library/memcached@sha256:c8503d4491edd3110cc07d0465089d3a41cf1daf8645e71149e39a51835e92cd
    Port:          11211/TCP
    Host Port:     0/TCP
    Args:
      -m 8192
      --extended=modern,track_sizes
      -I 5m
      -c 16384
      -v
      -u 11211
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/memcached/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/memcached/readyz delay=5s timeout=3s period=5s #success=1 #failure=6
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zj2bp (ro)
  exporter:
    Container ID:  containerd://75ff866ca71b6d90048696d35524bd0d39029d77eceb22089840cbe1ca748682
    Image:         prom/memcached-exporter:v0.15.3
    Image ID:      docker.io/prom/memcached-exporter@sha256:4356d5f0d2f0ba1525ad995d72fb83531ccaf85ee5468c02ab1049d4ccabd308
    Port:          9150/TCP
    Host Port:     0/TCP
    Args:
      --memcached.address=localhost:11211
      --web.listen-address=0.0.0.0:9150
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/exporter/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/exporter/readyz delay=5s timeout=3s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zj2bp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-zj2bp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-observability-loki-gateway-569b557c66-68wft
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=gateway
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  orchestrator/service=observability
                  pod-template-hash=569b557c66
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 261dab56e84637afd04244f372368af374b179a39958560cf0002814341a44b5
                  cni.projectcalico.org/containerID: e05b00e1ed2e460d0052ea0fa602ba66915a3b4362a73c87a14adc25d7b4012a
                  cni.projectcalico.org/podIP: 10.42.65.153/32
                  cni.projectcalico.org/podIPs: 10.42.65.153/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: nginx
                  kubectl.kubernetes.io/default-logs-container: nginx
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.153
IPs:
  IP:           10.42.65.153
Controlled By:  ReplicaSet/orchestrator-observability-loki-gateway-569b557c66
Init Containers:
  istio-init:
    Container ID:  containerd://5f3f89c4a4558925a2f8a4708e8e4b2283e2a2504932cea5bf435af3eb7fda9c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:54 +0000
      Finished:     Tue, 03 Feb 2026 17:35:54 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzvjg (ro)
  istio-proxy:
    Container ID:  containerd://8d7a8a373afe97dc9f8c05960e71e726805b605adcd97f317af6563eabe80e5c
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-loki-gateway-569b557c66-68wft (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     nginx
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-loki-gateway
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-loki-gateway
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/nginx/readyz":{"httpGet":{"path":"/","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzvjg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  nginx:
    Container ID:   containerd://dbbe8d17bb243f83c67469c8b1433029357ab6cb0e7b650fb490b1660552467b
    Image:          docker.io/nginxinc/nginx-unprivileged:1.29-alpine
    Image ID:       docker.io/nginxinc/nginx-unprivileged@sha256:5aea7cc516b419e3526f47dd1531be31a56a046cfe44754d94f9383e13e2ee99
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/nginx/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /docker-entrypoint.d from docker-entrypoint-d-override (rw)
      /etc/nginx from config (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fzvjg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-loki-gateway
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  docker-entrypoint-d-override:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-fzvjg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-observability-loki-results-cache-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-loki
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=memcached-results-cache
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=loki
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=orchestrator-observability-loki-results-cache-566ff4b4bc
                  name=memcached-results-cache
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=loki
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=orchestrator-observability-loki-results-cache-0
Annotations:      cni.projectcalico.org/containerID: c11f39088f8819072198313895815afd8d6de12a6234cf9b0a69c5ad6ba20726
                  cni.projectcalico.org/podIP: 10.42.65.150/32
                  cni.projectcalico.org/podIPs: 10.42.65.150/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: memcached
                  kubectl.kubernetes.io/default-logs-container: memcached
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.150
IPs:
  IP:           10.42.65.150
Controlled By:  StatefulSet/orchestrator-observability-loki-results-cache
Init Containers:
  istio-init:
    Container ID:  containerd://2e75b7f2706ff2a268e8e0372288bebc2937ffe99d2aeec4e5b89a9922e5b9d9
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:53 +0000
      Finished:     Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fg9vg (ro)
  istio-proxy:
    Container ID:  containerd://be36f68ffc6af0cc19a26163c7b1b4a89133e8392cdc89ca591e7ca832183ac6
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-loki-results-cache-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"client","containerPort":11211,"protocol":"TCP"}
                                         ,{"name":"http-metrics","containerPort":9150,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     memcached,exporter
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-loki-results-cache
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/orchestrator-observability-loki-results-cache
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/exporter/livez":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":5},"/app-health/exporter/readyz":{"httpGet":{"path":"/metrics","port":9150,"scheme":"HTTP"},"timeoutSeconds":3},"/app-health/memcached/livez":{"tcpSocket":{"port":11211},"timeoutSeconds":5},"/app-health/memcached/readyz":{"tcpSocket":{"port":11211},"timeoutSeconds":3}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fg9vg (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  memcached:
    Container ID:  containerd://e8469148e0ea5da3803d4030acba8de7592a6a547720fa1afdb35d4a54bc2fd2
    Image:         memcached:1.6.39-alpine
    Image ID:      docker.io/library/memcached@sha256:c8503d4491edd3110cc07d0465089d3a41cf1daf8645e71149e39a51835e92cd
    Port:          11211/TCP
    Host Port:     0/TCP
    Args:
      -m 1024
      --extended=modern,track_sizes
      -I 5m
      -c 16384
      -v
      -u 11211
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/memcached/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/memcached/readyz delay=5s timeout=3s period=5s #success=1 #failure=6
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fg9vg (ro)
  exporter:
    Container ID:  containerd://273eda4dfd9d0ce52da7719b0f2ee4a7bf3e396d2f2791d7d0d9ad0868f02117
    Image:         prom/memcached-exporter:v0.15.3
    Image ID:      docker.io/prom/memcached-exporter@sha256:4356d5f0d2f0ba1525ad995d72fb83531ccaf85ee5468c02ab1049d4ccabd308
    Port:          9150/TCP
    Host Port:     0/TCP
    Args:
      --memcached.address=localhost:11211
      --web.listen-address=0.0.0.0:9150
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/exporter/livez delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/exporter/readyz delay=5s timeout=3s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fg9vg (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-fg9vg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-observability-mimir-compactor-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:29 +0000
Labels:           app.kubernetes.io/component=compactor
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=orchestrator-observability-mimir-compactor-67cdf86df8
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=orchestrator-observability-mimir-compactor-0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: 6892f093f4fb0a4495cdb2cac5d2193a5b1b8df4a378a24da40a613eab464e78
                  cni.projectcalico.org/podIP: 10.42.65.186/32
                  cni.projectcalico.org/podIPs: 10.42.65.186/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: compactor
                  kubectl.kubernetes.io/default-logs-container: compactor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.186
IPs:
  IP:           10.42.65.186
Controlled By:  StatefulSet/orchestrator-observability-mimir-compactor
Init Containers:
  istio-init:
    Container ID:  containerd://e0de03c64c4adbf0bcf8689ae20b65337564bb9b1599e67d30c5f24aa309b04a
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:31 +0000
      Finished:     Tue, 03 Feb 2026 17:36:31 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f88p2 (ro)
  istio-proxy:
    Container ID:  containerd://fbaa052c7a3feed4cb41699454467859e63c64171b01b5eb649fb160d01a7cd7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:32 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-compactor-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     compactor
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-compactor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/orchestrator-observability-mimir-compactor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/compactor/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f88p2 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  compactor:
    Container ID:  containerd://f8aadc8c3160bcf44d4406e2f466fdb4dee04ca5f88753f3c60174de4e9c0236
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=compactor
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/compactor/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f88p2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-orchestrator-observability-mimir-compactor-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-f88p2:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=compactor,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-distributor-7f44cf9966-xf6r6
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=distributor
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=7f44cf9966
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: 9d01a5b222750006a14956001b0f63810203b529688de801766b8d8fd1ef907f
                  cni.projectcalico.org/podIP: 10.42.65.156/32
                  cni.projectcalico.org/podIPs: 10.42.65.156/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: distributor
                  kubectl.kubernetes.io/default-logs-container: distributor
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.156
IPs:
  IP:           10.42.65.156
Controlled By:  ReplicaSet/orchestrator-observability-mimir-distributor-7f44cf9966
Init Containers:
  istio-init:
    Container ID:  containerd://fa6f4f715075f036392fe3c96e74eebfff31d0a0a6a80a53d93cb0dbca6454d4
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:55 +0000
      Finished:     Tue, 03 Feb 2026 17:35:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9ffx7 (ro)
  istio-proxy:
    Container ID:  containerd://c24e3216d3520104b84853a9d7c370d52a2521c3f26ccd94ab90deacbf9ad954
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-distributor-7f44cf9966-xf6r6 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     distributor
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-distributor
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-mimir-distributor
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/distributor/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9ffx7 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  distributor:
    Container ID:  containerd://34d1936818ca217084198d3e377f9439de98d3e24bd56071dd595623c05e468c
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=distributor
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -server.grpc.keepalive.max-connection-age=60s
      -server.grpc.keepalive.max-connection-age-grace=5m
      -server.grpc.keepalive.max-connection-idle=1m
      -shutdown-delay=90s
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/distributor/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  8
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9ffx7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-9ffx7:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=distributor,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-gateway-86554bd659-5r5jx
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=gateway
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=86554bd659
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: 026a7cb983d5061fd0b2c6c0887bde903c46565bd3302bb9c86cb24b042a4643
                  cni.projectcalico.org/containerID: 70c054fbb58fbbc64fa8f3b874ef97d4387ceb07859b558c0d780428aee0acd1
                  cni.projectcalico.org/podIP: 10.42.65.147/32
                  cni.projectcalico.org/podIPs: 10.42.65.147/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: gateway
                  kubectl.kubernetes.io/default-logs-container: gateway
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.147
IPs:
  IP:           10.42.65.147
Controlled By:  ReplicaSet/orchestrator-observability-mimir-gateway-86554bd659
Init Containers:
  istio-init:
    Container ID:  containerd://d898ffe9a3f727c18a6c457b8dc0111739e207a9aa9206afd7c45cea07373cf6
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:52 +0000
      Finished:     Tue, 03 Feb 2026 17:35:52 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gdp5f (ro)
  istio-proxy:
    Container ID:  containerd://0dd739aad18de5fcb58ecf4495a31d93d8fc6ce8f5cebbb8b1ee963922ec5622
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-gateway-86554bd659-5r5jx (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     gateway
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-gateway
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-mimir-gateway
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/gateway/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gdp5f (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  gateway:
    Container ID:   containerd://ff396dfe48d0722d5df6d26e452694bec950902e4c276a4812006987672aa207
    Image:          docker.io/nginxinc/nginx-unprivileged:1.29-alpine
    Image ID:       docker.io/nginxinc/nginx-unprivileged@sha256:5aea7cc516b419e3526f47dd1531be31a56a046cfe44754d94f9383e13e2ee99
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/gateway/readyz delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /docker-entrypoint.d from docker-entrypoint-d-override (rw)
      /etc/nginx/nginx.conf from nginx-config (rw,path="nginx.conf")
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gdp5f (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  nginx-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-gateway-nginx
    Optional:  false
  docker-entrypoint-d-override:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-gdp5f:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=gateway,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-ingester-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:20 +0000
Labels:           app.kubernetes.io/component=ingester
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=orchestrator-observability-mimir-ingester-5dbd6fdf66
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=orchestrator-observability-mimir-ingester-0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: 340dec4168c4186a34faac7da410f499b20b24612e183930486bf7f3d9768b88
                  cni.projectcalico.org/podIP: 10.42.65.182/32
                  cni.projectcalico.org/podIPs: 10.42.65.182/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: ingester
                  kubectl.kubernetes.io/default-logs-container: ingester
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.182
IPs:
  IP:           10.42.65.182
Controlled By:  StatefulSet/orchestrator-observability-mimir-ingester
Init Containers:
  istio-init:
    Container ID:  containerd://f6d667e89d110a6a782d5cf5bd0b3e40de3ff11a511c36d15c4a935edbab944d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:24 +0000
      Finished:     Tue, 03 Feb 2026 17:36:24 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rb8nq (ro)
  istio-proxy:
    Container ID:  containerd://df69d89e7d8f56c160c82b703084fc579cab39a34a5140b72fddd0747a611577
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-ingester-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     ingester
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-ingester
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/orchestrator-observability-mimir-ingester
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/ingester/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rb8nq (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  ingester:
    Container ID:  containerd://7ec2d74d59f714b43bf6b4b64c629b04753fff15713b1d9eb4daf8c9784d3e30
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=ingester
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -ingester.ring.instance-availability-zone=zone-default
      -server.grpc-max-concurrent-streams=500
      -memberlist.abort-if-fast-join-fails=true
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:30 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/ingester/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  4
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rb8nq (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-orchestrator-observability-mimir-ingester-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-rb8nq:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=ingester,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-ingester-1
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:29 +0000
Labels:           app.kubernetes.io/component=ingester
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=1
                  controller-revision-hash=orchestrator-observability-mimir-ingester-5dbd6fdf66
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=orchestrator-observability-mimir-ingester-1
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: 44660baa6598fbdaba440a512002db11bcc2536c51a5399ffc0940c4546593b9
                  cni.projectcalico.org/podIP: 10.42.65.187/32
                  cni.projectcalico.org/podIPs: 10.42.65.187/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: ingester
                  kubectl.kubernetes.io/default-logs-container: ingester
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.187
IPs:
  IP:           10.42.65.187
Controlled By:  StatefulSet/orchestrator-observability-mimir-ingester
Init Containers:
  istio-init:
    Container ID:  containerd://c8fba95c30fe5893c9a1cd65584dab82b82f0be191b8ab3616783449c57a6622
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:31 +0000
      Finished:     Tue, 03 Feb 2026 17:36:31 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7sxgz (ro)
  istio-proxy:
    Container ID:  containerd://ff4092648e2a03679bae274e2f158f412ba3e46db4c35095221d04f4bd96763d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:32 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-ingester-1 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     ingester
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-ingester
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/orchestrator-observability-mimir-ingester
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/ingester/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7sxgz (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  ingester:
    Container ID:  containerd://4a2c90e0b22152d660899ce07b64b8669c794a52daae9e9fe51c4f49d53d82fe
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=ingester
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -ingester.ring.instance-availability-zone=zone-default
      -server.grpc-max-concurrent-streams=500
      -memberlist.abort-if-fast-join-fails=true
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/ingester/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  4
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7sxgz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-orchestrator-observability-mimir-ingester-1
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-7sxgz:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=ingester,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-make-minio-buckets-5.4.0-26v5m
Namespace:        orch-platform
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app=mimir-distributed-job
                  batch.kubernetes.io/controller-uid=74db3571-bd86-448b-aafc-f7a22fd369f4
                  batch.kubernetes.io/job-name=orchestrator-observability-mimir-make-minio-buckets-5.4.0
                  controller-uid=74db3571-bd86-448b-aafc-f7a22fd369f4
                  job-name=orchestrator-observability-mimir-make-minio-buckets-5.4.0
                  orchestrator/service=observability
                  release=orchestrator-observability
Annotations:      cni.projectcalico.org/containerID: ed32156ab55a223d9b4ee635bd8d7e3f32fbd5ff52196b35a16d8ef155237b85
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
Status:           Succeeded
IP:               10.42.65.151
IPs:
  IP:           10.42.65.151
Controlled By:  Job/orchestrator-observability-mimir-make-minio-buckets-5.4.0
Containers:
  minio-mc:
    Container ID:  containerd://5490b75d55153076198935a5cf2cc06afbedee7c20855f246010706d14f07c32
    Image:         quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
    Image ID:      quay.io/minio/mc@sha256:a7fe349ef4bd8521fb8497f55c6042871b2ae640607cf99d9bede5e9bdf11727
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      /config/initialize
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:11 +0000
      Finished:     Tue, 03 Feb 2026 17:36:39 +0000
    Ready:          False
    Restart Count:  0
    Requests:
      memory:  128Mi
    Environment:
      MINIO_ENDPOINT:  orchestrator-observability-minio
      MINIO_PORT:      9000
    Mounts:
      /config from minio-configuration (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qzlhs (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  minio-configuration:
    Type:                Projected (a volume that contains injected data from multiple sources)
    ConfigMapName:       orchestrator-observability-minio
    ConfigMapOptional:   <nil>
    SecretName:          orchestrator-observability-minio
    SecretOptionalName:  <nil>
  kube-api-access-qzlhs:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-observability-mimir-querier-5846f8d9d7-p8g9v
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:41 +0000
Labels:           app.kubernetes.io/component=querier
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=5846f8d9d7
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: 4a728b794e2a30dac4b84d7f408b57c99dde20fc54c5751c1d225902448153da
                  cni.projectcalico.org/podIP: 10.42.65.157/32
                  cni.projectcalico.org/podIPs: 10.42.65.157/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: querier
                  kubectl.kubernetes.io/default-logs-container: querier
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.157
IPs:
  IP:           10.42.65.157
Controlled By:  ReplicaSet/orchestrator-observability-mimir-querier-5846f8d9d7
Init Containers:
  istio-init:
    Container ID:  containerd://890deae46e75c95e17ad1080b03d61cb3f84e007bb7f74a3e62c87e6295ca817
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:55 +0000
      Finished:     Tue, 03 Feb 2026 17:35:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vw7jn (ro)
  istio-proxy:
    Container ID:  containerd://2f17c7bd3c4f33279ccc770b57353048a1f6e7400f9f5760936edc95c2afa64d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:00 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-querier-5846f8d9d7-p8g9v (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     querier
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-querier
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-mimir-querier
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/querier/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vw7jn (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  querier:
    Container ID:  containerd://ec93b361ad420345420b84767fabd20888e0392dcdd9e13c75c5d7cfe7f7a654
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=querier
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -querier.store-gateway-client.grpc-max-recv-msg-size=209715200
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/querier/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  5
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vw7jn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-vw7jn:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=querier,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-query-frontend-7ff9b89786tpfkz
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:40 +0000
Labels:           app.kubernetes.io/component=query-frontend
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  pod-template-hash=7ff9b89786
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: c9987412ee14f0a84181e17cd6b2b5ac9f94a8c9ce82901456dcab183c379ffe
                  cni.projectcalico.org/podIP: 10.42.65.159/32
                  cni.projectcalico.org/podIPs: 10.42.65.159/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: query-frontend
                  kubectl.kubernetes.io/default-logs-container: query-frontend
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.159
IPs:
  IP:           10.42.65.159
Controlled By:  ReplicaSet/orchestrator-observability-mimir-query-frontend-7ff9b89786
Init Containers:
  istio-init:
    Container ID:  containerd://891bfcffeb91b4b77d3be4312a5e254cb767f5f12504c938be0d29351270d621
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:55 +0000
      Finished:     Tue, 03 Feb 2026 17:35:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nv7v5 (ro)
  istio-proxy:
    Container ID:  containerd://1a0ed83ce608b6d53176aa28c23f5682ad4439e8282c0fd5178114520ff4e5ba
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-query-frontend-7ff9b89786tpfkz (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     query-frontend
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-query-frontend
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-mimir-query-frontend
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/query-frontend/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nv7v5 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  query-frontend:
    Container ID:  containerd://e6b7f3e88e00d490a1e31802eb09bf6ebfcb509b3e607f71273d5e191a562001
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      -target=query-frontend
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -server.grpc.keepalive.max-connection-age=30s
      -shutdown-delay=90s
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:15020/app-health/query-frontend/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nv7v5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-nv7v5:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=query-frontend,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-query-scheduler-9c946ff974prv8
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:41 +0000
Labels:           app.kubernetes.io/component=query-scheduler
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  pod-template-hash=9c946ff97
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: 98b23ecfc3561075e14de8a331a2505b986b28a4a05804185d39d5605df37b32
                  cni.projectcalico.org/podIP: 10.42.65.164/32
                  cni.projectcalico.org/podIPs: 10.42.65.164/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: query-scheduler
                  kubectl.kubernetes.io/default-logs-container: query-scheduler
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.164
IPs:
  IP:           10.42.65.164
Controlled By:  ReplicaSet/orchestrator-observability-mimir-query-scheduler-9c946ff97
Init Containers:
  istio-init:
    Container ID:  containerd://361c9b909e7d21b1ec7f7eb64b135e5bfc0bff9e514bfdda6f35c198c1941d81
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:58 +0000
      Finished:     Tue, 03 Feb 2026 17:35:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jqg8g (ro)
  istio-proxy:
    Container ID:  containerd://18a24b5f02d1a94b286ed4cf5928e15dd30a44257be652720552017c462f99c7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:03 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-query-scheduler-9c946ff974prv8 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     query-scheduler
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-query-scheduler
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-mimir-query-scheduler
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/query-scheduler/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jqg8g (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  query-scheduler:
    Container ID:  containerd://7793d70d29384257e8a6541946734493aadeeb0798b099cd1af06ec32209a402
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      -target=query-scheduler
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:13 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     128Mi
    Readiness:    http-get http://:15020/app-health/query-scheduler/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jqg8g (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jqg8g:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=query-scheduler,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-query-scheduler-9c946ff97vqq8r
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:41 +0000
Labels:           app.kubernetes.io/component=query-scheduler
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/version=3.0.0
                  helm.sh/chart=mimir-distributed-6.0.3
                  pod-template-hash=9c946ff97
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: bd29e1bde2c6273f7b199ff9d76efbc92799b553c5ab4a2eb2938b53c516e290
                  cni.projectcalico.org/podIP: 10.42.65.158/32
                  cni.projectcalico.org/podIPs: 10.42.65.158/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: query-scheduler
                  kubectl.kubernetes.io/default-logs-container: query-scheduler
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.158
IPs:
  IP:           10.42.65.158
Controlled By:  ReplicaSet/orchestrator-observability-mimir-query-scheduler-9c946ff97
Init Containers:
  istio-init:
    Container ID:  containerd://523348ba38522b6cc5c3bc3853a3118e88721c9df851d4a7316f0b48659abb7d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:55 +0000
      Finished:     Tue, 03 Feb 2026 17:35:55 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8vq5q (ro)
  istio-proxy:
    Container ID:  containerd://505e7c8d8311e86106e25047fc864c799bd1d1efa3c8b10e648678ce853a94cb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-query-scheduler-9c946ff97vqq8r (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     query-scheduler
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-query-scheduler
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-mimir-query-scheduler
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/query-scheduler/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8vq5q (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  query-scheduler:
    Container ID:  containerd://6aa6c0071240f88e57909c2efa0c38b50acaa249bf3f947a9d275e0cdcbbc5ef
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      -target=query-scheduler
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:12 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     128Mi
    Readiness:    http-get http://:15020/app-health/query-scheduler/readyz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8vq5q (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8vq5q:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=query-scheduler,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-mimir-store-gateway-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-mimir
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:33 +0000
Labels:           app.kubernetes.io/component=store-gateway
                  app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mimir
                  app.kubernetes.io/part-of=memberlist
                  app.kubernetes.io/version=3.0.0
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=orchestrator-observability-mimir-store-gateway-6fdb5b9dc7
                  helm.sh/chart=mimir-distributed-6.0.3
                  orchestrator/service=observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=mimir
                  service.istio.io/canonical-revision=3.0.0
                  statefulset.kubernetes.io/pod-name=orchestrator-observability-mimir-store-gateway-0
Annotations:      checksum/config: 623d96f1d84f0e7810e57ddff5dc25ddfe49af2bd68a74e9b90e793d60b35471
                  cni.projectcalico.org/containerID: c1a1c7a53e97039c633bc73ee0e7529f562562a975064f08bda14ef687f5fd34
                  cni.projectcalico.org/podIP: 10.42.65.190/32
                  cni.projectcalico.org/podIPs: 10.42.65.190/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: store-gateway
                  kubectl.kubernetes.io/default-logs-container: store-gateway
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.190
IPs:
  IP:           10.42.65.190
Controlled By:  StatefulSet/orchestrator-observability-mimir-store-gateway
Init Containers:
  istio-init:
    Container ID:  containerd://5fe8ba0cf51de5f20dc42b32fedc88045f6f85cb27cb497ad89beb6ec0211d02
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:35 +0000
      Finished:     Tue, 03 Feb 2026 17:36:35 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cgsst (ro)
  istio-proxy:
    Container ID:  containerd://6dd8a2231c1bdb8e7faf8f7bf7dc41a26cd978197cd621d474fb632506117e67
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-mimir-store-gateway-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-metrics","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9095,"protocol":"TCP"}
                                         ,{"name":"memberlist","containerPort":7946,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     store-gateway
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-mimir-store-gateway
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/orchestrator-observability-mimir-store-gateway
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/store-gateway/readyz":{"httpGet":{"path":"/ready","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cgsst (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  store-gateway:
    Container ID:  containerd://284d26943197a339621da9e1c590e85706680be170232880ac919797834824a6
    Image:         grafana/mimir:3.0.0
    Image ID:      docker.io/grafana/mimir@sha256:dfa02581c519a28e2ce50c7304d05a635bb1e14b36e2f8a7a54815b36d4d1f9b
    Ports:         8080/TCP, 9095/TCP, 7946/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      -target=store-gateway
      -config.expand-env=true
      -config.file=/etc/mimir/mimir.yaml
      -server.grpc-max-send-msg-size-bytes=209715200
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:39 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15020/app-health/store-gateway/readyz delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOMAXPROCS:  5
      GOMEMLIMIT:  1048576
    Mounts:
      /active-query-tracker from active-queries (rw)
      /data from storage (rw)
      /etc/mimir from config (rw)
      /var/mimir from runtime-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cgsst (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-orchestrator-observability-mimir-store-gateway-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-mimir-runtime
    Optional:  false
  active-queries:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-cgsst:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/component=store-gateway,app.kubernetes.io/instance=orchestrator-observability,app.kubernetes.io/name=mimir
Events:                       <none>


Name:             orchestrator-observability-minio-8557cf7d48-k6skv
Namespace:        orch-platform
Priority:         0
Service Account:  minio-sa
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:36:26 +0000
Labels:           app=minio
                  orchestrator/service=observability
                  pod-template-hash=8557cf7d48
                  release=orchestrator-observability
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=minio
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 098dde2f691ba149bbe46198125123931f1024ff7642ee1f50b4ebfc0ffb63b3
                  checksum/secrets: 40f852176adb55aa1a5c78615c26e933a910cc61543477cc4ff3767085fdbb37
                  cni.projectcalico.org/containerID: d400f513436a7b32194bebce35b4c76a072758f16655e89dd124a8188c966ad4
                  cni.projectcalico.org/podIP: 10.42.65.185/32
                  cni.projectcalico.org/podIPs: 10.42.65.185/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: minio
                  kubectl.kubernetes.io/default-logs-container: minio
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.185
IPs:
  IP:           10.42.65.185
Controlled By:  ReplicaSet/orchestrator-observability-minio-8557cf7d48
Init Containers:
  istio-init:
    Container ID:  containerd://613b29c91a5bc85d997516a6cc27388cc28b9edb3d28486e2ea644e84f0a9f3d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:29 +0000
      Finished:     Tue, 03 Feb 2026 17:36:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6d2nl (ro)
  istio-proxy:
    Container ID:  containerd://b095daa29e956fd20f1ae795e03f318c2ff335d8d44c7d645e61d9b43b0508ee
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:31 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-minio-8557cf7d48-k6skv (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":9000,"protocol":"TCP"}
                                         ,{"name":"http-console","containerPort":9001,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     minio
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-minio
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-minio
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6d2nl (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  minio:
    Container ID:  containerd://27166354fd9695aff6c92b60a66d7b92cd2131f6cd0c866a4c58035bec3cea38
    Image:         quay.io/minio/minio:RELEASE.2025-09-07T16-13-09Z
    Image ID:      quay.io/minio/minio@sha256:14cea493d9a34af32f524e538b8346cf79f3321eff8e708c1e2960462bd8936e
    Ports:         9000/TCP, 9001/TCP
    Host Ports:    0/TCP, 0/TCP
    Command:
      /bin/sh
      -ce
      /usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/ --address :9000 --console-address :9001
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      MINIO_ROOT_USER:             <set to the key 'rootUser' in secret 'orchestrator-observability-minio'>      Optional: false
      MINIO_ROOT_PASSWORD:         <set to the key 'rootPassword' in secret 'orchestrator-observability-minio'>  Optional: false
      MINIO_PROMETHEUS_AUTH_TYPE:  public
    Mounts:
      /export from export (rw)
      /tmp/credentials from minio-user (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6d2nl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  export:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  orchestrator-observability-minio
    ReadOnly:   false
  minio-user:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  orchestrator-observability-minio
    Optional:    false
  kube-api-access-6d2nl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-observability-opentelemetry-collector-6d75f56mzl5n
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-opentelemetry-collector
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:41 +0000
Labels:           app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=opentelemetry-collector
                  component=standalone-collector
                  pod-template-hash=6d75f56454
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=opentelemetry-collector
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: a6aee67e68dc12e44422c0b785487dc2679adfe9de5b3accb583f1d2625cd16b
                  cni.projectcalico.org/containerID: cb427f603791e8da2b2f543c8eb10e5b3a9376791e25d489617f113c83f4f970
                  cni.projectcalico.org/podIP: 10.42.65.163/32
                  cni.projectcalico.org/podIPs: 10.42.65.163/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: opentelemetry-collector
                  kubectl.kubernetes.io/default-logs-container: opentelemetry-collector
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.163
IPs:
  IP:           10.42.65.163
Controlled By:  ReplicaSet/orchestrator-observability-opentelemetry-collector-6d75f56454
Init Containers:
  istio-init:
    Container ID:  containerd://83853bcd6f2f3790a7944b045e0c4e6c5483c0794828fd983ce330863e37bd81
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:57 +0000
      Finished:     Tue, 03 Feb 2026 17:35:57 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8qhdw (ro)
  istio-proxy:
    Container ID:  containerd://61edc4399d852ff4e2743285d2d7cb89e40fa0f8c7badd725c699491ac5945b3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-opentelemetry-collector-6d75f56mzl5n (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"jaeger-compact","containerPort":6831,"protocol":"UDP"}
                                         ,{"name":"jaeger-grpc","containerPort":14250,"protocol":"TCP"}
                                         ,{"name":"jaeger-thrift","containerPort":14268,"protocol":"TCP"}
                                         ,{"name":"metrics","containerPort":8888,"protocol":"TCP"}
                                         ,{"name":"otlp","containerPort":4317,"protocol":"TCP"}
                                         ,{"name":"otlp-http","containerPort":4318,"protocol":"TCP"}
                                         ,{"name":"zipkin","containerPort":9411,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     opentelemetry-collector
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-opentelemetry-collector
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-observability-opentelemetry-collector
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/opentelemetry-collector/livez":{"httpGet":{"path":"/","port":13133,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/opentelemetry-collector/readyz":{"httpGet":{"path":"/","port":13133,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8qhdw (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  opentelemetry-collector:
    Container ID:  containerd://b3fbd8d023a931237d4e67ff82d9e27887da62f3b4516c6769661ee7aaf4c298
    Image:         otel/opentelemetry-collector-contrib:0.136.0
    Image ID:      docker.io/otel/opentelemetry-collector-contrib@sha256:45392d534c1edcc809c2d112394029246bc679d2ae5ea7081414a1fc74f2c621
    Ports:         6831/UDP, 14250/TCP, 14268/TCP, 8888/TCP, 4317/TCP, 4318/TCP, 9411/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP
    Command:
      /otelcol-contrib
    Args:
      --config=/conf/relay.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/opentelemetry-collector/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/opentelemetry-collector/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MY_POD_IP:    (v1:status.podIP)
      GOMEMLIMIT:  52428MiB
    Mounts:
      /conf from opentelemetry-collector-configmap (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8qhdw (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  opentelemetry-collector-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-opentelemetry-collector
    Optional:  false
  kube-api-access-8qhdw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-observability-opentelemetry-collector-daemons46rlr
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-observability-opentelemetry-collector-daemonset
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:38 +0000
Labels:           app.kubernetes.io/instance=orchestrator-observability
                  app.kubernetes.io/name=opentelemetry-collector-daemonset
                  component=agent-collector
                  controller-revision-hash=7b64fc57f8
                  pod-template-generation=1
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=opentelemetry-collector-daemonset
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/config: 88f8b9c076635f2b703ee540e7bf36ea7594e8144381e724d073ecbe7729c47d
                  cni.projectcalico.org/containerID: 221c4161b0dfd678fcfa300eb3075cdc4abe1540fc897f3015b7b7f042067221
                  cni.projectcalico.org/podIP: 10.42.65.74/32
                  cni.projectcalico.org/podIPs: 10.42.65.74/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: opentelemetry-collector-daemonset
                  kubectl.kubernetes.io/default-logs-container: opentelemetry-collector-daemonset
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.74
IPs:
  IP:           10.42.65.74
Controlled By:  DaemonSet/orchestrator-observability-opentelemetry-collector-daemonset-agent
Init Containers:
  istio-init:
    Container ID:  containerd://7aa58923be6c8aa6879b96d6a30ecca72eb4c47915d2f934475c0053eda82064
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:47 +0000
      Finished:     Tue, 03 Feb 2026 17:35:47 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-65v4d (ro)
  istio-proxy:
    Container ID:  containerd://c7017786ae10640ab6aa5b7f83c3cc59cf7f1f3156c24ded28e53f42cdc51a35
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:53 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-observability-opentelemetry-collector-daemons46rlr (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"jaeger-compact","hostPort":6831,"containerPort":6831,"protocol":"UDP"}
                                         ,{"name":"jaeger-grpc","hostPort":14250,"containerPort":14250,"protocol":"TCP"}
                                         ,{"name":"jaeger-thrift","hostPort":14268,"containerPort":14268,"protocol":"TCP"}
                                         ,{"name":"metrics","containerPort":8888,"protocol":"TCP"}
                                         ,{"name":"otlp","hostPort":4317,"containerPort":4317,"protocol":"TCP"}
                                         ,{"name":"otlp-http","hostPort":4318,"containerPort":4318,"protocol":"TCP"}
                                         ,{"name":"zipkin","hostPort":9411,"containerPort":9411,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     opentelemetry-collector-daemonset
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-observability-opentelemetry-collector-daemonset-agent
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/daemonsets/orchestrator-observability-opentelemetry-collector-daemonset-agent
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/opentelemetry-collector-daemonset/livez":{"httpGet":{"path":"/","port":13133,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/opentelemetry-collector-daemonset/readyz":{"httpGet":{"path":"/","port":13133,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-65v4d (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  opentelemetry-collector-daemonset:
    Container ID:  containerd://932f8f1bb7fef63a03146d5600cc192561e61adaf866fba467b6e9315fb65250
    Image:         otel/opentelemetry-collector-contrib:0.136.0
    Image ID:      docker.io/otel/opentelemetry-collector-contrib@sha256:45392d534c1edcc809c2d112394029246bc679d2ae5ea7081414a1fc74f2c621
    Ports:         6831/UDP, 14250/TCP, 14268/TCP, 8888/TCP, 4317/TCP, 4318/TCP, 9411/TCP
    Host Ports:    6831/UDP, 14250/TCP, 14268/TCP, 0/TCP, 4317/TCP, 4318/TCP, 9411/TCP
    Command:
      /otelcol-contrib
    Args:
      --config=/conf/relay.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/opentelemetry-collector-daemonset/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/opentelemetry-collector-daemonset/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MY_POD_IP:       (v1:status.podIP)
      K8S_NODE_NAME:   (v1:spec.nodeName)
      K8S_NODE_IP:     (v1:status.hostIP)
      GOMEMLIMIT:     52428MiB
    Mounts:
      /conf from opentelemetry-collector-daemonset-configmap (rw)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log/pods from varlogpods (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-65v4d (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  opentelemetry-collector-daemonset-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      orchestrator-observability-opentelemetry-collector-daemonset-agent
    Optional:  false
  varlogpods:
    Type:          HostPath (bare host directory volume)
    Path:          /var/log/pods
    HostPathType:  
  varlibdockercontainers:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/docker/containers
    HostPathType:  
  kube-api-access-65v4d:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:                      <none>


Name:             orchestrator-prometheus-ag-operator-7fdb85f887-bmj82
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-prometheus-ag-operator
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:30 +0000
Labels:           app=kube-prometheus-stack-operator
                  app.kubernetes.io/component=prometheus-operator
                  app.kubernetes.io/instance=orchestrator-prometheus-agent
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=kube-prometheus-stack-prometheus-operator
                  app.kubernetes.io/part-of=kube-prometheus-stack
                  app.kubernetes.io/version=79.7.1
                  chart=kube-prometheus-stack-79.7.1
                  heritage=Helm
                  pod-template-hash=7fdb85f887
                  release=orchestrator-prometheus-agent
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=kube-prometheus-stack-prometheus-operator
                  service.istio.io/canonical-revision=79.7.1
Annotations:      cni.projectcalico.org/containerID: 094c5b2d4b0a5829de515704575b18df4d5387553c481f935191413ebe52edaa
                  cni.projectcalico.org/podIP: 10.42.65.77/32
                  cni.projectcalico.org/podIPs: 10.42.65.77/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: kube-prometheus-stack
                  kubectl.kubernetes.io/default-logs-container: kube-prometheus-stack
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.77
IPs:
  IP:           10.42.65.77
Controlled By:  ReplicaSet/orchestrator-prometheus-ag-operator-7fdb85f887
Init Containers:
  istio-init:
    Container ID:  containerd://9b915a03d9c167ece5e567a52c513c1e97f7d65d81da006c82d86e2cec37e5f0
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:31 +0000
      Finished:     Tue, 03 Feb 2026 17:35:31 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sgqmm (ro)
  istio-proxy:
    Container ID:  containerd://2c24b87fa899c69ebeb9ac0ef72eeb6c14e03c913ff95bf62f596f07097c5af3
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:32 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-prometheus-ag-operator-7fdb85f887-bmj82 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     kube-prometheus-stack
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-prometheus-ag-operator
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-prometheus-ag-operator
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/kube-prometheus-stack/livez":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/kube-prometheus-stack/readyz":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sgqmm (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  kube-prometheus-stack:
    Container ID:  containerd://33ef269c14d5ce363b1ccb60d7e553974fbd037c3b0cc7fd8a51c18de86d8d38
    Image:         quay.io/prometheus-operator/prometheus-operator:v0.86.2
    Image ID:      quay.io/prometheus-operator/prometheus-operator@sha256:92757e4b90027e153dc09f2e01254c8402fd5268827d95532760836c2a117062
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-service=kube-system/orchestrator-prometheus-ag-kubelet
      --kubelet-endpoints=true
      --kubelet-endpointslice=false
      --localhost=127.0.0.1
      --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      --config-reloader-cpu-request=1m
      --config-reloader-cpu-limit=64
      --config-reloader-memory-request=1Mi
      --config-reloader-memory-limit=64Gi
      --alertmanager-instance-namespaces=orch-platform
      --alertmanager-config-namespaces=orch-platform
      --prometheus-instance-namespaces=orch-platform
      --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
      --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      --web.enable-tls=false
      --web.listen-address=:8080
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:42 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/kube-prometheus-stack/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/kube-prometheus-stack/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      GOGC:  30
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sgqmm (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-sgqmm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             orchestrator-prometheus-agent-kube-state-metrics-cc7cf7794vc9pc
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-prometheus-agent-kube-state-metrics
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:30 +0000
Labels:           app.kubernetes.io/component=metrics
                  app.kubernetes.io/instance=orchestrator-prometheus-agent
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=kube-state-metrics
                  app.kubernetes.io/part-of=kube-state-metrics
                  app.kubernetes.io/version=2.17.0
                  helm.sh/chart=kube-state-metrics-6.4.1
                  pod-template-hash=cc7cf7794
                  release=orchestrator-prometheus-agent
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=kube-state-metrics
                  service.istio.io/canonical-revision=2.17.0
Annotations:      cni.projectcalico.org/containerID: 30b98582406271c8e6123fd09f20a033ef89a8a962c7c11189b2f280f4f878c3
                  cni.projectcalico.org/podIP: 10.42.65.84/32
                  cni.projectcalico.org/podIPs: 10.42.65.84/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: kube-state-metrics
                  kubectl.kubernetes.io/default-logs-container: kube-state-metrics
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.84
IPs:
  IP:           10.42.65.84
Controlled By:  ReplicaSet/orchestrator-prometheus-agent-kube-state-metrics-cc7cf7794
Init Containers:
  istio-init:
    Container ID:  containerd://b75ef2de4c0d0854d424517ddd81ec9fb1bda7f7fc8b959f4e77e092cdc70fee
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:31 +0000
      Finished:     Tue, 03 Feb 2026 17:35:31 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qjgbj (ro)
  istio-proxy:
    Container ID:  containerd://e92e0c62734fe97d125017efc9c3f28e1a85f46d53f7f4fe9e4d683456aad661
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:32 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      orchestrator-prometheus-agent-kube-state-metrics-cc7cf7794vc9pc (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     kube-state-metrics
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      orchestrator-prometheus-agent-kube-state-metrics
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/orchestrator-prometheus-agent-kube-state-metrics
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/kube-state-metrics/livez":{"httpGet":{"path":"/livez","port":8080,"scheme":"HTTP"},"timeoutSeconds":5},"/app-health/kube-state-metrics/readyz":{"httpGet":{"path":"/readyz","port":8081,"scheme":"HTTP"},"timeoutSeconds":5}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qjgbj (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  kube-state-metrics:
    Container ID:  containerd://b30643beb422cccae348cf15592084139d1cbc95e1fb2b3d98f9bfae5665949e
    Image:         registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
    Image ID:      registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:2bbc915567334b13632bf62c0a97084aff72a36e13c4dabd5f2f11c898c5bacd
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      --port=8080
      --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/kube-state-metrics/livez delay=5s timeout=5s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/kube-state-metrics/readyz delay=5s timeout=5s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qjgbj (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-qjgbj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             platform-keycloak-0
Namespace:        orch-platform
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:33:01 +0000
Labels:           app=keycloak
                  app.kubernetes.io/component=server
                  app.kubernetes.io/instance=platform-keycloak
                  app.kubernetes.io/managed-by=keycloak-operator
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=platform-keycloak-79fdf47fdc
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=keycloak
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=platform-keycloak-0
Annotations:      cni.projectcalico.org/containerID: 9086ddb380670ea13bb0ad73163d13e0280b810ae07af3e67f6c9f2b5deb1072
                  cni.projectcalico.org/podIP: 10.42.65.113/32
                  cni.projectcalico.org/podIPs: 10.42.65.113/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: keycloak
                  kubectl.kubernetes.io/default-logs-container: keycloak
                  operator.keycloak.org/watched-secret-hash: f03148dcd412b83db766d28eb3042dde7b25e11c786d897e1f5471a96d468867
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.113
IPs:
  IP:           10.42.65.113
Controlled By:  StatefulSet/platform-keycloak
Init Containers:
  istio-init:
    Container ID:  containerd://09835677b607831524fec5ba14172a457176c2d1a8f927dc7cd5937c4cd84793
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:33:05 +0000
      Finished:     Tue, 03 Feb 2026 17:33:05 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8l8wz (ro)
  istio-proxy:
    Container ID:  containerd://4d45d5d728013d623be12ebb33e82e877982c733c8dd5d2be3c6506c4d268d28
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      platform-keycloak-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"https","containerPort":8443,"protocol":"TCP"}
                                         ,{"name":"http","containerPort":8080,"protocol":"TCP"}
                                         ,{"name":"management","containerPort":9000,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     keycloak
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      platform-keycloak
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/platform-keycloak
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/keycloak/livez":{"httpGet":{"path":"/health/live","port":9000,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/keycloak/readyz":{"httpGet":{"path":"/health/ready","port":9000,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/keycloak/startupz":{"httpGet":{"path":"/health/started","port":9000,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8l8wz (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  keycloak-builder:
    Container ID:    containerd://081b9f1c87b9ade5c4292ff2e2f36e50582ac3c0a57a791c6e75231865e524e7
    Image:           quay.io/keycloak/keycloak:26.5.0
    Image ID:        quay.io/keycloak/keycloak@sha256:5fdd7cda82e58775ed124294c7e16fabc33166d38dfc4aabebda7d64e7a964bf
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/bash
    Args:
      -c
      set -e
      /opt/keycloak/bin/kc.sh build \
        --db=postgres \
        --health-enabled=true \
        --metrics-enabled=true \
        --http-relative-path=/
      
      mkdir -p /shared/keycloak
      cp -r /opt/keycloak/* /shared/keycloak/
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:33:15 +0000
      Finished:     Tue, 03 Feb 2026 17:33:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1500m
      memory:  1Gi
    Requests:
      cpu:        500m
      memory:     512Mi
    Environment:  <none>
    Mounts:
      /shared/keycloak from shared-keycloak (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8l8wz (ro)
Containers:
  keycloak:
    Container ID:    containerd://eb3aa4634d626c6ee0177c64cd805ca1662efab615c64cfb7cbed69ab1969c20
    Image:           quay.io/keycloak/keycloak:26.5.0
    Image ID:        quay.io/keycloak/keycloak@sha256:5fdd7cda82e58775ed124294c7e16fabc33166d38dfc4aabebda7d64e7a964bf
    Ports:           8443/TCP, 8080/TCP, 9000/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -Djgroups.bind.address=$(POD_IP)
      --verbose
      start
      --optimized
    State:          Running
      Started:      Tue, 03 Feb 2026 17:33:39 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  2Gi
    Requests:
      cpu:      500m
      memory:   512Mi
    Liveness:   http-get http://:15020/app-health/keycloak/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/keycloak/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Startup:    http-get http://:15020/app-health/keycloak/startupz delay=0s timeout=1s period=1s #success=1 #failure=600
    Environment:
      KC_HTTP_ENABLED:                                                                         true
      KC_HTTP_PORT:                                                                            8080
      KC_HTTPS_PORT:                                                                           8443
      KC_DB:                                                                                   postgres
      KC_DB_USERNAME:                                                                          <set to the key 'PGUSER' in secret 'platform-keycloak-local-postgresql'>      Optional: false
      KC_DB_PASSWORD:                                                                          <set to the key 'PGPASSWORD' in secret 'platform-keycloak-local-postgresql'>  Optional: false
      KC_DB_URL_DATABASE:                                                                      orch-platform-platform-keycloak
      KC_DB_POOL_INITIAL_SIZE:                                                                 5
      KC_DB_POOL_MIN_SIZE:                                                                     5
      KC_DB_POOL_MAX_SIZE:                                                                     50
      KC_PROXY_HEADERS:                                                                        xforwarded
      KC_BOOTSTRAP_ADMIN_USERNAME:                                                             <set to the key 'username' in secret 'platform-keycloak'>  Optional: false
      KC_BOOTSTRAP_ADMIN_PASSWORD:                                                             <set to the key 'password' in secret 'platform-keycloak'>  Optional: false
      KC_HEALTH_ENABLED:                                                                       true
      KC_CACHE:                                                                                ispn
      KC_DB_URL_HOST:                                                                          <set to the key 'PGHOST' in secret 'platform-keycloak-local-postgresql'>  Optional: false
      KCKEY_DB_URL_HOST:                                                                       db-url-host
      KC_DB_URL_PORT:                                                                          <set to the key 'PGPORT' in secret 'platform-keycloak-local-postgresql'>  Optional: false
      KCKEY_DB_URL_PORT:                                                                       db-url-port
      KCKEY_DB_URL_DATABASE:                                                                   db-url-database
      KC_HOSTNAME_URL:                                                                         <set to the key 'PGDATABASE' in secret 'platform-keycloak-local-postgresql'>  Optional: false
      KCKEY_HOSTNAME_URL:                                                                      hostname-url
      KC_HOSTNAME_STRICT:                                                                      false
      KCKEY_HOSTNAME_STRICT:                                                                   hostname-strict
      KC_PROXY_ADDRESS_FORWARDING:                                                             true
      KCKEY_PROXY_ADDRESS_FORWARDING:                                                          proxy-address-forwarding
      KC_HTTP_RELATIVE_PATH:                                                                   /
      KCKEY_HTTP_RELATIVE_PATH:                                                                http-relative-path
      KCKEY_HTTP_ENABLED:                                                                      http-enabled
      KC_DB_URL_PROPERTIES:                                                                    ?tcpKeepAlives=true&socketTimeout=120&connectTimeout=120
      KCKEY_DB_URL_PROPERTIES:                                                                 db-url-properties
      KC_HTTP_MANAGEMENT_PORT:                                                                 9000
      KCKEY_HTTP_MANAGEMENT_PORT:                                                              http-management-port
      KC_SPI_LOGIN_PROTOCOL_OPENID_CONNECT_LEGACY_LOGOUT_REDIRECT_URI:                         true
      KCKEY_SPI_LOGIN_PROTOCOL_OPENID_CONNECT_LEGACY_LOGOUT_REDIRECT_URI:                      spi-login-protocol-openid-connect-legacy-logout-redirect-uri
      KC_SPI_BRUTE_FORCE_PROTECTOR_DEFAULT_BRUTE_FORCE_DETECTOR_ALLOW_CONCURRENT_REQUESTS:     true
      KCKEY_SPI_BRUTE_FORCE_PROTECTOR_DEFAULT_BRUTE_FORCE_DETECTOR_ALLOW_CONCURRENT_REQUESTS:  spi-brute-force-protector-default-brute-force-detector-allow-concurrent-requests
      KC_LOG_LEVEL:                                                                            INFO
      KCKEY_LOG_LEVEL:                                                                         log-level
      KC_LOG_CONSOLE_OUTPUT:                                                                   json
      KCKEY_LOG_CONSOLE_OUTPUT:                                                                log-console-output
      POD_IP:                                                                                   (v1:status.podIP)
      KC_SPI_CACHE_EMBEDDED_DEFAULT_MACHINE_NAME:                                               (v1:spec.nodeName)
      KC_TRUSTSTORE_PATHS:                                                                     /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      KC_TRACING_SERVICE_NAME:                                                                 platform-keycloak
      KC_TRACING_RESOURCE_ATTRIBUTES:                                                          k8s.namespace.name=orch-platform
    Mounts:
      /opt/keycloak from shared-keycloak (ro)
      /opt/keycloak/data from keycloak-data (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8l8wz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  shared-keycloak:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  1Gi
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  100Mi
  keycloak-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  500Mi
  kube-api-access-8l8wz:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Burstable
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app=keycloak,app.kubernetes.io/component=server,app.kubernetes.io/instance=platform-keycloak,app.kubernetes.io/managed-by=keycloak-operator
                              topology.kubernetes.io/zone:ScheduleAnyway when max skew 1 is exceeded for selector app=keycloak,app.kubernetes.io/component=server,app.kubernetes.io/instance=platform-keycloak,app.kubernetes.io/managed-by=keycloak-operator
Events:                       <none>


Name:             prometheus-orchestrator-prometheus-ag-prometheus-0
Namespace:        orch-platform
Priority:         0
Service Account:  orchestrator-prometheus-ag-prometheus
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:44 +0000
Labels:           app.kubernetes.io/instance=orchestrator-prometheus-ag-prometheus
                  app.kubernetes.io/managed-by=prometheus-operator
                  app.kubernetes.io/name=prometheus
                  app.kubernetes.io/version=3.7.3
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=prometheus-orchestrator-prometheus-ag-prometheus-f89d9f9d4
                  operator.prometheus.io/name=orchestrator-prometheus-ag-prometheus
                  operator.prometheus.io/shard=0
                  prometheus=orchestrator-prometheus-ag-prometheus
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=prometheus
                  service.istio.io/canonical-revision=3.7.3
                  statefulset.kubernetes.io/pod-name=prometheus-orchestrator-prometheus-ag-prometheus-0
Annotations:      cni.projectcalico.org/containerID: eed32be64d93d2279191804b8fa53e53d521ea70bd32599819f8343af245644a
                  cni.projectcalico.org/podIP: 10.42.65.165/32
                  cni.projectcalico.org/podIPs: 10.42.65.165/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: prometheus
                  kubectl.kubernetes.io/default-logs-container: prometheus
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
                  traffic.sidecar.istio.io/excludeOutboundPorts: 15090
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.165
IPs:
  IP:           10.42.65.165
Controlled By:  StatefulSet/prometheus-orchestrator-prometheus-ag-prometheus
Init Containers:
  istio-init:
    Container ID:  containerd://ccde6a0c7c74f13c0c53e1d31853cd34a58eede87c9b4ce6ae97dfff40000057
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      -o
      15090
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:58 +0000
      Finished:     Tue, 03 Feb 2026 17:35:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-246s8 (ro)
  istio-proxy:
    Container ID:  containerd://8631ab0d5c702fce5e69f95c91de4b633d4bb3ee1a642ee71434aba44b30e9cc
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:03 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      prometheus-orchestrator-prometheus-ag-prometheus-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http-web","containerPort":9090,"protocol":"TCP"}
                                         ,{"name":"reloader-web","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     prometheus,config-reloader
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      prometheus-orchestrator-prometheus-ag-prometheus
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/prometheus-orchestrator-prometheus-ag-prometheus
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/prometheus/livez":{"httpGet":{"path":"/-/healthy","port":9090,"scheme":"HTTP"},"timeoutSeconds":3},"/app-health/prometheus/readyz":{"httpGet":{"path":"/-/ready","port":9090,"scheme":"HTTP"},"timeoutSeconds":3},"/app-health/prometheus/startupz":{"httpGet":{"path":"/-/ready","port":9090,"scheme":"HTTP"},"timeoutSeconds":3}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-246s8 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  init-config-reloader:
    Container ID:  containerd://f62be005cf5e7f3af1f0ab77ed5c966fec6100a4abbb109d1bff86055d58797a
    Image:         quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
    Image ID:      quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
    Port:          8081/TCP
    Host Port:     0/TCP
    Command:
      /bin/prometheus-config-reloader
    Args:
      --watch-interval=0
      --listen-address=:8081
      --config-file=/etc/prometheus/config/prometheus.yaml.gz
      --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      --watched-dir=/etc/prometheus/rules/prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:36:13 +0000
      Finished:     Tue, 03 Feb 2026 17:36:13 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      POD_NAME:  prometheus-orchestrator-prometheus-ag-prometheus-0 (v1:metadata.name)
      SHARD:     0
    Mounts:
      /etc/prometheus/config from config (rw)
      /etc/prometheus/config_out from config-out (rw)
      /etc/prometheus/rules/prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0 from prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-246s8 (ro)
Containers:
  prometheus:
    Container ID:  containerd://e7df9603dc7ea4d314f4aa698425fbcd10fc971ec5a8691cae391fabc68723d1
    Image:         quay.io/prometheus/prometheus:v3.7.3
    Image ID:      quay.io/prometheus/prometheus@sha256:49214755b6153f90a597adcbff0252cc61069f8ab69ce8411285cd4a560e8038
    Port:          9090/TCP
    Host Port:     0/TCP
    Args:
      --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      --web.enable-lifecycle
      --web.external-url=http://orchestrator-prometheus-ag-prometheus.orch-platform:9090
      --web.route-prefix=/
      --storage.tsdb.retention.time=24h
      --storage.tsdb.path=/prometheus
      --storage.tsdb.wal-compression
      --web.config.file=/etc/prometheus/web_config/web-config.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/prometheus/livez delay=0s timeout=3s period=5s #success=1 #failure=6
    Readiness:    http-get http://:15020/app-health/prometheus/readyz delay=0s timeout=3s period=5s #success=1 #failure=3
    Startup:      http-get http://:15020/app-health/prometheus/startupz delay=0s timeout=3s period=15s #success=1 #failure=60
    Environment:  <none>
    Mounts:
      /etc/prometheus/certs from tls-assets (ro)
      /etc/prometheus/config_out from config-out (ro)
      /etc/prometheus/rules/prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0 from prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0 (rw)
      /etc/prometheus/web_config/web-config.yaml from web-config (ro,path="web-config.yaml")
      /prometheus from prometheus-orchestrator-prometheus-ag-prometheus-db (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-246s8 (ro)
  config-reloader:
    Container ID:  containerd://3f81eed17cea6f1be598efffd6c2394221764194d2b71515496b8e6391aeab54
    Image:         quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
    Image ID:      quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
    Port:          8080/TCP
    Host Port:     0/TCP
    Command:
      /bin/prometheus-config-reloader
    Args:
      --listen-address=:8080
      --reload-url=http://127.0.0.1:9090/-/reload
      --config-file=/etc/prometheus/config/prometheus.yaml.gz
      --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      --watched-dir=/etc/prometheus/rules/prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0
    State:          Running
      Started:      Tue, 03 Feb 2026 17:36:34 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      POD_NAME:  prometheus-orchestrator-prometheus-ag-prometheus-0 (v1:metadata.name)
      SHARD:     0
    Mounts:
      /etc/prometheus/config from config (rw)
      /etc/prometheus/config_out from config-out (rw)
      /etc/prometheus/rules/prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0 from prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-246s8 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-orchestrator-prometheus-ag-prometheus
    Optional:    false
  tls-assets:
    Type:                Projected (a volume that contains injected data from multiple sources)
    SecretName:          prometheus-orchestrator-prometheus-ag-prometheus-tls-assets-0
    SecretOptionalName:  <nil>
  config-out:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-orchestrator-prometheus-ag-prometheus-rulefiles-0
    Optional:  false
  web-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-orchestrator-prometheus-ag-prometheus-web-config
    Optional:    false
  prometheus-orchestrator-prometheus-ag-prometheus-db:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-246s8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             reloader-reloader-75dc59d467-h7rnc
Namespace:        orch-platform
Priority:         0
Service Account:  reloader-reloader
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:32:28 +0000
Labels:           app=reloader-reloader
                  app.kubernetes.io/instance=reloader
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=reloader
                  app.kubernetes.io/version=v1.4.8
                  chart=reloader-2.2.3
                  group=com.stakater.platform
                  helm.sh/chart=reloader-2.2.3
                  heritage=Helm
                  pod-template-hash=75dc59d467
                  provider=stakater
                  release=reloader
                  version=v1.4.8
Annotations:      cni.projectcalico.org/containerID: 7a8e63277daf9b233794ce15d1f584a4f5d4c90817a4da2f12bb43b0ada8716f
                  cni.projectcalico.org/podIP: 10.42.65.104/32
                  cni.projectcalico.org/podIPs: 10.42.65.104/32
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.104
IPs:
  IP:           10.42.65.104
Controlled By:  ReplicaSet/reloader-reloader-75dc59d467
Containers:
  reloader-reloader:
    Container ID:    containerd://ceabec91657e0ae2769685817a1b15421ec0733c2ab86e267e0353c79f0557aa
    Image:           ghcr.io/stakater/reloader:v1.4.8
    Image ID:        ghcr.io/stakater/reloader@sha256:90b6a327ba6d96415c1dc84e968b131a1539898dba4774ec4f843bf154664a48
    Port:            9090/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --log-level=info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:32:30 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:http/live delay=10s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:http/metrics delay=10s timeout=5s period=10s #success=1 #failure=5
    Environment:
      GOMAXPROCS:                64 (limits.cpu)
      GOMEMLIMIT:                68719476736 (limits.memory)
      RELOADER_NAMESPACE:        orch-platform (v1:metadata.namespace)
      RELOADER_DEPLOYMENT_NAME:  reloader-reloader
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6jfxs (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-6jfxs:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             rs-proxy-7dd87b8df7-v9r8p
Namespace:        orch-platform
Priority:         0
Service Account:  rs-proxy
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:12 +0000
Labels:           app.kubernetes.io/instance=rs-proxy
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=rs-proxy
                  app.kubernetes.io/version=2.10.2
                  helm.sh/chart=rs-proxy-25.2.3
                  pod-template-hash=7dd87b8df7
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=rs-proxy
                  service.istio.io/canonical-revision=2.10.2
Annotations:      cni.projectcalico.org/containerID: 9739143409256d8dcef873098547760bec803e95ec334f5d05cdb486e8e6fc3e
                  cni.projectcalico.org/podIP: 10.42.65.125/32
                  cni.projectcalico.org/podIPs: 10.42.65.125/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: rs-proxy
                  kubectl.kubernetes.io/default-logs-container: rs-proxy
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  reloader.stakater.com/auto: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.125
IPs:
  IP:           10.42.65.125
Controlled By:  ReplicaSet/rs-proxy-7dd87b8df7
Init Containers:
  istio-init:
    Container ID:  containerd://d23bc8354e0f3c7f9c68486892f706df71d98633c405fe3c25af1c7dc8ba9498
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:12 +0000
      Finished:     Tue, 03 Feb 2026 17:35:13 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mfqmk (ro)
  istio-proxy:
    Container ID:  containerd://7297ba5ae3cb227b9b4af72f13f84fd5ec64fb7c9f01493b641d0198ed1331eb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      rs-proxy-7dd87b8df7-v9r8p (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8081,"protocol":"TCP"}
                                         ,{"name":"https","containerPort":8443,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     rs-proxy
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      rs-proxy
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/rs-proxy
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/rs-proxy/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":10},"/app-health/rs-proxy/readyz":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":10}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mfqmk (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  rs-proxy:
    Container ID:    containerd://09038b89da1797454698f3fbfdfd6595b3a8f93dd6c6172d5ea65982d36004fa
    Image:           caddy:2.10.2
    Image ID:        docker.io/library/caddy@sha256:f20f80e1fb627294fb84b8515b7593aff8018c840f1396dc942a50ed0c2db648
    Ports:           8081/TCP, 8443/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:35:17 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/rs-proxy/livez delay=0s timeout=10s period=15s #success=1 #failure=5
    Readiness:    http-get http://:15020/app-health/rs-proxy/readyz delay=0s timeout=10s period=15s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /config/caddy from caddy-config (rw)
      /data-tls from certs-https (ro)
      /data/caddy from caddy-certs (rw)
      /etc/caddy from config (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mfqmk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  caddy-config
    Optional:    false
  certs-https:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  tls-rs-proxy
    Optional:    false
  caddy-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  500Mi
  caddy-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  500Mi
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  500Mi
  kube-api-access-mfqmk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             rs-proxy-files-86bb7fc666-qw67f
Namespace:        orch-platform
Priority:         0
Service Account:  rs-proxy
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:35:12 +0000
Labels:           app.kubernetes.io/instance=rs-proxy
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=rs-proxy-files
                  app.kubernetes.io/version=2.10.2
                  helm.sh/chart=rs-proxy-25.2.3
                  pod-template-hash=86bb7fc666
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=rs-proxy-files
                  service.istio.io/canonical-revision=2.10.2
Annotations:      cni.projectcalico.org/containerID: e0f031ad8813c9d615727902e8eb5fbf8d6e5ba77df731b058c6594c5fbaa898
                  cni.projectcalico.org/podIP: 10.42.65.124/32
                  cni.projectcalico.org/podIPs: 10.42.65.124/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: rs-proxy
                  kubectl.kubernetes.io/default-logs-container: rs-proxy
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  reloader.stakater.com/auto: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.124
IPs:
  IP:           10.42.65.124
Controlled By:  ReplicaSet/rs-proxy-files-86bb7fc666
Init Containers:
  istio-init:
    Container ID:  containerd://7b6d666fe6879a78e6d321e5a12ce07e2ab900f2ae41b4f7343361edf23dc266
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:35:12 +0000
      Finished:     Tue, 03 Feb 2026 17:35:12 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n52j7 (ro)
  istio-proxy:
    Container ID:  containerd://06afff05e509d3aee58239f3beee43e12ea7ea61968c5523fec369627cb0497f
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:13 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      rs-proxy-files-86bb7fc666-qw67f (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8081,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     rs-proxy
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      rs-proxy-files
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/rs-proxy-files
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/rs-proxy/livez":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/rs-proxy/readyz":{"httpGet":{"path":"/healthz","port":8081,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n52j7 (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  rs-proxy:
    Container ID:    containerd://ea856a12e0f86a7579cd90e4c860a9d7d0c6aa5b7b654089ce49912849b0bb7f
    Image:           caddy:2.10.2
    Image ID:        docker.io/library/caddy@sha256:f20f80e1fb627294fb84b8515b7593aff8018c840f1396dc942a50ed0c2db648
    Port:            8081/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:35:17 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/rs-proxy/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/rs-proxy/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/caddy from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n52j7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  caddy-config-files
    Optional:    false
  kube-api-access-n52j7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From     Message
  ----     ------     ----  ----     -------
  Warning  Unhealthy  41m   kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:             secrets-config-du2guq18-hbwbv
Namespace:        orch-platform
Priority:         0
Service Account:  secrets-config
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:59:16 +0000
Labels:           app.kubernetes.io/instance=secrets-config
                  app.kubernetes.io/name=secrets-config
                  batch.kubernetes.io/controller-uid=c9da2b71-b9b6-4310-b85e-a972e4d5d181
                  batch.kubernetes.io/job-name=secrets-config-du2guq18
                  controller-uid=c9da2b71-b9b6-4310-b85e-a972e4d5d181
                  job-name=secrets-config-du2guq18
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=secrets-config
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: c182cdb7cb02903c52b26916c2dd853f07584afe086d55953aaca64b376cdd49
                  cni.projectcalico.org/podIP: 
                  cni.projectcalico.org/podIPs: 
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: secrets-config
                  kubectl.kubernetes.io/default-logs-container: secrets-config
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Succeeded
IP:               10.42.65.255
IPs:
  IP:           10.42.65.255
Controlled By:  Job/secrets-config-du2guq18
Init Containers:
  istio-init:
    Container ID:  containerd://aeac3a7a0d732f74c19206f6b26f9754b0d537a4ea55ec09a78d0b8b8936bd14
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:59:19 +0000
      Finished:     Tue, 03 Feb 2026 17:59:19 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7lcjk (ro)
  istio-proxy:
    Container ID:  containerd://e754514ceaf0f5afba1a69ac057e5257df66eb6cebb7c28295e4400ade57be31
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:59:20 +0000
      Finished:     Tue, 03 Feb 2026 17:59:24 +0000
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      secrets-config-du2guq18-hbwbv (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     secrets-config
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      secrets-config-du2guq18
      ISTIO_META_OWNER:              kubernetes://apis/batch/v1/namespaces/orch-platform/jobs/secrets-config-du2guq18
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7lcjk (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  secrets-config:
    Container ID:    containerd://40133880ff1315f786709a0556c704bf8efd2fead8e9ccd20dc6eb41e3be6fe3
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/common/secrets-config:3.0.2
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/common/secrets-config@sha256:577aca1fe34ef7ed91c84ff7f635655cbc181c55ed03344fe7e91b308993bdfe
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Args:
      -logLevel=info
      -autoInit=true
      -autoUnseal=false
      -authOrchSvcsRoleMaxTTL=1h
      -authOIDCIdPAddr=http://platform-keycloak.orch-platform.svc
      -authOIDCIdPDiscoveryURL=http://platform-keycloak.orch-platform.svc/realms/master
      -authOIDCRoleMaxTTL=1h
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:59:23 +0000
      Finished:     Tue, 03 Feb 2026 17:59:24 +0000
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7lcjk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-7lcjk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  46m   default-scheduler  Successfully assigned orch-platform/secrets-config-du2guq18-hbwbv to orch-tf
  Normal   Pulled     46m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    46m   kubelet            Created container: istio-init
  Normal   Started    46m   kubelet            Started container istio-init
  Normal   Pulled     46m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    46m   kubelet            Created container: istio-proxy
  Normal   Started    46m   kubelet            Started container istio-proxy
  Warning  Unhealthy  46m   kubelet            Startup probe failed: Get "http://10.42.65.255:15021/healthz/ready": dial tcp 10.42.65.255:15021: connect: connection refused
  Normal   Pulled     46m   kubelet            Container image "registry-rs.edgeorchestration.intel.com/edge-orch/common/secrets-config:3.0.2" already present on machine
  Normal   Created    46m   kubelet            Created container: secrets-config
  Normal   Started    46m   kubelet            Started container secrets-config


Name:             vault-0
Namespace:        orch-platform
Priority:         0
Service Account:  vault
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:34:43 +0000
Labels:           app.kubernetes.io/instance=vault
                  app.kubernetes.io/name=vault
                  apps.kubernetes.io/pod-index=0
                  component=server
                  controller-revision-hash=vault-5c5658796d
                  helm.sh/chart=vault-0.31.0
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=vault
                  service.istio.io/canonical-revision=latest
                  statefulset.kubernetes.io/pod-name=vault-0
Annotations:      cni.projectcalico.org/containerID: a272225e1824225498166b32af2ca77931a78be0f7bdfb61ef29a9f8a28963c6
                  cni.projectcalico.org/podIP: 10.42.65.121/32
                  cni.projectcalico.org/podIPs: 10.42.65.121/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: vault
                  kubectl.kubernetes.io/default-logs-container: vault
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.121
IPs:
  IP:           10.42.65.121
Controlled By:  StatefulSet/vault
Init Containers:
  istio-init:
    Container ID:  containerd://0cc8044ca774776f8a5699aefc4417cd2b6684502b79981880d42f422fbe313b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:34:44 +0000
      Finished:     Tue, 03 Feb 2026 17:34:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qmjzw (ro)
  istio-proxy:
    Container ID:  containerd://3528ee68b893b032f9fdb6759e9a169decc1f13a9fc85f0cd10b2b201c3d90b7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:34:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      vault-0 (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8200,"protocol":"TCP"}
                                         ,{"name":"https-internal","containerPort":8201,"protocol":"TCP"}
                                         ,{"name":"http-rep","containerPort":8202,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     vault
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      vault
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/statefulsets/vault
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qmjzw (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
  storage-config:
    Container ID:    containerd://b84a956e199c3bc64bfce1ac77e01d426c24f12434388492ca1b029a7e2b4df4
    Image:           alpine:3.22.2
    Image ID:        docker.io/library/alpine@sha256:4b7ce07002c69e8f3d704a9c5d6fd3053be500b7f1c69fc0d80990c2ad8dd412
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      sh
      -c
    Args:
      echo "storage \"postgresql\" { connection_url = \"postgres://$PGUSER:$PGPASSWORD@$PGHOST:$PGPORT/$PGDATABASE\" ha_enabled=\"$HA_ENABLED\" }" > /vault/userconfig/vault-storage-config/storage.hcl
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:34:47 +0000
      Finished:     Tue, 03 Feb 2026 17:34:47 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      PGUSER:      <set to the key 'PGUSER' in secret 'vault-local-postgresql'>      Optional: false
      PGPASSWORD:  <set to the key 'PGPASSWORD' in secret 'vault-local-postgresql'>  Optional: false
      PGHOST:      <set to the key 'PGHOST' in secret 'vault-local-postgresql'>      Optional: false
      PGPORT:      <set to the key 'PGPORT' in secret 'vault-local-postgresql'>      Optional: false
      PGDATABASE:  <set to the key 'PGDATABASE' in secret 'vault-local-postgresql'>  Optional: false
      HA_ENABLED:  false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qmjzw (ro)
      /vault/userconfig/vault-storage-config from vault-storage-config (rw)
  init-table:
    Container ID:    containerd://19bfa2e7b1fe8e60a27b8d8e4e58ef7d2c6d1ec1cb46be2aa511ff3e348881c3
    Image:           library/postgres:14.19-alpine3.22
    Image ID:        docker.io/library/postgres@sha256:cb54bb67c0fca8b439f18c1daadb315ad67de1faf8c387988c63080d15a54145
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Command:
      bash
      -x
      -c
    Args:
      EXIST=$(psql -h $PGHOST -p $PGPORT -U $PGUSER -d $PGDATABASE -t -A -c \
        "SELECT EXISTS (SELECT FROM pg_tables WHERE tablename  = 'vault_kv_store' OR tablename = 'vault_ha_locks');"
      )
      if [[ $EXIST == "t" ]]; then
        echo "Tables already exist, skipping initialization"
        exit 0
      fi
      
      psql -h $PGHOST -p $PGPORT -U $PGUSER -d $PGDATABASE -c \
      "CREATE TABLE vault_kv_store (
        parent_path TEXT COLLATE \"C\" NOT NULL,
        path        TEXT COLLATE \"C\",
        key         TEXT COLLATE \"C\",
        value       BYTEA,
        CONSTRAINT pkey PRIMARY KEY (path, key)
      );
      CREATE INDEX parent_path_idx ON vault_kv_store (parent_path);
      CREATE TABLE vault_ha_locks (
        ha_key      TEXT COLLATE \"C\" NOT NULL,
        ha_identity TEXT COLLATE \"C\" NOT NULL,
        ha_value    TEXT COLLATE \"C\",
        valid_until TIMESTAMP WITH TIME ZONE NOT NULL,
        CONSTRAINT ha_key PRIMARY KEY (ha_key)
      );"
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:34:55 +0000
      Finished:     Tue, 03 Feb 2026 17:34:55 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      PGUSER:      <set to the key 'PGUSER' in secret 'vault-local-postgresql'>      Optional: false
      PGPASSWORD:  <set to the key 'PGPASSWORD' in secret 'vault-local-postgresql'>  Optional: false
      PGHOST:      <set to the key 'PGHOST' in secret 'vault-local-postgresql'>      Optional: false
      PGPORT:      <set to the key 'PGPORT' in secret 'vault-local-postgresql'>      Optional: false
      PGDATABASE:  <set to the key 'PGDATABASE' in secret 'vault-local-postgresql'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qmjzw (ro)
Containers:
  vault:
    Container ID:    containerd://2118865049db71c77fc03007d1bc9d72bc3bb72ce3685c4f024520089d5ecdcc
    Image:           hashicorp/vault:1.20.4
    Image ID:        docker.io/hashicorp/vault@sha256:268bb80aa9c6d13d65fcfa05c0c268caca068952240a8087291a6ce0b66e3a10
    Ports:           8200/TCP, 8201/TCP, 8202/TCP
    Host Ports:      0/TCP, 0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /bin/sh
      -ec
    Args:
      cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;
      [ -n "${HOST_IP}" ] && sed -Ei "s|HOST_IP|${HOST_IP?}|g" /tmp/storageconfig.hcl;
      [ -n "${POD_IP}" ] && sed -Ei "s|POD_IP|${POD_IP?}|g" /tmp/storageconfig.hcl;
      [ -n "${HOSTNAME}" ] && sed -Ei "s|HOSTNAME|${HOSTNAME?}|g" /tmp/storageconfig.hcl;
      [ -n "${API_ADDR}" ] && sed -Ei "s|API_ADDR|${API_ADDR?}|g" /tmp/storageconfig.hcl;
      [ -n "${TRANSIT_ADDR}" ] && sed -Ei "s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g" /tmp/storageconfig.hcl;
      [ -n "${RAFT_ADDR}" ] && sed -Ei "s|RAFT_ADDR|${RAFT_ADDR?}|g" /tmp/storageconfig.hcl;
      /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl -config=/vault/userconfig/vault-storage-config/storage.hcl
      
    State:          Running
      Started:      Tue, 03 Feb 2026 17:35:02 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  exec [/bin/sh -ec vault status -tls-skip-verify] delay=5s timeout=3s period=5s #success=1 #failure=2
    Environment:
      HOST_IP:               (v1:status.hostIP)
      POD_IP:                (v1:status.podIP)
      VAULT_K8S_POD_NAME:   vault-0 (v1:metadata.name)
      VAULT_K8S_NAMESPACE:  orch-platform (v1:metadata.namespace)
      VAULT_ADDR:           http://127.0.0.1:8200
      VAULT_API_ADDR:       http://$(POD_IP):8200
      SKIP_CHOWN:           true
      SKIP_SETCAP:          true
      HOSTNAME:             vault-0 (v1:metadata.name)
      VAULT_CLUSTER_ADDR:   https://$(HOSTNAME).vault-internal:8201
      HOME:                 /home/vault
      VAULT_LOG_LEVEL:      info
    Mounts:
      /home/vault from home (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qmjzw (ro)
      /vault/config from config (rw)
      /vault/userconfig/vault-storage-config from vault-storage-config (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      vault-config
    Optional:  false
  vault-storage-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  home:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-qmjzw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             vault-agent-injector-76f7c8b8f5-j8kjs
Namespace:        orch-platform
Priority:         0
Service Account:  vault-agent-injector
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:34:43 +0000
Labels:           app.kubernetes.io/instance=vault
                  app.kubernetes.io/name=vault-agent-injector
                  component=webhook
                  pod-template-hash=76f7c8b8f5
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=vault-agent-injector
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 4f5cf5691be545b2ed651ffebe5b27a67d6fc85e0fcad8135bd515be5fd2c76a
                  cni.projectcalico.org/podIP: 10.42.65.120/32
                  cni.projectcalico.org/podIPs: 10.42.65.120/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: sidecar-injector
                  kubectl.kubernetes.io/default-logs-container: sidecar-injector
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.120
IPs:
  IP:           10.42.65.120
Controlled By:  ReplicaSet/vault-agent-injector-76f7c8b8f5
Init Containers:
  istio-init:
    Container ID:  containerd://1ed40976b88d0777c5629181c96dc23825bbf39d757cd35b63db62924621c494
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:34:44 +0000
      Finished:     Tue, 03 Feb 2026 17:34:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-985dz (ro)
  istio-proxy:
    Container ID:  containerd://4adaa3375ecc914905dc4bc9f8549fc92cc3b0561228c8b590a95d9547edf3bb
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:34:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      vault-agent-injector-76f7c8b8f5-j8kjs (v1:metadata.name)
      POD_NAMESPACE:                 orch-platform (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                     ]
      ISTIO_META_APP_CONTAINERS:     sidecar-injector
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      vault-agent-injector
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-platform/deployments/vault-agent-injector
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/sidecar-injector/livez":{"httpGet":{"path":"/health/ready","port":8080,"scheme":"HTTPS"},"timeoutSeconds":5},"/app-health/sidecar-injector/readyz":{"httpGet":{"path":"/health/ready","port":8080,"scheme":"HTTPS"},"timeoutSeconds":5},"/app-health/sidecar-injector/startupz":{"httpGet":{"path":"/health/ready","port":8080,"scheme":"HTTPS"},"timeoutSeconds":5}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-985dz (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  sidecar-injector:
    Container ID:    containerd://a0e470fc5619c81e8eebfc6617bddb667031675f58753ed6a511c9c256ba5ad9
    Image:           hashicorp/vault-k8s:1.7.0
    Image ID:        docker.io/hashicorp/vault-k8s@sha256:8c18ccc87fd72930fd0c3f12ea444e9e57e83f119b93c546ed047aba29a05c5f
    Port:            <none>
    Host Port:       <none>
    SeccompProfile:  RuntimeDefault
    Args:
      agent-inject
      2>&1
    State:          Running
      Started:      Tue, 03 Feb 2026 17:34:48 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/sidecar-injector/livez delay=5s timeout=5s period=2s #success=1 #failure=2
    Readiness:  http-get http://:15020/app-health/sidecar-injector/readyz delay=5s timeout=5s period=2s #success=1 #failure=2
    Startup:    http-get http://:15020/app-health/sidecar-injector/startupz delay=5s timeout=5s period=5s #success=1 #failure=12
    Environment:
      AGENT_INJECT_LISTEN:                                 :8080
      AGENT_INJECT_LOG_LEVEL:                              info
      AGENT_INJECT_VAULT_ADDR:                             http://vault.orch-platform.svc:8200
      AGENT_INJECT_VAULT_AUTH_PATH:                        auth/kubernetes
      AGENT_INJECT_VAULT_IMAGE:                            hashicorp/vault:1.20.4
      AGENT_INJECT_TLS_AUTO:                               vault-agent-injector-cfg
      AGENT_INJECT_TLS_AUTO_HOSTS:                         vault-agent-injector-svc,vault-agent-injector-svc.orch-platform,vault-agent-injector-svc.orch-platform.svc
      AGENT_INJECT_LOG_FORMAT:                             standard
      AGENT_INJECT_REVOKE_ON_SHUTDOWN:                     false
      AGENT_INJECT_CPU_REQUEST:                            250m
      AGENT_INJECT_CPU_LIMIT:                              500m
      AGENT_INJECT_MEM_REQUEST:                            64Mi
      AGENT_INJECT_MEM_LIMIT:                              128Mi
      AGENT_INJECT_DEFAULT_TEMPLATE:                       map
      AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE:  true
      POD_NAME:                                            vault-agent-injector-76f7c8b8f5-j8kjs (v1:metadata.name)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-985dz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  kube-api-access-985dz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             external-secrets-cert-controller-d9997698-shd7p
Namespace:        orch-secret
Priority:         0
Service Account:  external-secrets-cert-controller
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:03 +0000
Labels:           app.kubernetes.io/instance=external-secrets
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=external-secrets-cert-controller
                  app.kubernetes.io/version=v0.20.4
                  helm.sh/chart=external-secrets-0.20.4
                  pod-template-hash=d9997698
Annotations:      cni.projectcalico.org/containerID: 601dc8ef2245c3755751e0f7155a2d16a516d1a465bd31ae1cd714ef1d794a68
                  cni.projectcalico.org/podIP: 10.42.65.89/32
                  cni.projectcalico.org/podIPs: 10.42.65.89/32
Status:           Running
IP:               10.42.65.89
IPs:
  IP:           10.42.65.89
Controlled By:  ReplicaSet/external-secrets-cert-controller-d9997698
Containers:
  cert-controller:
    Container ID:    containerd://bec697c357848d244d76e68523314642255f4d3bc4384be51316bdfe516b45e6
    Image:           oci.external-secrets.io/external-secrets/external-secrets:v0.20.4
    Image ID:        oci.external-secrets.io/external-secrets/external-secrets@sha256:0ec2b4d29f3bfbceef88b84f58e56ba934a5dff9f59a72445f92aff863c688a5
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      certcontroller
      --crd-requeue-interval=5m
      --service-name=external-secrets-webhook
      --service-namespace=orch-secret
      --secret-name=external-secrets-webhook
      --secret-namespace=orch-secret
      --metrics-addr=:8080
      --healthz-addr=:8081
      --loglevel=info
      --zap-time-encoding=epoch
      --enable-partial-cache=true
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:8081/readyz delay=20s timeout=1s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-59txc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-59txc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             external-secrets-fbf5bd76d-g755n
Namespace:        orch-secret
Priority:         0
Service Account:  external-secrets
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:03 +0000
Labels:           app.kubernetes.io/instance=external-secrets
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=external-secrets
                  app.kubernetes.io/version=v0.20.4
                  helm.sh/chart=external-secrets-0.20.4
                  pod-template-hash=fbf5bd76d
Annotations:      cni.projectcalico.org/containerID: f6ca639adb27ff16bd9f2241e0d3dcb672b1ca40d00ee5f1c38c764718efcd6f
                  cni.projectcalico.org/podIP: 10.42.65.88/32
                  cni.projectcalico.org/podIPs: 10.42.65.88/32
Status:           Running
IP:               10.42.65.88
IPs:
  IP:           10.42.65.88
Controlled By:  ReplicaSet/external-secrets-fbf5bd76d
Containers:
  external-secrets:
    Container ID:    containerd://d2c47dfcf847db7f0b984a5b1b6c8da5c837c1d256eb02f9c2bc7f5931f7c161
    Image:           oci.external-secrets.io/external-secrets/external-secrets:v0.20.4
    Image ID:        oci.external-secrets.io/external-secrets/external-secrets@sha256:0ec2b4d29f3bfbceef88b84f58e56ba934a5dff9f59a72445f92aff863c688a5
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      --concurrent=1
      --metrics-addr=:8080
      --loglevel=info
      --zap-time-encoding=epoch
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      http_proxy:   
      https_proxy:  
      no_proxy:     
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5ffbm (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-5ffbm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             external-secrets-webhook-7bf6784f94-cjdvl
Namespace:        orch-secret
Priority:         0
Service Account:  external-secrets-webhook
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:31:03 +0000
Labels:           app.kubernetes.io/instance=external-secrets
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=external-secrets-webhook
                  app.kubernetes.io/version=v0.20.4
                  helm.sh/chart=external-secrets-0.20.4
                  pod-template-hash=7bf6784f94
Annotations:      cni.projectcalico.org/containerID: 5970253565d33b00d3108b69df5011952deb5f1be60901edb0782421ea79b008
                  cni.projectcalico.org/podIP: 10.42.65.90/32
                  cni.projectcalico.org/podIPs: 10.42.65.90/32
Status:           Running
IP:               10.42.65.90
IPs:
  IP:           10.42.65.90
Controlled By:  ReplicaSet/external-secrets-webhook-7bf6784f94
Containers:
  webhook:
    Container ID:    containerd://2153003655b2e15775a13417be59e50baf978e176eb92c85c3affbc608b37ea2
    Image:           oci.external-secrets.io/external-secrets/external-secrets:v0.20.4
    Image ID:        oci.external-secrets.io/external-secrets/external-secrets@sha256:0ec2b4d29f3bfbceef88b84f58e56ba934a5dff9f59a72445f92aff863c688a5
    Ports:           8080/TCP, 10250/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      webhook
      --port=10250
      --dns-name=external-secrets-webhook.orch-secret.svc
      --cert-dir=/tmp/certs
      --check-interval=5m
      --metrics-addr=:8080
      --healthz-addr=:8081
      --loglevel=info
      --zap-time-encoding=epoch
    State:          Running
      Started:      Tue, 03 Feb 2026 17:31:08 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Readiness:    http-get http://:8081/readyz delay=20s timeout=1s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp/certs from certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmwzc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  external-secrets-webhook
    Optional:    false
  kube-api-access-hmwzc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             token-fs-64959b949-lqvlh
Namespace:        orch-secret
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:46:36 +0000
Labels:           app.kubernetes.io/instance=token-fs
                  app.kubernetes.io/name=token-fs
                  pod-template-hash=64959b949
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=token-fs
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: 46b0b8c5a61b32693a688ff9009b326450ee8f9dd189caa43d5a794044d28532
                  cni.projectcalico.org/podIP: 10.42.65.171/32
                  cni.projectcalico.org/podIPs: 10.42.65.171/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: token-fs
                  kubectl.kubernetes.io/default-logs-container: token-fs
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.171
IPs:
  IP:           10.42.65.171
Controlled By:  ReplicaSet/token-fs-64959b949
Init Containers:
  istio-init:
    Container ID:  containerd://08ad0db98237f3d14730c8d060d0c4207e4fca7c2dc97555df52375f9adb80d7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:46:38 +0000
      Finished:     Tue, 03 Feb 2026 17:46:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wj7st (ro)
  istio-proxy:
    Container ID:  containerd://d564953891df48fdc66c74c5919e5245777e1406779337cc58bf1c67385d1bdc
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:46:39 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      token-fs-64959b949-lqvlh (v1:metadata.name)
      POD_NAMESPACE:                 orch-secret (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":8080,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     token-fs
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      token-fs
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-secret/deployments/token-fs
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/token-fs/livez":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/token-fs/readyz":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wj7st (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  token-fs:
    Container ID:    containerd://019f417fbb655c6279db880841fb26a127553e7f252c3f4a489631a382ec750d
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/common/token-fs:26.0.1
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/common/token-fs@sha256:2cb5111d08c1009cd75b6aec2623ddea4650e20c67eb76635e8d1baa82deb059
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -jwksURL=http://platform-keycloak.orch-platform.svc:8080/realms/master/protocol/openid-connect/certs
      -rolesFile=/config/roles.txt
      -fileServerPath=/data/
      -emptyRSToken
    State:          Running
      Started:      Tue, 03 Feb 2026 17:46:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/token-fs/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/token-fs/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /config from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wj7st (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      token-fs
    Optional:  false
  kube-api-access-wj7st:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  59m   default-scheduler  Successfully assigned orch-secret/token-fs-64959b949-lqvlh to orch-tf
  Normal   Pulled     59m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    59m   kubelet            Created container: istio-init
  Normal   Started    59m   kubelet            Started container istio-init
  Normal   Pulled     59m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    59m   kubelet            Created container: istio-proxy
  Normal   Started    59m   kubelet            Started container istio-proxy
  Warning  Unhealthy  59m   kubelet            Startup probe failed: Get "http://10.42.65.171:15021/healthz/ready": dial tcp 10.42.65.171:15021: connect: connection refused
  Normal   Pulling    59m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/common/token-fs:26.0.1"
  Normal   Pulled     59m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/common/token-fs:26.0.1" in 2.165s (2.165s including waiting). Image size: 4011422 bytes.
  Normal   Created    59m   kubelet            Created container: token-fs
  Normal   Started    59m   kubelet            Started container token-fs


Name:             sre-exporter-5c5449557-vjbj7
Namespace:        orch-sre
Priority:         0
Service Account:  sre-exporter
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:54 +0000
Labels:           app.kubernetes.io/instance=sre-exporter
                  app.kubernetes.io/name=sre-exporter
                  pod-template-hash=5c5449557
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=sre-exporter
                  service.istio.io/canonical-revision=latest
Annotations:      cni.projectcalico.org/containerID: ab33eba4f61acec1db8e6323656f7c75a6cdb930bd68b556edbad9c26bad3f20
                  cni.projectcalico.org/podIP: 10.42.65.200/32
                  cni.projectcalico.org/podIPs: 10.42.65.200/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: metrics-exporter
                  kubectl.kubernetes.io/default-logs-container: metrics-exporter
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/proxyCPU: 15m
                  sidecar.istio.io/proxyCPULimit: 1000m
                  sidecar.istio.io/proxyMemory: 64Mi
                  sidecar.istio.io/proxyMemoryLimit: 1Gi
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.200
IPs:
  IP:           10.42.65.200
Controlled By:  ReplicaSet/sre-exporter-5c5449557
Init Containers:
  istio-init:
    Container ID:  containerd://662ca1a0e5ae80434087758a8f470a37746f2f9c78870e21330282e21db146fd
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:56 +0000
      Finished:     Tue, 03 Feb 2026 17:47:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:     15m
      memory:  64Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zg6mf (ro)
  istio-proxy:
    Container ID:  containerd://e086ad1fcdb926b9e371c8f4dca5e4c085ab9cfef3f70fd737402cd8dba2794d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:      15m
      memory:   64Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      sre-exporter-5c5449557-vjbj7 (v1:metadata.name)
      POD_NAMESPACE:                 orch-sre (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               1 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"containerPort":50051,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     metrics-exporter,otel-collector,config-reloader
      GOMEMLIMIT:                    1073741824 (limits.memory)
      GOMAXPROCS:                    1 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      sre-exporter
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-sre/deployments/sre-exporter
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/config-reloader/livez":{"grpc":{"port":50051,"service":""},"timeoutSeconds":1},"/app-health/config-reloader/readyz":{"grpc":{"port":50051,"service":""},"timeoutSeconds":1},"/app-health/metrics-exporter/livez":{"httpGet":{"path":"/","port":9141,"scheme":"HTTP","httpHeaders":[{"name":"Authorization","value":"Basic c3JlOjEyMw=="}]},"timeoutSeconds":1},"/app-health/metrics-exporter/readyz":{"httpGet":{"path":"/","port":9141,"scheme":"HTTP","httpHeaders":[{"name":"Authorization","value":"Basic c3JlOjEyMw=="}]},"timeoutSeconds":1},"/app-health/otel-collector/livez":{"httpGet":{"path":"/","port":13133,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/otel-collector/readyz":{"httpGet":{"path":"/metrics","port":8888,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zg6mf (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  metrics-exporter:
    Container ID:  containerd://a9f01504a9571aff820417cf5f928ce8c8bf32fa5bb276b89f2460f1f37e51ae
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-metrics-exporter:0.9.2
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-metrics-exporter@sha256:327ffa0ec6e77ce6805c1d68779958d7073dc82e545418c5aefca30ae25806b5
    Port:          <none>
    Host Port:     <none>
    Args:
      -config=/etc/config/sre-exporter-orch.json
      -config=/etc/config/sre-exporter-edge-node.json
      -listenAddress=:9141
      -customerLabel=local
      -vaultNamespace=orch-platform
      -vaultURI=vault-0
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:04 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/metrics-exporter/livez delay=5s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/metrics-exporter/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/config from sre-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zg6mf (ro)
  otel-collector:
    Container ID:  containerd://f2fdc389dd7aa1c1fd04dfc7e4616b9e5250ee3bda3fe6a16340cd5e0e513f70
    Image:         otel/opentelemetry-collector-contrib:0.111.0
    Image ID:      docker.io/otel/opentelemetry-collector-contrib@sha256:a2a52e43c1a80aa94120ad78c2db68780eb90e6d11c8db5b3ce2f6a0cc6b5029
    Port:          <none>
    Host Port:     <none>
    Args:
      --config
      /etc/otel/otel-secret.yaml
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:21 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/otel-collector/livez delay=3s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/otel-collector/readyz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DESTINATION_URL:            <set to the key 'url' in secret 'destination-secret-url'>    Optional: false
      DESTINATION_AUTH_USERNAME:  <set to the key 'username' in secret 'basic-auth-username'>  Optional: false
      DESTINATION_AUTH_PASSWORD:  <set to the key 'password' in secret 'basic-auth-password'>  Optional: false
      GOMEMLIMIT:                 2000MiB
    Mounts:
      /etc/otel from otel-secret (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zg6mf (ro)
  config-reloader:
    Container ID:  containerd://e252bae7273ea0f12501c4042c17390b285c9d6bb6a479ae39eb8e2457b2c50b
    Image:         registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-config-reloader:0.9.2
    Image ID:      registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-config-reloader@sha256:660a05ba8df077ca956e5ab3c2628d605a5871c2012037a082a26e42af35ab95
    Port:          50051/TCP
    Host Port:     0/TCP
    Args:
      -namespace=orch-sre
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:31 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/config-reloader/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/config-reloader/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:  sre-exporter-5c5449557-vjbj7 (v1:metadata.name)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zg6mf (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  sre-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      sre-exporter-config
    Optional:  false
  otel-secret:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sre-otel-secret
    Optional:    false
  kube-api-access-zg6mf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  58m   default-scheduler  Successfully assigned orch-sre/sre-exporter-5c5449557-vjbj7 to orch-tf
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-init
  Normal   Started    58m   kubelet            Started container istio-init
  Normal   Pulled     58m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m   kubelet            Created container: istio-proxy
  Normal   Started    58m   kubelet            Started container istio-proxy
  Warning  Unhealthy  58m   kubelet            Startup probe failed: Get "http://10.42.65.200:15021/healthz/ready": dial tcp 10.42.65.200:15021: connect: connection refused
  Normal   Pulling    58m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-metrics-exporter:0.9.2"
  Normal   Pulled     58m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-metrics-exporter:0.9.2" in 3.35s (3.35s including waiting). Image size: 17089671 bytes.
  Normal   Created    58m   kubelet            Created container: metrics-exporter
  Normal   Started    58m   kubelet            Started container metrics-exporter
  Normal   Pulling    58m   kubelet            Pulling image "otel/opentelemetry-collector-contrib:0.111.0"
  Normal   Pulled     57m   kubelet            Successfully pulled image "otel/opentelemetry-collector-contrib:0.111.0" in 14.869s (14.869s including waiting). Image size: 73587938 bytes.
  Normal   Created    57m   kubelet            Created container: otel-collector
  Normal   Started    57m   kubelet            Started container otel-collector
  Normal   Pulling    57m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-config-reloader:0.9.2"
  Normal   Pulled     57m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/o11y/sre-config-reloader:0.9.2" in 8.876s (8.876s including waiting). Image size: 18867728 bytes.
  Normal   Created    57m   kubelet            Created container: config-reloader
  Normal   Started    57m   kubelet            Started container config-reloader


Name:             metadata-broker-orch-metadata-broker-554f685544-d656f
Namespace:        orch-ui
Priority:         0
Service Account:  mdb-service-account
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:47:54 +0000
Labels:           app=metadata-broker
                  app.kubernetes.io/instance=metadata-broker
                  app.kubernetes.io/name=orch-metadata-broker
                  app.kubernetes.io/part-of=ledge-park-metadata
                  pod-template-hash=554f685544
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=orch-metadata-broker
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/nginx-config: 9b8a97753fc385a54078be632b6ccbf990719c26b49f6b5ec0024f871ba15d12
                  cni.projectcalico.org/containerID: 42aa3ebe4071f3d51ef441dd77a7fc1bec2c30b69e6f119e86d333331ab43e4b
                  cni.projectcalico.org/podIP: 10.42.65.201/32
                  cni.projectcalico.org/podIPs: 10.42.65.201/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: orch-metadata-broker
                  kubectl.kubernetes.io/default-logs-container: orch-metadata-broker
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.201
IPs:
  IP:           10.42.65.201
Controlled By:  ReplicaSet/metadata-broker-orch-metadata-broker-554f685544
Init Containers:
  istio-init:
    Container ID:  containerd://4c59f2af81962e745d3da41002c2cb4240210c042a34257b1a220d0f44b0d5df
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:47:56 +0000
      Finished:     Tue, 03 Feb 2026 17:47:56 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-shnsn (ro)
  istio-proxy:
    Container ID:  containerd://b070892bf9ec1dcefec0705b764b21b161668711cf871c7f26d8f9c7da015915
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:47:58 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      metadata-broker-orch-metadata-broker-554f685544-d656f (v1:metadata.name)
      POD_NAMESPACE:                 orch-ui (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"rest","containerPort":9988,"protocol":"TCP"}
                                         ,{"name":"grpc","containerPort":9987,"protocol":"TCP"}
                                         ,{"name":"opa","containerPort":9986,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     orch-metadata-broker,openpolicyagent
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      metadata-broker-orch-metadata-broker
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-ui/deployments/metadata-broker-orch-metadata-broker
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
      ISTIO_KUBE_APP_PROBERS:        {"/app-health/openpolicyagent/livez":{"httpGet":{"path":"/health?bundle=true","port":9986,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/openpolicyagent/readyz":{"httpGet":{"path":"/health?bundle=true","port":9986,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/orch-metadata-broker/livez":{"httpGet":{"path":"/healthz","port":9988,"scheme":"HTTP"},"timeoutSeconds":1},"/app-health/orch-metadata-broker/readyz":{"httpGet":{"path":"/healthz","port":9988,"scheme":"HTTP"},"timeoutSeconds":1}}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-shnsn (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  orch-metadata-broker:
    Container ID:    containerd://8d595b5020ace4ca4f22760cbb0035de3766a9fe6c275774615f329dfeb31c7e
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/metadata-broker:1.0.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/metadata-broker@sha256:7b1314798d0971dbcf0bd88616762ea83b6f473c0a3ff2ba3301890ec5b60701
    Ports:           9988/TCP, 9987/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      -restPort=9988
      -grpcPort=9987
      -opaPort=9986
      -backupFile=/data/metadata.json
      -backupFolder=/data
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:05 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Liveness:   http-get http://:15020/app-health/orch-metadata-broker/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:15020/app-health/orch-metadata-broker/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      OIDC_SERVER_URL:                http://platform-keycloak.orch-platform.svc/realms/master
      OIDC_SERVER_URL_EXTERNAL:       
      OIDC_TLS_INSECURE_SKIP_VERIFY:  false
    Mounts:
      /data from metadata-data (rw)
      /etc/dazl from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-shnsn (ro)
  openpolicyagent:
    Container ID:    containerd://4e1c774bde5d1f0471057fe479582d784bea4ac8796ae70b62af652509c96a77
    Image:           openpolicyagent/opa:1.10.1-static
    Image ID:        docker.io/openpolicyagent/opa@sha256:e1f196b5316301785d5251543c9d0f3f6c83ce6ac23263b494730427fa80e248
    Port:            9986/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Args:
      run
      --server
      /etc/opa/rego
      --addr
      :9986
      --log-level
      info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:48:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Liveness:     http-get http://:15020/app-health/openpolicyagent/livez delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:15020/app-health/openpolicyagent/readyz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/opa/rego from openpolicyagent (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-shnsn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      metadata-broker-orch-metadata-broker-logging
    Optional:  false
  metadata-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  openpolicyagent:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      metadata-broker-orch-metadata-broker-opa-rego
    Optional:  false
  kube-api-access-shnsn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  58m                default-scheduler  Successfully assigned orch-ui/metadata-broker-orch-metadata-broker-554f685544-d656f to orch-tf
  Normal   Pulled     58m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m                kubelet            Created container: istio-init
  Normal   Started    58m                kubelet            Started container istio-init
  Normal   Pulled     58m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    58m                kubelet            Created container: istio-proxy
  Normal   Started    58m                kubelet            Started container istio-proxy
  Warning  Unhealthy  58m (x2 over 58m)  kubelet            Startup probe failed: Get "http://10.42.65.201:15021/healthz/ready": dial tcp 10.42.65.201:15021: connect: connection refused
  Normal   Pulling    58m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/metadata-broker:1.0.0"
  Normal   Pulled     58m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/metadata-broker:1.0.0" in 3.425s (3.425s including waiting). Image size: 18086678 bytes.
  Normal   Created    58m                kubelet            Created container: orch-metadata-broker
  Normal   Started    58m                kubelet            Started container orch-metadata-broker
  Normal   Pulled     58m                kubelet            Container image "openpolicyagent/opa:1.10.1-static" already present on machine
  Normal   Created    58m                kubelet            Created container: openpolicyagent
  Normal   Started    58m                kubelet            Started container openpolicyagent


Name:             web-ui-admin-77d89994c-9t4mj
Namespace:        orch-ui
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:57:25 +0000
Labels:           app=admin
                  app.kubernetes.io/part-of=orch-ui
                  pod-template-hash=77d89994c
                  release=web-ui
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=admin
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/nginx-config: 96c6f1abbabf775692da4451ecc7730dc2420d13bba85a556b86f78b64b3befb
                  checksum/runtime-config: 95583b3e08fb652f35acdb2f262c727947142ef25d0c5c810626d9f2551680f4
                  cni.projectcalico.org/containerID: 7db3152e66763d2c370fd4bd2b5655ee23cf1848cc49a5d1ead943b1814f901e
                  cni.projectcalico.org/podIP: 10.42.65.249/32
                  cni.projectcalico.org/podIPs: 10.42.65.249/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: admin
                  kubectl.kubernetes.io/default-logs-container: admin
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.249
IPs:
  IP:           10.42.65.249
Controlled By:  ReplicaSet/web-ui-admin-77d89994c
Init Containers:
  istio-init:
    Container ID:  containerd://5bb15fdfa00d5844adaad23e1f550da7888d3919305ca39e8d36f023d535b0f8
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:57:29 +0000
      Finished:     Tue, 03 Feb 2026 17:57:30 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-62xsz (ro)
  istio-proxy:
    Container ID:  containerd://1756fdc1460b40b223b099c0bb1895e5e616a17068045e924cb79a6b9bd63bb7
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:57:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      web-ui-admin-77d89994c-9t4mj (v1:metadata.name)
      POD_NAMESPACE:                 orch-ui (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":3000,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     admin
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      web-ui-admin
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-ui/deployments/web-ui-admin
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-62xsz (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  admin:
    Container ID:    containerd://93a6a2a001000bf2262b32a46f4d833321992947f30522d6a016051fbaed37c9
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/admin:3.2.4
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/admin@sha256:7c384e384fde2aa073de679c79f2df6a3d38f674b201fa347857ef87783acf8c
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:57:51 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/nginx/conf.d/default.conf from nginx-config (rw,path="default.conf")
      /tmp from tmp (rw)
      /usr/share/nginx/html/runtime-config.js from runtime-config (rw,path="runtime-config.js")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-62xsz (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  nginx-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-admin-nginx-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-admin-runtime-config
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-62xsz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  48m                default-scheduler  Successfully assigned orch-ui/web-ui-admin-77d89994c-9t4mj to orch-tf
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-init
  Normal   Started    48m                kubelet            Started container istio-init
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-proxy
  Normal   Started    48m                kubelet            Started container istio-proxy
  Warning  Unhealthy  48m (x6 over 48m)  kubelet            Startup probe failed: Get "http://10.42.65.249:15021/healthz/ready": dial tcp 10.42.65.249:15021: connect: connection refused
  Normal   Pulling    48m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/admin:3.2.4"
  Normal   Pulled     48m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/admin:3.2.4" in 5.762s (5.762s including waiting). Image size: 22086240 bytes.
  Normal   Created    48m                kubelet            Created container: admin
  Normal   Started    48m                kubelet            Started container admin


Name:             web-ui-app-orch-758ffdd4ff-njlbm
Namespace:        orch-ui
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:57:33 +0000
Labels:           app=app-orch
                  app.kubernetes.io/part-of=orch-ui
                  pod-template-hash=758ffdd4ff
                  release=web-ui
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=app-orch
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/nginx-config: 49b4cf1bd25d72d1fa279468ac3e259046ec992f85f580c663a52842f5013a53
                  checksum/runtime-config: d6561dc7f763fb51bac8c4bcaba5bdd3cc9747a71fdcdd9e7f065b9007deb878
                  cni.projectcalico.org/containerID: f4bd8a00152aa586783d11848fc9c0b7c5af0947ee50e552131ad7b51fb23332
                  cni.projectcalico.org/podIP: 10.42.65.253/32
                  cni.projectcalico.org/podIPs: 10.42.65.253/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: app-orch
                  kubectl.kubernetes.io/default-logs-container: app-orch
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.253
IPs:
  IP:           10.42.65.253
Controlled By:  ReplicaSet/web-ui-app-orch-758ffdd4ff
Init Containers:
  istio-init:
    Container ID:  containerd://604912346b02868ff328aa54bf3ad12cfbc2a09268b8f657c49d4d3bb705c527
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:57:39 +0000
      Finished:     Tue, 03 Feb 2026 17:57:40 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t7xqc (ro)
  istio-proxy:
    Container ID:  containerd://ba43ffd3533049e615978323e35c754313734b22bbfa35ca2d144be9080d36ad
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:57:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      web-ui-app-orch-758ffdd4ff-njlbm (v1:metadata.name)
      POD_NAMESPACE:                 orch-ui (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":3000,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     app-orch
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      web-ui-app-orch
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-ui/deployments/web-ui-app-orch
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t7xqc (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  app-orch:
    Container ID:    containerd://e5c988851bea5d41d7e508d1cc12b909be3aa1796915fb44ba4abd25ba062209
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/app-orch:4.0.2
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/app-orch@sha256:eed2b18f6faa0da19e401a3b3f7360dd78fbdfe02bed56554bdb651781134bb0
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:57:54 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/nginx/conf.d/default.conf from nginx-config (rw,path="default.conf")
      /tmp from tmp (rw)
      /usr/share/nginx/html/runtime-config.js from runtime-config (rw,path="runtime-config.js")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t7xqc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  nginx-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-app-orch-nginx-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-app-orch-runtime-config
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-t7xqc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  48m                default-scheduler  Successfully assigned orch-ui/web-ui-app-orch-758ffdd4ff-njlbm to orch-tf
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-init
  Normal   Started    48m                kubelet            Started container istio-init
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-proxy
  Normal   Started    48m                kubelet            Started container istio-proxy
  Warning  Unhealthy  48m (x4 over 48m)  kubelet            Startup probe failed: Get "http://10.42.65.253:15021/healthz/ready": dial tcp 10.42.65.253:15021: connect: connection refused
  Normal   Pulling    48m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/app-orch:4.0.2"
  Normal   Pulled     48m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/app-orch:4.0.2" in 3.974s (3.974s including waiting). Image size: 22507202 bytes.
  Normal   Created    48m                kubelet            Created container: app-orch
  Normal   Started    48m                kubelet            Started container app-orch


Name:             web-ui-cluster-orch-65684cc4b8-w6fdn
Namespace:        orch-ui
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:57:30 +0000
Labels:           app=cluster-orch
                  app.kubernetes.io/part-of=orch-ui
                  pod-template-hash=65684cc4b8
                  release=web-ui
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=cluster-orch
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/nginx-config: fb72e781b975a4b2e4bc40ae7eea73efcb044bbe911dcbbe867d821e10fe971a
                  checksum/runtime-config: 6289f0904475aa747b3bc9db45ac6506f521d1bf0e59f7c4a08696294af833b4
                  cni.projectcalico.org/containerID: b1dec0875cb1b1b670b29678126c8f4dfe7b36b99bf8cd2fb6e959e7c0e4c59f
                  cni.projectcalico.org/podIP: 10.42.65.252/32
                  cni.projectcalico.org/podIPs: 10.42.65.252/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: cluster-orch
                  kubectl.kubernetes.io/default-logs-container: cluster-orch
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.252
IPs:
  IP:           10.42.65.252
Controlled By:  ReplicaSet/web-ui-cluster-orch-65684cc4b8
Init Containers:
  istio-init:
    Container ID:  containerd://be2b336fdc373c5c715ff3bd2c7631fa281fcb4478d5796dffaf90e0e12b3db5
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:57:37 +0000
      Finished:     Tue, 03 Feb 2026 17:57:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4vgt (ro)
  istio-proxy:
    Container ID:  containerd://1e0ee4642316bfc67b15c1648438cc5523916844181b9b35d98fb54f4817bbe9
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:57:39 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      web-ui-cluster-orch-65684cc4b8-w6fdn (v1:metadata.name)
      POD_NAMESPACE:                 orch-ui (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":3000,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     cluster-orch
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      web-ui-cluster-orch
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-ui/deployments/web-ui-cluster-orch
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4vgt (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  cluster-orch:
    Container ID:    containerd://05493e1775309968fe06cb5db4a87ff7c35029ba498088a3a1a9aecfae443d46
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/cluster-orch:4.0.0
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/cluster-orch@sha256:cca26a9a8d26571bfc1362916a67d0f452f1cb559d5387bd96432d7095a08eeb
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:57:53 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/nginx/conf.d/default.conf from nginx-config (rw,path="default.conf")
      /tmp from tmp (rw)
      /usr/share/nginx/html/runtime-config.js from runtime-config (rw,path="runtime-config.js")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4vgt (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  nginx-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-cluster-orch-nginx-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-cluster-orch-runtime-config
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-t4vgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  48m                default-scheduler  Successfully assigned orch-ui/web-ui-cluster-orch-65684cc4b8-w6fdn to orch-tf
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-init
  Normal   Started    48m                kubelet            Started container istio-init
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-proxy
  Normal   Started    48m                kubelet            Started container istio-proxy
  Warning  Unhealthy  48m (x5 over 48m)  kubelet            Startup probe failed: Get "http://10.42.65.252:15021/healthz/ready": dial tcp 10.42.65.252:15021: connect: connection refused
  Normal   Pulling    48m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/cluster-orch:4.0.0"
  Normal   Pulled     48m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/cluster-orch:4.0.0" in 4.933s (4.933s including waiting). Image size: 22592322 bytes.
  Normal   Created    48m                kubelet            Created container: cluster-orch
  Normal   Started    48m                kubelet            Started container cluster-orch


Name:             web-ui-infra-6969c745c9-9snbf
Namespace:        orch-ui
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:57:30 +0000
Labels:           app=infra
                  app.kubernetes.io/part-of=orch-ui
                  pod-template-hash=6969c745c9
                  release=web-ui
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=infra
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/nginx-config: 7519e2ba95e036ec1bd9e11d3ccbd02f192dc72f519ecdf0a8c35fc3c9dce266
                  checksum/runtime-config: 88f92540a5e80e023edc9291de00aa8b42984e9eccc6a33fc8dbfab0b9fddb7e
                  cni.projectcalico.org/containerID: 0dfde149015bf6fe076200a86138c8dd4b8eb11f6fa6cdd8525c4c9a7dcce411
                  cni.projectcalico.org/podIP: 10.42.65.251/32
                  cni.projectcalico.org/podIPs: 10.42.65.251/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: infra
                  kubectl.kubernetes.io/default-logs-container: infra
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.251
IPs:
  IP:           10.42.65.251
Controlled By:  ReplicaSet/web-ui-infra-6969c745c9
Init Containers:
  istio-init:
    Container ID:  containerd://e4362ffa7b11a8072acac46531bc2f1fc2ea60159aa39f705f8bab03967de00b
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:57:37 +0000
      Finished:     Tue, 03 Feb 2026 17:57:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fz5jt (ro)
  istio-proxy:
    Container ID:  containerd://a832a74a53fea65af31645df959ab922702257f490862efd3466d77b4f37d914
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:57:40 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      web-ui-infra-6969c745c9-9snbf (v1:metadata.name)
      POD_NAMESPACE:                 orch-ui (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":3000,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     infra
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      web-ui-infra
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-ui/deployments/web-ui-infra
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fz5jt (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  infra:
    Container ID:    containerd://829f0f27b6d557157638ff740b84cf63780eb237987c0a65a3b9cf1364da5249
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/infra:3.2.17
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/infra@sha256:06997c6b42e2288473b831dbc0e0ab34f906ce4ce7eb1176db74eaf9e368b157
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:57:54 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/nginx/conf.d/default.conf from nginx-config (rw,path="default.conf")
      /tmp from tmp (rw)
      /usr/share/nginx/html/runtime-config.js from runtime-config (rw,path="runtime-config.js")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fz5jt (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  nginx-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-infra-nginx-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-infra-runtime-config
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-fz5jt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  48m                default-scheduler  Successfully assigned orch-ui/web-ui-infra-6969c745c9-9snbf to orch-tf
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-init
  Normal   Started    48m                kubelet            Started container istio-init
  Normal   Pulled     48m                kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m                kubelet            Created container: istio-proxy
  Normal   Started    48m                kubelet            Started container istio-proxy
  Warning  Unhealthy  48m (x6 over 48m)  kubelet            Startup probe failed: Get "http://10.42.65.251:15021/healthz/ready": dial tcp 10.42.65.251:15021: connect: connection refused
  Normal   Pulling    48m                kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/infra:3.2.17"
  Normal   Pulled     48m                kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/infra:3.2.17" in 5.2s (5.2s including waiting). Image size: 22662910 bytes.
  Normal   Created    48m                kubelet            Created container: infra
  Normal   Started    48m                kubelet            Started container infra


Name:             web-ui-root-5c4557b8f7-krj8n
Namespace:        orch-ui
Priority:         0
Service Account:  default
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:57:38 +0000
Labels:           app=root
                  app.kubernetes.io/part-of=orch-ui
                  pod-template-hash=5c4557b8f7
                  release=web-ui
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=root
                  service.istio.io/canonical-revision=latest
Annotations:      checksum/nginx-config: 67a2ce8c0715ff585d135b7cf82cbdd85811731cdd3070eb2f39f506f7940596
                  checksum/runtime-config: 71000b712bad19a9c5b351f860f0e7e264d8996efdab78b75d55301b3f09370f
                  cni.projectcalico.org/containerID: 9e2e2971f1cbd82c7cd4acbc935ff8ac05c7225ae0b7eac52855a439d3b5c579
                  cni.projectcalico.org/podIP: 10.42.65.254/32
                  cni.projectcalico.org/podIPs: 10.42.65.254/32
                  istio.io/rev: default
                  kubectl.kubernetes.io/default-container: root
                  kubectl.kubernetes.io/default-logs-container: root
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init","istio-proxy"],"containers":null,"volumes":["workload-socket","credential-socket","workload-certs","istio-...
Status:           Running
IP:               10.42.65.254
IPs:
  IP:           10.42.65.254
Controlled By:  ReplicaSet/web-ui-root-5c4557b8f7
Init Containers:
  istio-init:
    Container ID:  containerd://97974eeb6066a84cc63fc26fbd234579b03f7ab6cd211a71415add76b2c3c034
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Feb 2026 17:57:44 +0000
      Finished:     Tue, 03 Feb 2026 17:57:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:     1m
      memory:  1Mi
    Environment:
      WORKLOAD_RSA_KEY_SIZE:  3072
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lmwfp (ro)
  istio-proxy:
    Container ID:  containerd://14bea036fe51343c29d87143a578e198c75a4b5d90a97f93447ed731d1f9c44d
    Image:         docker.io/istio/proxyv2:1.28.0
    Image ID:      docker.io/istio/proxyv2@sha256:f95a5e91204afbfad9d33d6f1bc0556a96bf997b18edede6c9e6b86eb07543a1
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
    State:          Running
      Started:      Tue, 03 Feb 2026 17:57:46 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:      1m
      memory:   1Mi
    Readiness:  http-get http://:15021/healthz/ready delay=0s timeout=3s period=15s #success=1 #failure=4
    Startup:    http-get http://:15021/healthz/ready delay=0s timeout=3s period=1s #success=1 #failure=600
    Environment:
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      web-ui-root-5c4557b8f7-krj8n (v1:metadata.name)
      POD_NAMESPACE:                 orch-ui (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      ISTIO_CPU_LIMIT:               64 (limits.cpu)
      PROXY_CONFIG:                  {"proxyMetadata":{"WORKLOAD_RSA_KEY_SIZE":"3072"},"holdApplicationUntilProxyStarts":true}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"http","containerPort":3000,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     root
      GOMEMLIMIT:                    68719476736 (limits.memory)
      GOMAXPROCS:                    64 (limits.cpu)
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_NODE_NAME:           (v1:spec.nodeName)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      web-ui-root
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/orch-ui/deployments/web-ui-root
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
      WORKLOAD_RSA_KEY_SIZE:         3072
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/istio/crl from istio-ca-crl (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lmwfp (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Containers:
  root:
    Container ID:    containerd://6d468180af7f4a57be738dcd988e6125df564a362351a75a5d340c08b72fd7aa
    Image:           registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/root:4.0.1
    Image ID:        registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/root@sha256:ca4076a0edded07ee443769e57cc285489b80d576588b2954f5fd10b54ffc6b7
    Port:            3000/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Tue, 03 Feb 2026 17:57:53 +0000
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     64
      memory:  64Gi
    Requests:
      cpu:        1m
      memory:     1Mi
    Environment:  <none>
    Mounts:
      /etc/nginx/conf.d/default.conf from nginx-config (rw,path="default.conf")
      /tmp from tmp (rw)
      /usr/share/nginx/html/runtime-config.js from runtime-config (rw,path="runtime-config.js")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lmwfp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  istio-ca-crl:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-crl
    Optional:  true
  nginx-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-root-nginx-config
    Optional:  false
  runtime-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      web-ui-root-runtime-config
    Optional:  false
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-lmwfp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  48m   default-scheduler  Successfully assigned orch-ui/web-ui-root-5c4557b8f7-krj8n to orch-tf
  Normal   Pulled     48m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m   kubelet            Created container: istio-init
  Normal   Started    48m   kubelet            Started container istio-init
  Normal   Pulled     48m   kubelet            Container image "docker.io/istio/proxyv2:1.28.0" already present on machine
  Normal   Created    48m   kubelet            Created container: istio-proxy
  Normal   Started    48m   kubelet            Started container istio-proxy
  Warning  Unhealthy  48m   kubelet            Startup probe failed: Get "http://10.42.65.254:15021/healthz/ready": dial tcp 10.42.65.254:15021: connect: connection refused
  Normal   Pulling    48m   kubelet            Pulling image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/root:4.0.1"
  Normal   Pulled     48m   kubelet            Successfully pulled image "registry-rs.edgeorchestration.intel.com/edge-orch/orch-ui/root:4.0.1" in 3.481s (3.481s including waiting). Image size: 22409304 bytes.
  Normal   Created    48m   kubelet            Created container: root
  Normal   Started    48m   kubelet            Started container root


Name:             postgresql-operator-cloudnative-pg-747d847484-cswpd
Namespace:        postgresql-operator
Priority:         0
Service Account:  postgresql-operator-cloudnative-pg
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:30:28 +0000
Labels:           app.kubernetes.io/instance=postgresql-operator
                  app.kubernetes.io/name=cloudnative-pg
                  pod-template-hash=747d847484
Annotations:      checksum/config: b8efcff391876b8751bf14d4023f1c693dfd55c881808210ecbc87131a402d9c
                  checksum/monitoring-config: fced28b195e7cfcfe7905fa882c95993d97d43ff88e4fdd4852cf14d61a2d9b8
                  checksum/rbac: 25c9ff08ecd2a102f514fd7150d320eb545f1e82fb960c914277b77e9b58d235
                  cni.projectcalico.org/containerID: 548bdd1154114718447426245d6d99aa9a5f831e09df8450a5d9b3eac64d313f
                  cni.projectcalico.org/podIP: 10.42.65.86/32
                  cni.projectcalico.org/podIPs: 10.42.65.86/32
Status:           Running
SeccompProfile:   RuntimeDefault
IP:               10.42.65.86
IPs:
  IP:           10.42.65.86
Controlled By:  ReplicaSet/postgresql-operator-cloudnative-pg-747d847484
Containers:
  manager:
    Container ID:    containerd://b81aa33ff6c7e4bb98f4b634f8fd79369346812f4d06d93e6f24f32ddc45b850
    Image:           ghcr.io/cloudnative-pg/cloudnative-pg:1.27.0
    Image ID:        ghcr.io/cloudnative-pg/cloudnative-pg@sha256:9e5633b36f1f3ff0bb28b434ce51c95fbb8428a4ab47bc738ea403eb09dbf945
    Ports:           8080/TCP, 9443/TCP
    Host Ports:      0/TCP, 0/TCP
    SeccompProfile:  RuntimeDefault
    Command:
      /manager
    Args:
      controller
      --leader-elect
      --max-concurrent-reconciles=10
      --config-map-name=cnpg-controller-manager-config
      --webhook-port=9443
    State:          Running
      Started:      Tue, 03 Feb 2026 17:30:32 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get https://:9443/readyz delay=3s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get https://:9443/readyz delay=3s timeout=1s period=10s #success=1 #failure=3
    Startup:        http-get https://:9443/readyz delay=0s timeout=1s period=5s #success=1 #failure=6
    Environment:
      OPERATOR_IMAGE_NAME:           ghcr.io/cloudnative-pg/cloudnative-pg:1.27.0
      OPERATOR_NAMESPACE:            postgresql-operator (v1:metadata.namespace)
      MONITORING_QUERIES_CONFIGMAP:  cnpg-default-monitoring
    Mounts:
      /controller from scratch-data (rw)
      /run/secrets/cnpg.io/webhook from webhook-certificates (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-szwmn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  scratch-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  webhook-certificates:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cnpg-webhook-cert
    Optional:    true
  kube-api-access-szwmn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             tigera-operator-7d5b68db9f-j47rn
Namespace:        tigera-operator
Priority:         0
Service Account:  tigera-operator
Node:             orch-tf/192.168.99.10
Start Time:       Tue, 03 Feb 2026 17:23:36 +0000
Labels:           k8s-app=tigera-operator
                  name=tigera-operator
                  pod-template-hash=7d5b68db9f
Annotations:      <none>
Status:           Running
IP:               192.168.99.10
IPs:
  IP:           192.168.99.10
Controlled By:  ReplicaSet/tigera-operator-7d5b68db9f
Containers:
  tigera-operator:
    Container ID:  containerd://0da03b9e8fbf03dde748b78da1211308910394b693ed6e94a4b18e5d785ad52a
    Image:         docker.io/rancher/mirrored-calico-operator:v1.38.6
    Image ID:      sha256:1911afdd8478c6ca3036ff85614050d5d19acc0f0c3f6a5a7b3e34b38dd309c9
    Port:          <none>
    Host Port:     <none>
    Command:
      operator
    Args:
      -manage-crds=false
    State:          Running
      Started:      Tue, 03 Feb 2026 17:23:37 +0000
    Ready:          True
    Restart Count:  0
    Environment Variables from:
      kubernetes-services-endpoint  ConfigMap  Optional: true
    Environment:
      WATCH_NAMESPACE:                     
      POD_NAME:                            tigera-operator-7d5b68db9f-j47rn (v1:metadata.name)
      OPERATOR_NAME:                       tigera-operator
      TIGERA_OPERATOR_INIT_IMAGE_VERSION:  v1.38.6
    Mounts:
      /var/lib/calico from var-lib-calico (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pxcl7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  kube-api-access-pxcl7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 :NoExecute op=Exists
                             :NoSchedule op=Exists
Events:
  Type     Reason           Age   From          Message
  ----     ------           ----  ----          -------
  Warning  PolicyViolation  13m   kyverno-scan  policy restricted-policy-orch/restricted-policy-orch fail: Validation rule 'restricted-policy-orch' failed. It violates PodSecurity "restricted:latest": (Forbidden reason: restricted volume types, field error list: [spec.volumes[0].hostPath: Forbidden])(Forbidden reason: runAsNonRoot != true, field error list: [spec.containers[0].securityContext.runAsNonRoot: Required value])(Forbidden reason: seccompProfile, field error list: [spec.containers[0].securityContext.seccompProfile.type: Required value])(Forbidden reason: allowPrivilegeEscalation != false, field error list: [spec.containers[0].securityContext.allowPrivilegeEscalation: Required value])(Forbidden reason: unrestricted capabilities, field error list: [spec.containers[0].securityContext.capabilities.drop: Required value])(Forbidden reason: host namespaces, field error list: [spec.hostNetwork is forbidden, forbidden values found: true])(Forbidden reason: hostPath volumes, field error list: [spec.volumes[0].hostPath is forbidden, forbidden values foun...
  Warning  PolicyViolation  13m   kyverno-scan  policy require-ro-rootfs/validate-readOnlyRootFilesystem fail: validation error: Root filesystem must be read-only. rule validate-readOnlyRootFilesystem failed at path /spec/containers/0/securityContext/
